{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UQRegressors","text":"<p>UQRegressors is a Python package that provides machine learning regression models capable of generating prediction intervals for a user-specified confidence level. These models not only estimate the expected output but also quantify the uncertainty around each prediction. For instance, a model from UQRegressors trained to predict house prices could, given a new set of input features, return a 95% confidence interval\u2014indicating that the true price is expected to lie between a predicted lower and upper bound with 95% certainty. Several models from Bayesian and Conformal Prediction literature are implemented and validated on a PyTorch backend, which can be easily applied to regression problems through a scikit-learn <code>fit</code>, <code>predict</code> interface. </p> <p>Key Features</p> <ul> <li>Highly customizable parameters for each model</li> <li>Easy-to-use interface with built in dataset validation</li> <li>GPU compatibility with PyTorch backend </li> <li>Validated implementations with comparisons to published results </li> <li>Easy saving and loading of created models </li> <li>Wide variety of metrics available to assess quality of fit and prediction intervals</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>There are five main capabilities of UQRegessors: </p> <ol> <li>Dataset Loading and Validation </li> <li>Regression using models of various types created with UQ capability</li> <li>Hyperparameter Tuning using bayesian optimization (wrapper around Optuna)</li> <li>Metrics for evaluating goodness of fit and quality of uncertainty intervals</li> <li>Visualization of metrics, goodness of fit, and quality of uncertainty intervals</li> </ol> <p>For a simple demonstration of how these features could be used for your problem, check the \"Is UQRegressors right for me?\" example. For a more holistic view of UQRegressors' cabilities, look at the \"Getting Started\" example. </p>"},{"location":"#installation","title":"Installation","text":"<p>To install all core components of UQRegressors, run: <pre><code>pip install UQRegressors \n</code></pre></p>"},{"location":"#installing-pytorch","title":"Installing PyTorch","text":"<p>UQRegressors requires PyTorch, which must be installed separately to match your system's configuration.</p>"},{"location":"#cpu-only","title":"CPU-only:","text":"<pre><code>pip install torch torchvision torchaudio\n</code></pre>"},{"location":"#with-cuda-support-for-gpu","title":"With CUDA support for GPU:","text":"<p>Choose the appropriate command based on your CUDA version:</p> <ul> <li>CUDA 11.8: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></li> <li>CUDA 12.1: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre></li> </ul> <p>For other versions or platforms, check the official PyTorch installation page</p>"},{"location":"#what-next","title":"What Next?","text":"<p>To look more into the capabilities of UQRegressors, look at the Examples, particularly the getting started example. For notes on the types of regressors which were implemented, check the Regressor Details section. For detailed documentation on functions and how to use UQRegressors, explore the API Reference. For any other questions, email arjunrs@stanford.edu. </p>"},{"location":"regressor_details/","title":"Uncertainty Estimation Methods","text":""},{"location":"regressor_details/#uncertainty-estimation-with-distributional-methods","title":"Uncertainty Estimation with Distributional Methods","text":"<p>For generating the uncertainty intervals returned by a function, we make a distinction between distributional methods and non-distributional methods. Distributional methods seek to return a distribution instead of a point prediction for each point, whereas non-distributional methods return an upper and lower bound with some confidence \\(1-\\alpha\\). Many distributional methods exist, but the below methods were selected for their published performance on non-aerospace datasets.</p>"},{"location":"regressor_details/#standard-gaussian-process-regression","title":"Standard Gaussian Process Regression","text":"<p>Gaussian process regression is perhaps one of the most established and widely used methods for regression with uncertainty quantification within the aerospace discipline. A brief introduction is given here to standard Gaussian process regression, with a full treatment in (Rasmussen). Gaussian processes model a distribution over candidate functions which fit the training data by assuming that the training outputs and the test outputs are drawn from a jointly distributed Multi-variate Gaussian. Neglecting measurement noise and assuming a zero mean prior for simplicity of explanation, we assume that the joint distribution of the training outputs, \\(y_{tr}\\) and the testing outputs, \\(y_{te}\\), is given by:</p> \\[ \\begin{bmatrix}     f(X_{tr}) \\\\     f(X_{te}) \\end{bmatrix} = \\mathcal{N}\\left(0, \\begin{bmatrix}     K &amp;  K_* \\\\     K_*^{T} &amp; K_{**} \\end{bmatrix} \\right) \\] <p>for covariance matrices given by some measure of similarity (kernel function: \\(\\text{cov}\\)) \\(K=\\text{cov}(X_{tr}, X_{tr}), K_* = \\text{cov}(X_{tr}, X_{te}), K_{**} = \\text{cov}(X_{te}, X_{te})\\). After obtaining the data, \\(f(X_{tr}) = y_{tr}\\), we obtain the posterior distribution by conditioning this multivariate normal distribution:</p> \\[ y_{te} | y_{tr}, X_{tr}, X_{te} = \\mathcal{N}\\left(K_*K^{-1}y_{tr}, K_{**} - K_*K^{-1}K_*^T\\right) \\] <p>As an intuitive explanation of this formula, the mean of this normal distribution is the outputs of the training data weighted by how similar their corresponding inputs are to the test input, while the variance is entirely dependent on this similarity. Similarity is determined by the kernel function mentioned above, which uses the kernel trick to evaluate the similarity of the input vectors in a much higher dimensional space to evaluate a rich measure of similarity. One of the most commonly used kernel functions is the Radial Basis Function (RBF), which is computed for the length scale parameter, \\(l\\) as:</p> \\[ \\text{cov}(x_i, x_j) = \\exp\\left(-\\frac{||x_i-x_j||^2_2}{2l^2}\\right). \\] <p>This kernel function is used for several desirable properties; namely its infinite dimensional feature space, exponentially decaying value of similarity, and positive definiteness. One limitation of this kernel function is that it implicitly assumes that the underlying function is smooth, and can give uninformative measures of similarity in high dimension, where the Euclidean distance between points tends to concentrate.</p> <p>This kernel is also strongly dependent on a reasonable choice of the length-scale parameter \\(l\\), which is often optimized by maximizing the log-likelihood of the training data, \\(y\\) with gradient based methods. The log-likelihood is written as:</p> \\[ \\log p(y_{tr} | X_{tr}, l) = -\\frac{1}{2}y_{tr}^T K^{-1}y_{tr} - \\frac{1}{2} \\log|K| - C_1 \\] <p>The first term of this measures the relation between the output values, and the relation expected by the covariance matrix structure. For example, if the covariance matrix predicts that y values should be highly correlated and they are not, the loss will be large. The second term is a measurement complexity term, which tries to drive the outputs of the kernel covariance lower. In the case of the RBF kernel, this corresponds to prioritizing smooth functions which still have a good data-fit. The constant, \\(C_1\\) is not important for the hyperparameter optimization. In practice, a measurement noise term, \\(\\sigma_n^2\\) is co-optimized with \\(l\\), but it is not included here for simplicity of explanation.</p> <p>Gaussian process regression with hyperparameter optimization can often fit low-dimensional smooth functions extremely well, but also has a time complexity of \\(O(n^3)\\) to fit data arising from the need to invert the \\(n \\times n\\) covariance matrix. This makes Gaussian process regression a natural choice for datasets with only a few number of points. After being trained, the time complexity to perform inference on \\(m\\) points is \\(O(n^2m)\\), meaning that Gaussian process regression can quickly become impractical for optimization routines or other use cases where the number of predictions required is extremely large. Typically, standard Gaussian process regression is impractical for datasets beyond a few thousand points.</p>"},{"location":"regressor_details/#black-box-matrix-multiplication-gaussian-process-regression","title":"Black-Box Matrix Multiplication Gaussian Process Regression","text":"<p>The key reason why Gaussian Process Regressions have a large training time complexity is the requirement to invert the matrix \\(K\\). This is typically done using the Cholesky decomposition, but there exist other methods of approximate inversion which can significantly reduce the training time complexity. In particular, the black-box matrix multiplication variation of a GP uses \\(T\\) iterations of the conjugate gradients algorithm with a low rank preconditioner in order to compute an approximate inversion in \\(O(n^2T)\\) time, giving an exact solution if \\(T=n\\) (Gardner). Additionally, they have optimized their methodology for parallel computing hardware, with packaged code available in the python repository GPytorch. Since all of the deep learning models to follow are implemented for GPU hardware using PyTorch, this allows for a fair comparison of training and inference time.</p>"},{"location":"regressor_details/#monte-carlo-dropout","title":"Monte-Carlo Dropout","text":"<p>Monte-Carlo dropout is a method of uncertainty quantification for arbitrary neural networks, which is a neural network approximation of Gaussian processes. Critically, because of the deep learning architecture, it can avoid the large time complexity of Gaussian Processes such that it can scale to extremely large datasets. To train a neural network with dropout, at each iteration, each neuron in the hidden layers is assigned a binary variable with probability \\(p_{drop}\\) (typically near 0.1 or 0.2) of being 0. Each neuron assigned a value of 0 is dropped for that training pass, meaning that it simply returns a value of zero. The parameters are trained to minimize the mean squared error loss in this stochastic setting, where each iteration has a different combination of parameters. To perform inference, we perform \\(T\\) passes through the network with the same dropout methodology as during training to obtain \\(T\\) different samples of function values at each point. If desired, a distribution (typically Gaussian) can be fit to the outputs at each point. The theoretical results of (Gal) demonstrate that this approximates Bayesian Neural networks, where a distribution is placed over the weights within the neural network. However, Bayesian neural networks require a doubling of the parameters of a standard neural network, or one with dropout, and can also have undesirable optimization landscapes with poor convergence properties. The empirical results of (Gal) demonstrate comparable or better performance of Monte-Carlo dropout as compared to Bayesian Neural Networks on a variety of datasets, which is why Monte-Carlo dropout is implemented over Bayesian Neural Networks for this comparison.</p> <p>While Monte-Carlo dropout is an extremely efficient method of obtaining uncertainty distributions in addition to a mean prediction, the outputs are random, so they can be non-smooth, which can cause difficulties for optimization. Additionally, each training iteration optimizes different network parameters for a new random realization of the mean function, so the accuracy of this method may be less than that of traditional neural networks without dropout. Key benefits of Monte-Carlo dropout include that there are no required assumptions on the distribution of uncertainty, and that implementation requires minimum modification to standard neural networks.</p>"},{"location":"regressor_details/#deep-ensemble","title":"Deep Ensemble","text":"<p>Deep Ensembles, proposed by (Lakshminarayanan), are another relatively inexpensive approximation of Bayesian Neural Networks, which involve training several (i.e., an ensemble) of models and combining their outputs. Each regressor within the ensemble assumes that each point is an observation from a Gaussian distribution, and is trained to predict both the mean and standard deviation of an output given an input. During training, the negative log likelihood of the data given the predicted mean and standard deviation is minimized. During inference, the mean and variance from each regressor are combined by assuming that the overall distribution of uncertainty is also Gaussian with the mean and variance of the mixture. For \\(K\\) models, the mean and standard deviation are calculated as:</p> \\[ \\mu_*(x) = K^{-1}\\sum_{k=1}^K \\mu_{\\theta_k}(x), \\sigma_*^2(x)=K^{-1}\\sum_{k=1}^K(\\sigma_{\\theta_m}^2(x) + \\mu_{\\theta_m}^2(x)) - \\mu_*^2(x) \\] <p>When implementing this model, it is useful to let the outputs of each model predict the log of the standard deviation instead of the standard deviation itself for numerical stability and positive definiteness of the standard deviation. Additionally, one variant of this model is to randomly select a different subset of the training examples for every regressor to prioritize diversity in prediction and calibrate the uncertainty intervals towards difficulty of prediction. However, this was not necessary in order to produce well-calibrated intervals during the empirical studies by (Lakshminarayanan). In these studies, Deep Ensembles were shown to perform similarly or slightly worse as compared to Monte-Carlo dropout on a variety of public regression datasets on the basis of RMSE, but to significantly outperform Monte-Carlo dropout on the basis of negative log likelihood evaluated on the test set. This implies that deep ensembles output well calibrated uncertainty distributions, as the log-likelihood will penalize both poor fit and either over or under confident uncertainty intervals. This is explained in the paper by the fact that deep ensembles explicitly optimize for negative log likelihood during training as opposed to mean squared error.</p> <p>Deep ensembles are able to produce smooth uncertainty estimates in a well-calibrated sense, but assume that the uncertainty follows a known distribution. In many cases, a Gaussian distribution may be a good approximation to the distribution of uncertainty, unless prior knowledge about the problem indicates that a different distribution should be used. Additionally, Deep ensembles require fitting \\(K\\) estimators, although they can be fit in parallel to accomplish the same running time as standard neural networks. Ensemble predictors are generally desirable, as they can reduce the variance of the output function found through training, particularly for sparse datasets in high dimensions. More detail on the variance reduction of ensemble predictors is described in Variance reduction with ensemble methods section below.</p>"},{"location":"regressor_details/#uncertainty-estimation-with-distribution-free-methods","title":"Uncertainty Estimation with Distribution-free Methods","text":"<p>While assuming a Gaussian distribution on uncertainty is often a reasonable assumption, this may not hold for certain processes or certain datasets. As a simple example, if we consider the drag of an airfoil near stall, very small changes in the angle of attack can result in large drops in lift. Therefore, slight modeling errors will result in a non-Gaussian distribution of error, where errors are more likely to be distributed below the prediction, to have a long-tailed distribution, and perhaps even demonstrate bi-modality. This can become especially important if the observations come from the physical world, as in a wind tunnel test, which may have measurement noise which does not demonstrate a Gaussian distribution. For this reason, a method of quantifying uncertainty which does not assume a distribution on uncertainty is included in these comparisons. A method which is rising in popularity in the machine learning community to accomplish distribution-free uncertainty estimation is conformal prediction. An overview of the simplest method of conformal prediction, split conformal prediction, followed by a description of more sophisticated methods.</p> <p>The key idea of conformal prediction is to return a set, \\(\\hat C (X_{te_i})\\), for each test input, such that:</p> \\[ \\mathbb{P}(y_{te_i} \\in \\hat C(X_{te_i})) \\geq 1- \\alpha, \\] <p>where \\(\\alpha\\) is a user-specified error rate. In this way, the model developer has a \\(1-\\alpha\\) level of confidence that the returned intervals contain the true response values (Vovk2022). While several methods of conformal prediction exist, we explain the process with the simplest implementation: split conformal prediction.</p>"},{"location":"regressor_details/#split-conformal-prediction","title":"Split Conformal Prediction","text":"<p>Conformal prediction returns statistically valid sets with minimal assumptions on the data and no assumptions on the predictor by leveraging ideas from rank statistics. First, the available data is partitioned into a training, \\(S_1\\), and calibration, \\(S_2\\), set. A point predictor, \\(\\hat{f}\\), is fit on the training data, and absolute residuals are found on the calibration set. The absolute residuals are of the form:</p> \\[ R_i=|y_i-\\hat{f}(X_i)|, \\ i \\in S_2. \\] <p>For a feature-response pair in the test set with residual \\(R_{te_i}\\), we expect that the rank (or the sorted position) of the calibration residuals and the test residual is evenly distributed. This assumes that the new residual is exchangeable with the calibration residuals. We can then form the rank-adjusted quantile:</p> \\[ \\hat{q} = \\text{the } \\frac{\\lceil(n_2+1)(1-\\alpha)\\rceil}{n_2} \\text{ quantile of } R_i, \\ i \\in S_2, \\] <p>where the term \\(\\frac{(n_2 + 1)}{n_2}\\) is a finite sample correction such that:</p> \\[ \\mathbb{P}(R_{te_i} \\leq \\hat{q})  \\geq 1-\\alpha. \\] <p>A prediction interval satisfying the above can then be formed as:</p> \\[ \\hat C(X_{te_i}) = [\\hat{f}(X_{te_i}) - \\hat{q},\\hat{f}(X_{te_i})+\\hat{q}], \\] <p>The only assumption made about the data and the predictor is that the residuals of the calibration set and the residuals of new test points are exchangeable, meaning that their joint probability distribution is the same regardless of their permutation. Split conformal prediction is a simple method of generating statistically valid prediction intervals, but it requires partitioning the available data into a training and calibration set, which means that not all of the available data is used to train the model (low statistical efficiency) (Angelopoulos2023).</p> <p>The size of the calibration set also has a substantial impact on the performance of split conformal prediction. Although the average coverage is guaranteed to be \\(1-\\alpha\\), the total coverage over different random splits of the data follows a beta distribution with approximate variance \\(\\frac{\\alpha(1-\\alpha)}{n_2+2}\\). Therefore, if \\(n_2\\) is small, a given split of the available data can substantially undercover. In practical implementations of split conformal prediction, it is recommended to use calibration sets of at least 100 feature response pairs. To remedy the low statistical efficiency of this method, ensemble regression can be used through the methodology of K-fold CV+.</p>"},{"location":"regressor_details/#k-fold-cv-and-cv-minmax","title":"K-fold CV+ and CV-minmax","text":"<p>The exchangeability requirement mandates determining the residuals on a set of data not used to train the model. K-fold CV is a method of conformal prediction based on ideas from cross-validation, where the data is split into \\(K\\) folds, and \\(K\\) predictors are trained on each permutation of \\(K-1\\) folds. For each predictor, calibration residuals are evaluated on the fold that is left out from training. In this way, a set of calibration residuals which are exchangeable with the residuals from unseen data can be generated while still using a majority of data for model training. The interval half-width can then be constructed as in split conformal prediction, using the combined set of calibration residuals from each fold (barber2020).</p> <p>Two methods of generating prediction intervals with K-fold CV provide statistical guarantees. CV+ centers the prediction interval around the average of the \\(K\\) predictors:</p> \\[ \\hat C(X_{n+1}) = [\\hat{f}_{mean}(X_{n+1}) - \\hat{q},\\hat{f}_{mean} (X_{n+1})+\\hat{q}],  \\ \\hat{f}_{mean} (X_{n+1}) = \\frac{1}{K} \\sum_{i=1}^K \\hat{f}_i(X_{n+1}), \\] <p>whereas CV-minmax constructs bounds around the minimum and maximum of the \\(K\\) predictors:</p> \\[ \\hat C(X_{n+1}) = [\\hat{f}_{low}(X_{n+1}) - \\hat{q},\\hat{f}_{high} (X_{n+1})+\\hat{q}],  \\ \\hat{f}_{low} (X_{n+1}) = \\text{min}(\\hat{f}_i), \\ \\hat{f}_{high} (X_{n+1}) = \\text{max}(\\hat{f}_i), \\ i \\in K. \\] <p>In exchange for higher statistical efficiency, CV+ has a relaxed statistical guarantee. The average coverage guarantee of CV+ is \\(1-2\\alpha - \\sqrt{\\frac{2}{n_2}}\\), while the average coverage guarantee of CV-minmax is \\(1-\\alpha\\). In practice, however, CV+ generally has an average coverage of \\(1-\\alpha\\), while the coverage of CV-minmax is typically larger than \\(1-\\alpha\\).</p> <p>Thus, while CV+ does not enjoy the same guarantee as split conformal prediction, it typically performs well in practice (barber2020). CV-minmax always returns prediction intervals which are wider than CV+, and tends to overcover in practice.</p> <p>An additional downside of generic split conformal prediction is that the interval half-width, \\(\\hat{q}\\), is constant across the output space, meaning that the intervals will generally be too small for input values corresponding to outputs which are difficult to predict, and too large for easily predictable outputs, despite maintaining a \\(1-\\alpha\\) average coverage (Romano2019). A constant width is uninformative when it comes to adaptive sampling, so different methods of conformal prediction modify the methodology to generate adaptive interval widths.</p>"},{"location":"regressor_details/#conformal-quantile-regression","title":"Conformal Quantile Regression","text":"<p>Conformal quantile regression produces locally adaptive prediction intervals by wrapping conformal prediction around an interval predictor instead of a point predictor. Instead of fitting a predictor to the mean of the available data, two predictors are fit to the \\(\\alpha/2\\) and the \\(1-\\alpha/2\\) quantiles of the data. This alone does not provide any statistical guarantees, but the split or CV conformal prediction methods can then be applied to the quantile estimators to produce modifications to the interval width that satisfy the desired statistical guarantee (Romano2019).</p> <p>For many regression methods, fitting predictors to quantiles of the data can be done by modifying the loss function. In this work, neural networks are used with a tilted loss function to predict quantiles. The neural network predicts a quantile, \\(\\tau\\) by minimizing the loss, \\(L\\) over all points, where:</p> \\[ L(X_i)=\\max\\left(-\\tau (\\hat{f}(X_i)-Y_i), (1-\\tau)(\\hat{f}(X_i)-Y_i)\\right). \\] <p>A simple synthetic dataset to demonstrate the use of wrapping a quantile regressor with conformal prediction was created. A set of 1,000 data pairs was generated with:</p> \\[ y=0.1(\\sin(2\\pi x) + 0.4 \\epsilon (0.1 + x)), \\ x \\in[0,1], \\ \\epsilon \\sim \\mathcal{N}(0, 1). \\] <p>Defining \\(\\alpha=0.1\\), neural networks were fitted to the 0.05 and 0.95 quantiles using 250 training points. Split conformal prediction was used with a calibration set size of 250. Finally, the coverage was evaluated on a set of 500 test points. The prediction intervals and test points are shown in the figure below. The conformal prediction interval bounds are slightly larger than the prediction made by the quantile neural network, which ensures the statistical guarantee is met. The coverage was evaluated over 10,000 random splits of the data, shown in the histogram below. The histogram demonstrates that the average coverage over these trials meets the desired coverage of 0.9. The variance of the coverage could be reduced by increasing the number of calibration points used.</p> <p> </p> <p>By wrapping quantile regressors with CV+ or CV-minmax, statistically-valid and locally-adaptive prediction intervals can be generated with relatively high statistical efficiency compared to split conformal prediction. The results presented in this work use quantile regressors wrapped with CV+. K-fold CV+ enables locally adaptive interval widths with relatively high statistical efficiency, but will still struggle to produce well-calibrated with datasets with less than a couple hundred points, as the small calibration set size will result in a high-variance distribution of coverage and interval widths. Additionally, since optimization of the mean function does not occur directly during training, the RMSE of this method as compared to other methods may be higher.</p>"},{"location":"regressor_details/#normalized-conformal-prediction-conformal-ensemble","title":"Normalized Conformal Prediction (Conformal Ensemble)","text":"<p>Using quantile regression is an effective method of obtaining locally adaptive interval widths when there are enough training points to give the model information about quantiles of the dataset. Another method of conformal prediction is known as normalized conformal prediction, which follows the same methodology as split conformal prediction, but constructs adaptive intervals based on normalized non-conformity scores. The key idea is that instead of choosing \\(\\hat{q} = \\text{the } \\frac{\\lceil(n_2+1)(1-\\alpha)\\rceil}{n_2} \\text{ quantile of } R_i, \\ i \\in S_2\\), we choose</p> \\[ \\hat{q} = \\text{the } \\frac{\\lceil(n_2+1)(1-\\alpha)\\rceil}{n_2} \\text{ quantile of } \\frac{R_i}{\\sigma(X_i)}, \\ i \\in S_2, \\] <p>where \\(\\sigma(X_i)\\) is an estimator of the difficulty of predicting \\(y_i\\). For points which are difficult to predict, \\(\\sigma(X_i)\\) should be large and vice versa, such that the normalized residuals, \\(\\frac{R_i}{\\sigma(X_i)}\\), are approximately equal. To construct the interval for a point in the test interval, we first compute \\(\\sigma(X_{te_i})\\), and then construct the interval as:</p> \\[ \\hat C(X_{te_i}) = [\\hat{f}(X_{te_i}) - \\hat{q}\\sigma(X_{te_i}),\\hat{f}(X_{te_i})+\\hat{q}\\sigma(X_{te_i})] \\] <p>In this way, if \\(\\sigma\\) is a well calibrated estimate of prediction difficulty, conformal prediction will return adaptive interval widths. While this method trains a mean regressor and can often be easier to implement than conformal quantile regression, it loses statistical guarantees without strong assumptions on the difficulty function, \\(\\sigma\\). However, this method has been shown empirically to provide good coverage (Nolte). Several methods exist to construct the difficulty function. One intuitive method is to train an ensemble of regressors, and to use the variance of the ensemble predictions at any point as an estimate of the prediction difficulty. This idea has close ties to the method of deep ensembles, where we assume that if models with different training trajectories provide vastly different outputs for a given input, then the uncertainty of the overall ensemble (the mean of their predictions) should also be large.</p> <p>To fit with the conformal prediction methodology, we must split the training dataset into a training and calibration set in order to use this method. In contrast to conformal K-fold quantile regression, we cannot use the K-fold training split here to achieve high statistical efficiency because in order to evaluate the normalized residual, we must calculate \\(\\sigma\\) for each point in the calibration set, which must be representative of the ensemble variance of a test point. Therefore, before beginning training, we split the data into a training and calibration set (typically an 80-20 split), then train each of \\(K\\) regressors on the training set. For each point in the calibration set and each regressor \\(k\\), we evaluate \\(\\mu_{\\theta_k}\\). We take the ensemble prediction \\(\\mu_*\\) to be the mean of the individual predictions, and \\(\\sigma(X_{cal_i})\\) to be the variance of the individual predictions. We then construct the normalized residuals for each point in the calibration set and store these values. To perform inference, we use the same methodology as on the calibration set to evaluate \\(\\mu_*(X_{te_i})\\), and \\(\\sigma(X_{te_i})\\), and construct the conformal interval as above.</p> <p>Normalized conformal prediction is simple to implement, and can provide adaptive intervals satisfying empirical coverage, but suffers from low statistical efficiency as the K-fold methodology cannot be used. Therefore, this method will likely perform poorly on datasets smaller than a few hundred points.</p>"},{"location":"api/metrics/","title":"uqregressors.metrics.metrics","text":"<p>This script contains many metrics which can be used to compare the efficacy of different models. The methods are described along with their functions below.  </p>"},{"location":"api/metrics/#uqregressors.metrics.metrics.RMSCD","title":"<code>RMSCD(lower, upper, y_true, alpha, n_bins=10)</code>","text":"<p>Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins (see group_conditional_coverage).</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The root mean square coverage deviation from alpha.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def RMSCD(lower, upper, y_true, alpha, n_bins=10): \n    \"\"\"\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins (see group_conditional_coverage).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The root mean square coverage deviation from alpha.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true, alpha)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    return np.sqrt(np.mean((gcc[\"bin_coverages\"] - (1-alpha)) ** 2))\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.RMSCD_under","title":"<code>RMSCD_under(lower, upper, y_true, alpha, n_bins=10)</code>","text":"<p>Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated only over bins which do not meet nominal coverage (see RMSCD, group_conditional_coverage).</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The root mean square coverage deviation from alpha over bins which do not meet nominal coverage.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def RMSCD_under(lower, upper, y_true, alpha, n_bins=10):\n    \"\"\"\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated only over bins which do not meet nominal coverage (see RMSCD, group_conditional_coverage).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The root mean square coverage deviation from alpha over bins which do not meet nominal coverage.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true, alpha)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    miscovered_bins = gcc[\"bin_coverages\"][gcc[\"bin_coverages\"] &lt; (1-alpha)]\n    if len(miscovered_bins) == 0: \n        rmscd = 0.0\n    else: \n        rmscd = np.sqrt(np.mean((miscovered_bins - (1-alpha)) ** 2))\n    return rmscd\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.average_interval_width","title":"<code>average_interval_width(lower, upper, **kwargs)</code>","text":"<p>Computes the average interval width (distance between the predicted upper and lower bounds). </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average distance between the upper and lower bound.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def average_interval_width(lower, upper, **kwargs):\n    \"\"\"\n    Computes the average interval width (distance between the predicted upper and lower bounds). \n\n    Args:\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n\n    Returns: \n        (float): Average distance between the upper and lower bound.\n    \"\"\"\n    _, lower, upper, _, _ = validate_inputs(lower, lower, upper, lower)\n    return np.mean(upper - lower)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.compute_all_metrics","title":"<code>compute_all_metrics(mean, lower, upper, y_true, alpha, n_bins=10, excluded_metrics=['group_conditional_coverage'])</code>","text":"<p>Compute all standard uncertainty quantification metrics and return as a dictionary. Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins. </p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[Tensor, ndarray]</code> <p>The mean predictions to compute metrics for, should be able to be flattened to one dimension.</p> required <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension. </p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as mean.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as mean.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into for conditional coverage metrics. </p> <code>10</code> <code>excluded_metrics</code> <code>list</code> <p>The key of any metrics to exclude from being returned.</p> <code>['group_conditional_coverage']</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the following metrics, except those named in excluded_metrics.</p> <p>rmse (float): Root Mean Square Error. </p> <p>coverage (float): Marginal coverage. </p> <p>average interval width (float): Average distance between upper and lower bound predictions.</p> <p>interval_score (float): Interval score between predictions and data. </p> <p>nll_gaussian (float): Average Negative Log Likelihood of data given predictions under Gaussian assumption.</p> <p>error_width_corr (float): Pearson correlation coefficient between true errors and predicted interval width. </p> <p>group_conditional_coverage (dict): Dictionary containing the mean and coverage of each bin when the outputs are split between several bins.</p> <p>RMSCD (float): Root mean square coverage deviation between the coverage conditional on output bin and the nominal coverage.</p> <p>RMSCD_under (float): Root mean square coverage deviation for all bins which undercover compared to nominal coverage.</p> <p>lowest_group_coverage (float): The lowest coverage of any bin into which the outputs were binned.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def compute_all_metrics(mean, lower, upper, y_true, alpha, n_bins=10, excluded_metrics=[\"group_conditional_coverage\"]):\n    \"\"\"\n    Compute all standard uncertainty quantification metrics and return as a dictionary.\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins. \n\n    Args: \n        mean (Union[torch.Tensor, np.ndarray]): The mean predictions to compute metrics for, should be able to be flattened to one dimension.\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension. \n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as mean.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as mean.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into for conditional coverage metrics. \n        excluded_metrics (list): The key of any metrics to exclude from being returned.\n\n    Returns: \n        (dict): dictionary containing the following metrics, except those named in excluded_metrics.\n\n            rmse (float): Root Mean Square Error. \n\n            coverage (float): Marginal coverage. \n\n            average interval width (float): Average distance between upper and lower bound predictions.\n\n            interval_score (float): Interval score between predictions and data. \n\n            nll_gaussian (float): Average Negative Log Likelihood of data given predictions under Gaussian assumption.\n\n            error_width_corr (float): Pearson correlation coefficient between true errors and predicted interval width. \n\n            group_conditional_coverage (dict): Dictionary containing the mean and coverage of each bin when the outputs are split between several bins.\n\n            RMSCD (float): Root mean square coverage deviation between the coverage conditional on output bin and the nominal coverage.\n\n            RMSCD_under (float): Root mean square coverage deviation for all bins which undercover compared to nominal coverage.\n\n            lowest_group_coverage (float): The lowest coverage of any bin into which the outputs were binned. \n    \"\"\"\n\n    mean, lower, upper, y_true, alpha = validate_inputs(mean, lower, upper, y_true, alpha)\n\n    metrics_dict = {\n        \"rmse\": rmse(mean, y_true, alpha=alpha),\n        \"coverage\": coverage(lower, upper, y_true, alpha=alpha),\n        \"average_interval_width\": average_interval_width(lower, upper, alpha=alpha),\n        \"interval_score\": interval_score(lower, upper, y_true, alpha),\n        \"nll_gaussian\": nll_gaussian(mean, lower, upper, y_true, alpha),\n        \"error_width_corr\": error_width_corr(mean, lower, upper, y_true), \n        \"group_conditional_coverage\": group_conditional_coverage(lower, upper, y_true, n_bins),\n        \"RMSCD\": RMSCD(lower, upper, y_true, alpha, n_bins),\n        \"RMSCD_under\": RMSCD_under(lower, upper, y_true, alpha, n_bins),\n        \"lowest_group_coverage\": lowest_group_coverage(lower, upper, y_true, n_bins)\n    }\n\n    return_dict = {}\n    for metric, value in metrics_dict.items(): \n        if metric not in excluded_metrics: \n            return_dict[metric] = value \n\n    return return_dict\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.coverage","title":"<code>coverage(lower, upper, y_true, **kwargs)</code>","text":"<p>Computes the coverage as a float between 0 and 1. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Coverage as a scalar between 0.0 and 1.0.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def coverage(lower, upper, y_true, **kwargs):\n    \"\"\"\n    Computes the coverage as a float between 0 and 1. \n\n    Args:\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n\n    Returns: \n        (float): Coverage as a scalar between 0.0 and 1.0.\n    \"\"\"\n    _, lower, upper, y_true, _ = validate_inputs(lower, lower, upper, y_true)\n    covered = (y_true &gt;= lower) &amp; (y_true &lt;= upper)\n    return np.mean(covered)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.error_width_corr","title":"<code>error_width_corr(mean, lower, upper, y_true, **kwargs)</code>","text":"<p>Computes the Pearson correlation coefficient between true errors and the predicted interval width.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Correlation coefficient between residuals and predicted interval width, bounded in [-1, 1].</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def error_width_corr(mean, lower, upper, y_true, **kwargs): \n    \"\"\"\n    Computes the Pearson correlation coefficient between true errors and the predicted interval width.\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n\n    Returns: \n        (float): Correlation coefficient between residuals and predicted interval width, bounded in [-1, 1].\n    \"\"\"\n    mean, lower, upper, y_true, _ = validate_inputs(mean, lower, upper, y_true)\n    width = upper - lower \n    res = np.abs(mean - y_true)\n    corr = np.corrcoef(width, res)[0, 1]\n    return corr\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.group_conditional_coverage","title":"<code>group_conditional_coverage(lower, upper, y_true, n_bins=10)</code>","text":"<p>Divides the outputs into approximately equal bins, and computes the coverage in each bin. Returns a dictionary containing the mean of the  output in each bin, and the coverage in each bin. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to compute conditional coverage for.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the following keys: </p> <p>y_true_bin_means (np.ndarray): One dimensional array of the mean of the outputs within each bin.</p> <p>bin_coverages (np.ndarray): One dimensional array of the coverage of the predictions within each bin.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def group_conditional_coverage(lower, upper, y_true, n_bins = 10): \n    \"\"\"\n    Divides the outputs into approximately equal bins, and computes the coverage in each bin. Returns a dictionary containing the mean of the \n    output in each bin, and the coverage in each bin. \n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        n_bins (int): The number of bins to compute conditional coverage for.\n\n    Returns: \n        (dict): dictionary containing the following keys: \n\n            y_true_bin_means (np.ndarray): One dimensional array of the mean of the outputs within each bin.\n\n            bin_coverages (np.ndarray): One dimensional array of the coverage of the predictions within each bin.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    coverage_mask = (y_true &gt; lower) &amp; (y_true &lt; upper)\n    sort_ind = np.argsort(y_true)\n    y_true_sort = y_true[sort_ind]\n    coverage_mask_sort = coverage_mask[sort_ind]\n    split_y_true = np.array_split(y_true_sort, n_bins)\n    split_coverage_mask = np.array_split(coverage_mask_sort, n_bins)\n    bin_means = [np.mean(bin) for bin in split_y_true]\n    bin_coverages = [np.mean(bin) for bin in split_coverage_mask]\n    return {\"y_true_bin_means\": np.array(bin_means), \n            \"bin_coverages\": np.array(bin_coverages)}\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.interval_score","title":"<code>interval_score(lower, upper, y_true, alpha, **kwargs)</code>","text":"<p>Computes the interval score as given in Gneiting and Raftery, 2007.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Interval score.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def interval_score(lower, upper, y_true, alpha, **kwargs):\n    \"\"\"\n    Computes the interval score as given in [Gneiting and Raftery, 2007](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n\n    Returns: \n        (float): Interval score.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    width = upper - lower\n    penalty_lower = (2 / alpha) * (lower - y_true) * (y_true &lt; lower)\n    penalty_upper = (2 / alpha) * (y_true - upper) * (y_true &gt; upper)\n    return np.mean(width + penalty_lower + penalty_upper)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.lowest_group_coverage","title":"<code>lowest_group_coverage(lower, upper, y_true, n_bins=10)</code>","text":"<p>Computes the coverage of the bin with lowest coverage when the outputs are divided into several bins and coverage is evaluated conditional on each bin. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The coverage of the least covered bin of outputs, float between 0 and 1.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def lowest_group_coverage(lower, upper, y_true, n_bins=10): \n    \"\"\"\n    Computes the coverage of the bin with lowest coverage when the outputs are divided into several bins and coverage is evaluated conditional on each bin. \n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The coverage of the least covered bin of outputs, float between 0 and 1. \n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    return np.min(gcc[\"bin_coverages\"])\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.nll_gaussian","title":"<code>nll_gaussian(mean, lower, upper, y_true, alpha, **kwargs)</code>","text":"<p>Computes the average negative log likelihood of the data given the predictions and assuming a Gaussian distribution of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Average negative log likelihood of the data given the predictions.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def nll_gaussian(mean, lower, upper, y_true, alpha, **kwargs):\n    \"\"\"\n    Computes the average negative log likelihood of the data given the predictions and assuming a Gaussian distribution of predictions.\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n\n    Returns: \n        (float): Average negative log likelihood of the data given the predictions.\n    \"\"\"\n    mean, lower, upper, y_true, alpha = validate_inputs(mean, lower, upper, y_true, alpha)\n    z = norm.ppf(1 - alpha / 2)\n    std = (upper - lower) / (2 * z)\n    std = np.clip(std, 1e-6, None)\n\n    log_likelihoods = -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((y_true - mean) / std) ** 2\n    return -np.mean(log_likelihoods)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.rmse","title":"<code>rmse(mean, y_true, **kwargs)</code>","text":"<p>Computes the root mean square error of the predictions compared to the targets.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[ndarray, Tensor]</code> <p>The mean predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as mean. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar root mean squared error.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def rmse(mean, y_true, **kwargs):\n    \"\"\"\n    Computes the root mean square error of the predictions compared to the targets.\n\n    Args: \n        mean (Union[np.ndarray, torch.Tensor]): The mean predictions made by the model, should be able to be flattened to 1 dimension.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as mean. \n\n    Returns: \n        (float): Scalar root mean squared error.\n    \"\"\"\n    mean, _, _, y_true, _ = validate_inputs(mean, mean, mean, y_true)\n    return np.sqrt(np.mean((mean - y_true) ** 2))\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.validate_inputs","title":"<code>validate_inputs(mean, lower, upper, y_true, alpha=0.5)</code>","text":"<p>Ensure inputs are converted to 1D numpy arrays and alpha is a float in (0, 1) for use in computing metrics.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[Tensor, ndarray]</code> <p>The mean predictions to compute metrics for, should be able to be flattened to one dimension.</p> required <code>lower</code> <code>Union[Tensor, ndarray]</code> <p>The lower bound predictions to compute metrics for, should be the same shape as mean. </p> required <code>upper</code> <code>Union[Tensor, ndarray]</code> <p>The upper bound predictions to compute metrics for, should be the same shape as mean. </p> required <code>y_true</code> <code>Union[Tensor, ndarray]</code> <p>The targets to compute metrics with, should be the same shape as mean.</p> required <code>alpha</code> <code>float</code> <p>The desired confidence level, if relevannt, should be a float between 0 and 1.</p> <code>0.5</code> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def validate_inputs(mean, lower, upper, y_true, alpha=0.5):\n    \"\"\"\n    Ensure inputs are converted to 1D numpy arrays and alpha is a float in (0, 1) for use in computing metrics.\n\n    Args: \n        mean (Union[torch.Tensor, np.ndarray]): The mean predictions to compute metrics for, should be able to be flattened to one dimension.\n        lower (Union[torch.Tensor, np.ndarray]): The lower bound predictions to compute metrics for, should be the same shape as mean. \n        upper (Union[torch.Tensor, np.ndarray]): The upper bound predictions to compute metrics for, should be the same shape as mean. \n        y_true (Union[torch.Tensor, np.ndarray]): The targets to compute metrics with, should be the same shape as mean.\n        alpha (float): The desired confidence level, if relevannt, should be a float between 0 and 1. \n    \"\"\"\n\n    def to_1d_numpy(x):\n        if isinstance(x, torch.Tensor):\n            x = x.detach().cpu().numpy()\n        x = np.asarray(x)\n        if x.ndim != 1:\n            x = x.flatten()\n        return x\n\n    mean = to_1d_numpy(mean).astype(np.float64)\n    lower = to_1d_numpy(lower).astype(np.float64)\n    upper = to_1d_numpy(upper).astype(np.float64)\n    y_true = to_1d_numpy(y_true).astype(np.float64)\n\n    if not (0 &lt; float(alpha) &lt; 1):\n        raise ValueError(f\"alpha must be in (0, 1), got {alpha}\")\n\n    length = len(mean)\n    if not (len(lower) == len(upper) == len(y_true) == length):\n        raise ValueError(\"All input arrays must be of the same length.\")\n\n    return mean, lower, upper, y_true, float(alpha)\n</code></pre>"},{"location":"api/plotting/","title":"uqregressors.plotting.plotting","text":""},{"location":"api/plotting/#uqregressors.plotting.plotting--plotting","title":"Plotting","text":"<p>A collection of functions to visualize data generated by UQregressors. </p> The supported types of plots are <ul> <li>Calibration curves </li> <li>Predicted values vs. true values </li> <li>Bar chart of model comparisons based on metrics</li> </ul>"},{"location":"api/plotting/#uqregressors.plotting.plotting.generate_cal_curve","title":"<code>generate_cal_curve(model, X_test, y_test, alphas=np.linspace(0.7, 0.01, 10), refit=False, X_train=None, y_train=None)</code>","text":"<p>Generate the data for a calibration curve, which can be plotted with plot_cal_curve. </p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>The model for which to generate the calibration curve. </p> required <code>X_test</code> <code>array - like</code> <p>An array of testing features to generate the calibration curve for. </p> required <code>y_test</code> <code>array - like</code> <p>An array of testing targets to generate the calibration curve for. </p> required <code>alphas</code> <code>array - like</code> <p>The complement of the confidence intervals tested. If none, 10 alphas between 0.7 and 0.01 are linearly generated.</p> <code>linspace(0.7, 0.01, 10)</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the model for each alpha (useful for models like CQR where the underlying regressor depends on alpha).</p> <code>False</code> <code>X_train</code> <code>array - like</code> <p>Training features if refit is True. </p> <code>None</code> <code>y_train</code> <code>array - like</code> <p>Training targets if refit is True.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>(ndarray, ndarray, ndarray)</code> <p>The desired coverages, the empirical coverages, and the average interval widths for each alpha.</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def generate_cal_curve(model, X_test, y_test, alphas=np.linspace(0.7, 0.01, 10), refit=False, \n                       X_train=None, y_train=None):\n    \"\"\"\n    Generate the data for a calibration curve, which can be plotted with plot_cal_curve. \n\n    Args: \n        model (BaseEstimator): The model for which to generate the calibration curve. \n        X_test (array-like): An array of testing features to generate the calibration curve for. \n        y_test (array-like): An array of testing targets to generate the calibration curve for. \n        alphas (array-like): The complement of the confidence intervals tested. If none, 10 alphas between 0.7 and 0.01 are linearly generated.\n        refit (bool): Whether to re-fit the model for each alpha (useful for models like CQR where the underlying regressor depends on alpha).\n        X_train (array-like): Training features if refit is True. \n        y_train (array-like): Training targets if refit is True.\n\n    Returns: \n        Tuple(np.ndarray, np.ndarray, np.ndarray): The desired coverages, the empirical coverages, and the average interval widths for each alpha. \n    \"\"\"\n    if (refit == True) and (X_train is None or y_train is None): \n        raise ValueError(\"X_train and y_train must be given to generate a calibration curve with refit=True\")\n    alphas = np.array(alphas)\n    desired_coverage = 1 - alphas \n    coverages = np.zeros_like(desired_coverage)\n    avg_interval_widths = np.zeros_like(desired_coverage)\n\n    for i, alpha in enumerate(alphas): \n        # Clone model: \n        with tempfile.TemporaryDirectory() as tmpdirname: \n            fm = FileManager(tmpdirname)\n            saved_path = fm.save_model(model, name=None, path=None)\n            cloned_model = fm.load_model(model.__class__, path=saved_path)[\"model\"]\n        cloned_model.alpha = alpha \n        if refit == True: \n            cloned_model.fit(X_train, y_train)\n\n        mean, lower, upper = cloned_model.predict(X_test)\n        coverages[i] = coverage(lower, upper, y_test)\n        avg_interval_widths[i] = average_interval_width(lower, upper)\n\n    return desired_coverage, coverages, avg_interval_widths \n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_cal_curve","title":"<code>plot_cal_curve(desired_coverage, coverages, show=False, save_dir=None, filename='calibration_curve.png', title=None, figsize=(8, 5))</code>","text":"<p>Plot a calibration curve with data generated from uqregressors.plotting.plotting.generate_cal_curve. </p> <p>Parameters:</p> Name Type Description Default <code>desired_coverage</code> <code>array - like</code> <p>An array of the desired coverages for which the model was evaluated.</p> required <code>coverages</code> <code>array - like</code> <p>An array of the empirical coverages achieved by the model for each desired coverage. </p> required <code>show</code> <code>bool</code> <p>Whether to display the plot after generating it (True) or simply close (False).</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>If not None, the plot will be saved to the directory: save_dir/plots/filename. If associated with a model,              it is recommended that this directory is the directory in which the model is saved. </p> <code>None</code> <code>filename</code> <code>str</code> <p>The filename, including extension, to which the plots will be saved. </p> <code>'calibration_curve.png'</code> <code>title</code> <code>str</code> <p>The title included in the plot, if not None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure to be generated.</p> <code>(8, 5)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>If save_dir is not none, the path to which the file was saved is returned. Otherwise None is returned.</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_cal_curve(desired_coverage, coverages, show=False, save_dir=None, filename=\"calibration_curve.png\", title=None, figsize=(8, 5)): \n    \"\"\"\n    Plot a calibration curve with data generated from uqregressors.plotting.plotting.generate_cal_curve. \n\n    Args: \n        desired_coverage (array-like): An array of the desired coverages for which the model was evaluated.\n        coverages (array-like): An array of the empirical coverages achieved by the model for each desired coverage. \n        show (bool): Whether to display the plot after generating it (True) or simply close (False).\n        save_dir (str): If not None, the plot will be saved to the directory: save_dir/plots/filename. If associated with a model, \n                        it is recommended that this directory is the directory in which the model is saved. \n        filename (str): The filename, including extension, to which the plots will be saved. \n        title (str): The title included in the plot, if not None.\n        figsize (tuple): The size of the figure to be generated.\n\n    Returns: \n        (Union[str, None]): If save_dir is not none, the path to which the file was saved is returned. Otherwise None is returned. \n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.set_theme(style='whitegrid')\n    sns.lineplot(x=desired_coverage, y=coverages, marker='o', label='Empirical Coverage')\n    plt.plot([0, 1], [0, 1], 'k--', label='Ideal (y = x)')\n    plt.xlabel('Desired Coverage (1 - alpha)')\n    plt.ylabel('Empirical Coverage')\n    if title is not None: \n        plt.title(title)\n    plt.legend()\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.tight_layout()\n\n    if save_dir is not None: \n        fm = FileManager(save_dir)\n        save_path = fm.save_plot(plt.gcf(), save_dir, filename, show=show)\n\n        print (f\"Saved calibration curve to {save_path}\")\n        return save_path\n\n    else: \n        return None\n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_metrics_comparisons","title":"<code>plot_metrics_comparisons(solution_dict, y_test, alpha, excluded_metrics=[], show=False, save_dir=None, filename='.png', log_metrics=['rmse', 'interval_score', 'average_interval_width'], figsize=(8, 5))</code>","text":"<p>Generate bar charts which compare several models on the basis of all available metrics. </p> <p>Parameters:</p> Name Type Description Default <code>solution_dict</code> <code>dict[str</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]]): A dictionary containing the names of the methods to plot as the keys and  a tuple containing the mean, lower, and upper predictions of the model on the test set as the values. </p> required <code>y_test</code> <code>array - like</code> <p>The true values of the targets to compare against. </p> required <code>alpha</code> <code>float</code> <p>1 - the confidence level of predictions. Should be a float between 0 and 1. </p> required <code>excluded_metrics</code> <code>list[str]</code> <p>The names of metrics to exclude. See uqregressors.metrics.metrics.compute_all_metrics for a list of possible keys </p> <code>[]</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default: False.</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>Directory to save the figure. If None, the figure is not saved.</p> <code>None</code> <code>filename</code> <code>str</code> <p>File name for the plot. Default: \"pred_vs_true.png\".</p> <code>'.png'</code> <code>log_metrics</code> <code>list</code> <p>A list containing the keys of metrics to display on a log scale. </p> <code>['rmse', 'interval_score', 'average_interval_width']</code> <code>figsize</code> <code>tuple</code> <p>Desired figure size. </p> <code>(8, 5)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The save path to the directory in which plots were saved if save_dir is True, otherwise None</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_metrics_comparisons(solution_dict, y_test, alpha, excluded_metrics=[], show=False, save_dir=None, filename=\".png\", log_metrics = [\"rmse\", \"interval_score\", \"average_interval_width\"], figsize=(8, 5)): \n    \"\"\"\n    Generate bar charts which compare several models on the basis of all available metrics. \n\n    Args: \n        solution_dict (dict[str: Tuple[np.ndarray, np.ndarray, np.ndarray]]): A dictionary containing the names of the methods to plot as the keys and \n            a tuple containing the mean, lower, and upper predictions of the model on the test set as the values. \n        y_test (array-like): The true values of the targets to compare against. \n        alpha (float): 1 - the confidence level of predictions. Should be a float between 0 and 1. \n        excluded_metrics (list[str]): The names of metrics to exclude. See uqregressors.metrics.metrics.compute_all_metrics for a list of possible keys \n        show (bool): Whether to display the plot. Default: False.\n        save_dir (str, optional): Directory to save the figure. If None, the figure is not saved.\n        filename (str): File name for the plot. Default: \"pred_vs_true.png\".\n        log_metrics (list): A list containing the keys of metrics to display on a log scale. \n        figsize (tuple): Desired figure size. \n\n    Returns: \n        (Union[str, None]): The save path to the directory in which plots were saved if save_dir is True, otherwise None\n    \"\"\"\n\n    better_direction = {\n        \"rmse\": \"lower is better\",\n        \"nll_gaussian\": \"lower is better\",\n        \"interval_score\": \"lower is better\",\n        \"coverage\": \"closer to {:.2f} is better\".format(1 - alpha),\n        \"average_interval_width\": \"lower is better\",\n        \"error_width_corr\":\"higher is better\", \n        \"RMSCD\": \"lower is better\", \n        \"RMSCD_under\": \"lower is better\", \n        \"lowest_group_coverage\": \"higher is better\"\n    }\n\n    rows = [] \n    for method, (mean, lower, upper) in solution_dict.items(): \n        metrics = compute_all_metrics(mean, lower, upper, y_test, alpha, excluded_metrics=excluded_metrics + [\"group_conditional_coverage\"])\n        metrics[\"method\"] = method \n        rows.append(metrics)\n        rows.append(metrics)\n    metrics_df = pd.DataFrame(rows)\n\n    metrics = [col for col in metrics_df.columns if col!=\"method\"]\n\n    for metric in metrics: \n        plt.figure(figsize=figsize)\n        sns.barplot(data=metrics_df, x=\"method\", y=metric)\n        plt.xticks(rotation=45)\n\n        if metric in log_metrics:\n            plt.yscale(\"log\")\n\n        if metric == \"coverage\": \n            plt.axhline(1 - alpha, color=\"red\", linestyle=\"--\", label=\"Nominal\")\n            plt.legend() \n\n        title = f\"{metric} ({better_direction.get(metric, '')})\"\n        plt.title(title)\n        plt.ylabel(metric)\n        plt.xlabel(\"Method\")\n        plt.tight_layout()\n\n        if save_dir is not None: \n            fm = FileManager(save_dir)\n            save_path = fm.save_plot(plt.gcf(), save_dir, metric.strip() + \"_\" + filename, show=show)\n\n            print (f\"Saved model comparison to {save_path}\")\n\n    if save_dir is not None: \n        return save_dir\n    else: \n        return None\n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_pred_vs_true","title":"<code>plot_pred_vs_true(mean, lower, upper, y_true, samples=None, include_confidence=True, show=False, save_dir=None, filename='pred_vs_true.png', title=None, alpha=None, figsize=(8, 8))</code>","text":"<p>Plot predicted vs true values with optional confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>array - like</code> <p>Predicted mean values.</p> required <code>lower</code> <code>array - like</code> <p>Lower bound of prediction intervals.</p> required <code>upper</code> <code>array - like</code> <p>Upper bound of prediction intervals.</p> required <code>y_true</code> <code>array - like</code> <p>True target values.</p> required <code>samples</code> <code>int</code> <p>Number of samples to plot. Defaults to all.</p> <code>None</code> <code>include_confidence</code> <code>bool</code> <p>Whether to plot error bars. Default: True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default: False.</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>Directory to save the figure. If None, the figure is not saved.</p> <code>None</code> <code>filename</code> <code>str</code> <p>File name for the plot. Default: \"pred_vs_true.png\".</p> <code>'pred_vs_true.png'</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Confidence level (e.g., 0.1 for 90% interval).</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure to be generated. </p> <code>(8, 8)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The save path if the plot should be saved, otherwise None</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_pred_vs_true(mean, lower, upper, y_true, samples=None, include_confidence=True, show=False, save_dir=None, filename=\"pred_vs_true.png\", title=None, alpha=None, figsize=(8, 8)):\n    \"\"\"\n    Plot predicted vs true values with optional confidence intervals.\n\n    Args:\n        mean (array-like): Predicted mean values.\n        lower (array-like): Lower bound of prediction intervals.\n        upper (array-like): Upper bound of prediction intervals.\n        y_true (array-like): True target values.\n        samples (int): Number of samples to plot. Defaults to all.\n        include_confidence (bool): Whether to plot error bars. Default: True.\n        show (bool): Whether to display the plot. Default: False.\n        save_dir (str): Directory to save the figure. If None, the figure is not saved.\n        filename (str): File name for the plot. Default: \"pred_vs_true.png\".\n        title (str): Title of the plot.\n        alpha (float): Confidence level (e.g., 0.1 for 90% interval).\n        figsize (tuple): Size of the figure to be generated. \n\n    Returns: \n        (Union[str, None]): The save path if the plot should be saved, otherwise None\n    \"\"\"\n    mean = np.asarray(mean)\n    lower = np.asarray(lower)\n    upper = np.asarray(upper)\n    y_true = np.asarray(y_true)\n\n    n = len(y_true)\n    idx = np.arange(n)\n    if samples is not None:\n        samples = min(samples, n)\n        idx = np.random.choice(n, samples, replace=False)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    if include_confidence:\n        ax.errorbar(y_true[idx], mean[idx], \n                    yerr=[mean[idx] - lower[idx], upper[idx] - mean[idx]], \n                    fmt='o', ecolor='gray', alpha=0.75, capsize=3, label=f\"Predictions w/ CI\")\n    else:\n        ax.scatter(y_true[idx], mean[idx], alpha=0.75, label=\"Predictions\")\n\n    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', label=\"y = x\")\n    ax.set_xlabel(\"True values\")\n    ax.set_ylabel(\"Predicted values\")\n    ax.legend()\n\n    if alpha is not None:\n        ax.text(0.05, 0.95, f\"$\\\\alpha$ = {alpha}\", transform=ax.transAxes, va='top')\n\n    if title:\n        ax.set_title(title)\n\n    plt.tight_layout()\n    if save_dir is not None: \n        fm = FileManager(save_dir)\n        save_path = fm.save_plot(plt.gcf(), save_dir, filename, show=show)\n\n        print (f\"Saved calibration curve to {save_path}\")\n        return save_path\n\n    else: \n        return None\n</code></pre>"},{"location":"api/tuning/","title":"uqregressors.tuning.tuning","text":"<p>Tuning contains the helper function tune_hyperparams which uses the Bayesian hyperparameter optimization framework, Optuna as well as some examples of potential scoring functions. </p> Important features of this hyperparameter optimization method are <ul> <li>Customizable scoring function </li> <li>Customizable hyperparameters to be tuned </li> <li>Customizable number of tuning iterations, search space </li> <li>Support for cross-validation</li> </ul>"},{"location":"api/tuning/#uqregressors.tuning.tuning.interval_score","title":"<code>interval_score(estimator, X, y)</code>","text":"<p>Example of interval_score scoring function for hyperparameter tuning, greater=False</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def interval_score(estimator, X, y): \n    \"\"\"\n    Example of interval_score scoring function for hyperparameter tuning, greater=False\n    \"\"\"\n    alpha = estimator.alpha\n    _, lower, upper = estimator.predict(X)\n    width = upper - lower\n    penalty_lower = (2 / alpha) * (lower - y) * (y &lt; lower)\n    penalty_upper = (2 / alpha) * (y - upper) * (y &gt; upper)\n    return np.mean(width + penalty_lower + penalty_upper)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.interval_width","title":"<code>interval_width(estimator, X, y)</code>","text":"<p>Example of minimizing interval width scoring function for hyperparameter tuning, greater=False</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def interval_width(estimator, X, y): \n    \"\"\"\n    Example of minimizing interval width scoring function for hyperparameter tuning, greater=False\n    \"\"\"\n    _, lower, upper = estimator.predict(X)\n    return np.mean(upper - lower)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.log_likelihood","title":"<code>log_likelihood(estimator, X, y)</code>","text":"<p>Example of maximizing log likelihood scoring function for hyperparameter tuning, greater=True</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def log_likelihood(estimator, X, y): \n    \"\"\"\n    Example of maximizing log likelihood scoring function for hyperparameter tuning, greater=True\n    \"\"\"\n    mean, lower, upper = estimator.predict(X)\n    alpha = estimator.alpha \n    z = norm.ppf(1 - alpha / 2)\n    std = (upper - lower) / (2 * z)\n    std = np.clip(std, 1e-6, None)\n\n    log_likelihoods = -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((y.ravel() - mean) / std) ** 2\n    return np.mean(log_likelihoods)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.tune_hyperparams","title":"<code>tune_hyperparams(regressor, param_space, X, y, score_fn, greater_is_better, initial_params=None, n_trials=20, n_splits=3, random_state=42, test_size=0.2, verbose=True)</code>","text":"<p>Optimizes a scikit-learn-style regressor using Optuna.</p> <p>Supports CV when n_splits &gt; 1, otherwise uses train/val split.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>BaseEstimator</code> <p>An instance of a base regressor (must have .fit and .predict).</p> required <code>param_space</code> <code>dict</code> <p>Dict mapping param name \u2192 optuna suggest function (e.g., lambda t: t.suggest_float(...)).</p> required <code>X</code> <code>Union[Tensor, ndarray, DataFrame, Series]</code> <p>Training inputs.</p> required <code>y</code> <code>Union[Tensor, ndarray, DataFrame, Series]</code> <p>Training targets.</p> required <code>score_fn</code> <code>Callable(estimator, X_val, y_val) \u2192 float</code> <p>Scoring function.</p> required <code>greater_is_better</code> <code>bool</code> <p>Whether score_fn should be maximized (True) or minimized (False).</p> required <code>initial_params</code> <code>dict</code> <p>A dictionary of the initial params to start with. Otherwise, chosen by Optuna algorithm.</p> <code>None</code> <code>n_trials</code> <code>int</code> <p>Number of Optuna trials.</p> <code>20</code> <code>n_splits</code> <code>int</code> <p>If &gt;1, uses KFold CV; otherwise single train/val split.</p> <code>3</code> <code>random_state</code> <code>int</code> <p>For reproducibility.</p> <code>42</code> <code>test_size</code> <code>float</code> <p>Proportion of training examples to allocate to validation set; between 0 and 1. </p> <code>0.2</code> <code>verbose</code> <code>bool</code> <p>Print status messages.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[BaseEstimator, float, study]</code> <p>A tuple containing the estimator with the optimized                                           hyperparameters, the minimum score, and the optuna study object</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def tune_hyperparams(\n    regressor,\n    param_space,\n    X,\n    y,\n    score_fn,\n    greater_is_better,\n    initial_params = None,\n    n_trials=20,\n    n_splits=3,\n    random_state=42,\n    test_size=0.2,\n    verbose=True,\n):\n    \"\"\"\n    Optimizes a scikit-learn-style regressor using Optuna.\n\n    Supports CV when n_splits &gt; 1, otherwise uses train/val split.\n\n    Args:\n        regressor (BaseEstimator): An instance of a base regressor (must have .fit and .predict).\n        param_space (dict): Dict mapping param name \u2192 optuna suggest function (e.g., lambda t: t.suggest_float(...)).\n        X (Union[torch.Tensor, np.ndarray, pd.DataFrame, pd.Series]): Training inputs.\n        y (Union[torch.Tensor, np.ndarray, pd.DataFrame, pd.Series]): Training targets.\n        score_fn (Callable(estimator, X_val, y_val) \u2192 float): Scoring function.\n        greater_is_better (bool): Whether score_fn should be maximized (True) or minimized (False).\n        initial_params (dict): A dictionary of the initial params to start with. Otherwise, chosen by Optuna algorithm.\n        n_trials (int): Number of Optuna trials.\n        n_splits (int): If &gt;1, uses KFold CV; otherwise single train/val split.\n        random_state (int): For reproducibility.\n        test_size (float): Proportion of training examples to allocate to validation set; between 0 and 1. \n        verbose (bool): Print status messages.\n\n    Returns:\n        (Tuple[BaseEstimator, float, optuna.study]): A tuple containing the estimator with the optimized \n                                                     hyperparameters, the minimum score, and the optuna study object\n    \"\"\"\n\n    direction = \"maximize\" if greater_is_better else \"minimize\"\n\n    def objective(trial):\n        # Sample hyperparameters\n        trial_params = {k: suggest_fn(trial) for k, suggest_fn in param_space.items()}\n\n        scores = []\n\n        if n_splits == 1:\n            # Single train/val split\n            X_train, X_val, y_train, y_val = train_test_split(\n                X, y, test_size=test_size, random_state=random_state\n            )\n\n            # Copy the regressor if it has already been fit\n            if regressor.fitted: \n                with tempfile.TemporaryDirectory() as tmpdir: \n                    regressor.save(tmpdir)\n                    estimator = regressor.__class__.load(tmpdir)\n            else: \n                estimator = regressor\n\n            for param_name, param_value in trial_params.items():\n                setattr(estimator, param_name, param_value)\n\n            estimator.fit(X_train, y_train)\n            score = score_fn(estimator, X_val, y_val)\n            scores.append(score)\n        else:\n            # K-fold CV\n            kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n            for train_idx, val_idx in kf.split(X):\n                X_train, X_val = X[train_idx], X[val_idx]\n                y_train, y_val = y[train_idx], y[val_idx]\n\n                # Copy the regressor if it has already been fit \n                if regressor.fitted: \n                    with tempfile.TemporaryDirectory() as tmpdir: \n                        regressor.save(tmpdir)\n                        estimator = regressor.__class__.load(tmpdir)\n\n                else: \n                    estimator = regressor\n\n                for param_name, param_value in trial_params.items():\n                    setattr(estimator, param_name, param_value)\n\n                estimator.fit(X_train, y_train)\n                score = score_fn(estimator, X_val, y_val)\n                scores.append(score)\n\n        mean_score = np.mean(scores)\n\n        if verbose:\n            print(f\"Trial params: {trial_params} -&gt; Score: {mean_score:.4f}\")\n\n        return mean_score\n\n    study = optuna.create_study(direction=direction)\n\n    if initial_params is not None: \n        study.enqueue_trial(initial_params)\n\n    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n\n    # Re-train on full data with best hyperparameters\n    best_params = study.best_params\n\n    with tempfile.TemporaryDirectory() as tmpdir: \n        regressor.save(tmpdir)\n        best_estimator = regressor.__class__.load(tmpdir)\n\n    for k, v in best_params.items():\n        setattr(best_estimator, k, v)\n    best_estimator.fit(X, y)\n\n    if verbose:\n        print(\"Best score:\", study.best_value)\n        print(\"Best hyperparameters:\", best_params)\n\n    return best_estimator, study.best_value, study\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/","title":"uqregressors.bayesian.bbmm_gp","text":"<p>A wrapper for GPyTorch  is created which implements BlackBox Matrix-Matrix Inference Gaussian Process regression (BBMM-GP) as described in  Gardner et al., 2018</p>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP","title":"<code>BBMM_GP</code>","text":"<p>A wrapper around GPyTorch's ExactGP for regression with uncertainty quantification.</p> <p>Supports custom kernels, optimizers, learning schedules, and logging. Outputs mean predictions and confidence intervals using predictive variance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model instance.</p> <code>'BBMM_GP_Regressor'</code> <code>kernel</code> <code>Kernel</code> <p>Covariance kernel.</p> <code>ScaleKernel(RBFKernel())</code> <code>likelihood</code> <code>Likelihood</code> <p>Likelihood function used in GP.</p> <code>GaussianLikelihood()</code> <code>alpha</code> <code>float</code> <p>Significance level for predictive intervals (e.g. 0.1 = 90% CI).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>If True, returns tensors requiring gradients during prediction.</p> <code>False</code> <code>learning_rate</code> <code>float</code> <p>Optimizer learning rate.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>optimizer_cls</code> <code>Callable</code> <p>Optimizer class (e.g., torch.optim.Adam).</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Extra keyword arguments for the optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>Callable or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Extra keyword arguments for the scheduler.</p> <code>None</code> <code>loss_fn</code> <code>Callable or None</code> <p>Custom loss function. Defaults to negative log marginal likelihood.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to train the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>If True, enables wandb logging.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>Name of the wandb project.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>Name of the wandb run.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to scale input and output data.</p> <code>True</code> <code>input_scaler</code> <code>object or None</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>object or None</code> <p>Scaler for target values.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>List[Logger]</code> <p>Optional list of loggers from hyperparameter tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>_loggers</code> <code>list</code> <p>Logger of training loss.</p> <code>fitted</code> <code>bool</code> <p>Whether the fit method has been successfully called.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>class BBMM_GP: \n    \"\"\"\n    A wrapper around GPyTorch's ExactGP for regression with uncertainty quantification.\n\n    Supports custom kernels, optimizers, learning schedules, and logging.\n    Outputs mean predictions and confidence intervals using predictive variance.\n\n    Args:\n        name (str): Name of the model instance.\n        kernel (gpytorch.kernels.Kernel): Covariance kernel.\n        likelihood (gpytorch.likelihoods.Likelihood): Likelihood function used in GP.\n        alpha (float): Significance level for predictive intervals (e.g. 0.1 = 90% CI).\n        requires_grad (bool): If True, returns tensors requiring gradients during prediction.\n        learning_rate (float): Optimizer learning rate.\n        epochs (int): Number of training epochs.\n        optimizer_cls (Callable): Optimizer class (e.g., torch.optim.Adam).\n        optimizer_kwargs (dict): Extra keyword arguments for the optimizer.\n        scheduler_cls (Callable or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Extra keyword arguments for the scheduler.\n        loss_fn (Callable or None): Custom loss function. Defaults to negative log marginal likelihood.\n        device (str): Device to train the model on (\"cpu\" or \"cuda\").\n        use_wandb (bool): If True, enables wandb logging.\n        wandb_project (str or None): Name of the wandb project.\n        wandb_run_name (str or None): Name of the wandb run.\n        scale_data (bool): Whether to scale input and output data.\n        input_scaler (object or None): Scaler for input features.\n        output_scaler (object or None): Scaler for target values.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (List[Logger]): Optional list of loggers from hyperparameter tuning.\n\n    Attributes: \n        _loggers (list): Logger of training loss.\n        fitted (bool): Whether the fit method has been successfully called.\n    \"\"\"\n    def __init__(self, \n                 name=\"BBMM_GP_Regressor\",\n                 kernel=gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), \n                 likelihood=gpytorch.likelihoods.GaussianLikelihood(), \n                 alpha=0.1,\n                 requires_grad=False,\n                 learning_rate=1e-3,\n                 epochs=200, \n                 optimizer_cls=torch.optim.Adam,\n                 optimizer_kwargs=None,\n                 scheduler_cls=None,\n                 scheduler_kwargs=None,\n                 loss_fn=None, \n                 device=\"cpu\", \n                 use_wandb=False,\n                 wandb_project=None,\n                 wandb_run_name=None,\n                 scale_data = True, \n                 input_scaler = None, \n                 output_scaler = None, \n                 random_seed=None, \n                 tuning_loggers=[],\n            ):\n        self.name = name\n        self.kernel = kernel \n        self.likelihood = likelihood\n        self.alpha = alpha \n        self.requires_grad = requires_grad\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.optimizer_cls = optimizer_cls \n        self.optimizer_kwargs = optimizer_kwargs or {} \n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n        self.device = device \n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project \n        self.wandb_run_name = wandb_run_name\n        self.model = None\n        self.random_seed = random_seed\n\n        self._loggers = []\n        self.training_logs = None \n        self.tuning_loggers = tuning_loggers \n        self.tuning_logs = None\n\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self.train_X = None \n        self.train_y = None\n\n        self.fitted = False\n\n    def fit(self, X, y): \n        \"\"\"\n        Fits the GP model to training data.\n\n        Args:\n            X (np.ndarray or torch.Tensor): Training features of shape (n_samples, n_features).\n            y (np.ndarray or torch.Tensor): Training targets of shape (n_samples,).\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n\n        if self.scale_data:\n            if self.requires_grad:\n                # Use clone to avoid in-place operations that break gradient flow\n                X_tensor_scaled = self.input_scaler.fit_transform(X_tensor.detach()).clone()\n                X_tensor_scaled.requires_grad_(True)\n                y_tensor_scaled = self.output_scaler.fit_transform(y_tensor.detach()).clone()\n                y_tensor_scaled.requires_grad_(True)\n                X_tensor = X_tensor_scaled\n                y_tensor = y_tensor_scaled\n            else:\n                X_tensor = self.input_scaler.fit_transform(X_tensor)\n                y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        y_tensor = y_tensor.view(-1)\n\n        self.train_X = X_tensor \n        self.train_y = y_tensor\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        model = ExactGP(self.kernel, X_tensor, y_tensor, self.likelihood)\n        self.model = model.to(self.device)\n\n        self.model.train()\n        self.likelihood.train()\n\n        if self.loss_fn == None: \n            self.mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, model)\n            self.loss_fn = self.mll_loss\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        for epoch in range(self.epochs): \n            optimizer.zero_grad()\n            preds = model(X_tensor)\n            loss = self.loss_fn(preds, y_tensor)\n            loss.backward()\n            optimizer.step() \n\n            if scheduler is not None:\n                scheduler.step()\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": loss})\n\n        self._loggers.append(logger)\n        self.fitted=True\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        X_tensor = validate_X_input(X, device=self.device, requires_grad=True)\n        if self.scale_data:\n            if self.requires_grad:\n                # Use clone to avoid in-place operations that break gradient flow\n                X_tensor_scaled = self.input_scaler.transform(X_tensor.detach()).clone()\n                X_tensor_scaled.requires_grad_(True)\n                X_tensor = X_tensor_scaled\n            else:\n                X_tensor = self.input_scaler.transform(X_tensor)\n\n        self.model.eval()\n        self.likelihood.eval() \n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var(): \n            preds = self.likelihood(self.model(X_tensor))\n\n        with torch.no_grad(): \n            mean = preds.mean\n            lower_2std, upper_2std = preds.confidence_region() \n            low_std, up_std = (mean - lower_2std) / 2, (upper_2std - mean) / 2 \n\n        z_score = st.norm.ppf(1 - self.alpha / 2)\n        lower = mean - z_score * low_std\n        upper = mean + z_score * up_std\n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def mll_loss(self, preds, y): \n        \"\"\"\n        Computes the negative log marginal likelihood (default loss function).\n\n        Args:\n            preds (gpytorch.distributions.MultivariateNormal): GP predictive distribution.\n            y (torch.Tensor): Ground truth targets.\n\n        Returns:\n            (torch.Tensor): Negative log marginal likelihood loss.\n        \"\"\"\n        return -torch.sum(self.mll(preds, y))\n\n\n    def save(self, path):\n        \"\"\"\n        Saves model configuration, weights, and training data to disk.\n\n        Args:\n            path (Union[str, Path]): Path to save directory.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"model\", \"kernel\", \"likelihood\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\", \"train_X\", \"train_y\", \"input_scaler\", \"output_scaler\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module, torch.Tensor))\n        }\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.optimizer_cls.__class__.__name__ if self.scheduler_cls is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.kernel, self.likelihood, self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, \n                         self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n        torch.save([self.train_X, self.train_y], path / f\"train.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Loads a saved BBMM_GP model from disk.\n\n        Args:\n            path (Union[str, Path]): Path to saved model directory.\n            device (str): Device to map model to (\"cpu\" or \"cuda\").\n            load_logs (bool): If True, also loads training/tuning logs.\n\n        Returns:\n            (BBMM_GP): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            kernel, likelihood, optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        train_X, train_y = torch.load(path / f\"train.pt\")\n        model.model = ExactGP(kernel, train_X, train_y, likelihood)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.fitted = fitted\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.fit","title":"<code>fit(X, y)</code>","text":"<p>Fits the GP model to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or Tensor</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray or Tensor</code> <p>Training targets of shape (n_samples,).</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fits the GP model to training data.\n\n    Args:\n        X (np.ndarray or torch.Tensor): Training features of shape (n_samples, n_features).\n        y (np.ndarray or torch.Tensor): Training targets of shape (n_samples,).\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n\n    if self.scale_data:\n        if self.requires_grad:\n            # Use clone to avoid in-place operations that break gradient flow\n            X_tensor_scaled = self.input_scaler.fit_transform(X_tensor.detach()).clone()\n            X_tensor_scaled.requires_grad_(True)\n            y_tensor_scaled = self.output_scaler.fit_transform(y_tensor.detach()).clone()\n            y_tensor_scaled.requires_grad_(True)\n            X_tensor = X_tensor_scaled\n            y_tensor = y_tensor_scaled\n        else:\n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    y_tensor = y_tensor.view(-1)\n\n    self.train_X = X_tensor \n    self.train_y = y_tensor\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n\n    config = {\n        \"learning_rate\": self.learning_rate,\n        \"epochs\": self.epochs,\n    }\n\n    logger = Logger(\n        use_wandb=self.use_wandb,\n        project_name=self.wandb_project,\n        run_name=self.wandb_run_name,\n        config=config,\n    )\n\n    model = ExactGP(self.kernel, X_tensor, y_tensor, self.likelihood)\n    self.model = model.to(self.device)\n\n    self.model.train()\n    self.likelihood.train()\n\n    if self.loss_fn == None: \n        self.mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, model)\n        self.loss_fn = self.mll_loss\n\n    optimizer = self.optimizer_cls(\n        model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n    )\n\n    scheduler = None\n    if self.scheduler_cls is not None:\n        scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n    for epoch in range(self.epochs): \n        optimizer.zero_grad()\n        preds = model(X_tensor)\n        loss = self.loss_fn(preds, y_tensor)\n        loss.backward()\n        optimizer.step() \n\n        if scheduler is not None:\n            scheduler.step()\n        if epoch % (self.epochs / 20) == 0:\n            logger.log({\"epoch\": epoch, \"train_loss\": loss})\n\n    self._loggers.append(logger)\n    self.fitted=True\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Loads a saved BBMM_GP model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to saved model directory.</p> required <code>device</code> <code>str</code> <p>Device to map model to (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>If True, also loads training/tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>BBMM_GP</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Loads a saved BBMM_GP model from disk.\n\n    Args:\n        path (Union[str, Path]): Path to saved model directory.\n        device (str): Device to map model to (\"cpu\" or \"cuda\").\n        load_logs (bool): If True, also loads training/tuning logs.\n\n    Returns:\n        (BBMM_GP): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        kernel, likelihood, optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    train_X, train_y = torch.load(path / f\"train.pt\")\n    model.model = ExactGP(kernel, train_X, train_y, likelihood)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.fitted = fitted\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.mll_loss","title":"<code>mll_loss(preds, y)</code>","text":"<p>Computes the negative log marginal likelihood (default loss function).</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>MultivariateNormal</code> <p>GP predictive distribution.</p> required <code>y</code> <code>Tensor</code> <p>Ground truth targets.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Negative log marginal likelihood loss.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def mll_loss(self, preds, y): \n    \"\"\"\n    Computes the negative log marginal likelihood (default loss function).\n\n    Args:\n        preds (gpytorch.distributions.MultivariateNormal): GP predictive distribution.\n        y (torch.Tensor): Ground truth targets.\n\n    Returns:\n        (torch.Tensor): Negative log marginal likelihood loss.\n    \"\"\"\n    return -torch.sum(self.mll(preds, y))\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    X_tensor = validate_X_input(X, device=self.device, requires_grad=True)\n    if self.scale_data:\n        if self.requires_grad:\n            # Use clone to avoid in-place operations that break gradient flow\n            X_tensor_scaled = self.input_scaler.transform(X_tensor.detach()).clone()\n            X_tensor_scaled.requires_grad_(True)\n            X_tensor = X_tensor_scaled\n        else:\n            X_tensor = self.input_scaler.transform(X_tensor)\n\n    self.model.eval()\n    self.likelihood.eval() \n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var(): \n        preds = self.likelihood(self.model(X_tensor))\n\n    with torch.no_grad(): \n        mean = preds.mean\n        lower_2std, upper_2std = preds.confidence_region() \n        low_std, up_std = (mean - lower_2std) / 2, (upper_2std - mean) / 2 \n\n    z_score = st.norm.ppf(1 - self.alpha / 2)\n    lower = mean - z_score * low_std\n    upper = mean + z_score * up_std\n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.save","title":"<code>save(path)</code>","text":"<p>Saves model configuration, weights, and training data to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to save directory.</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Saves model configuration, weights, and training data to disk.\n\n    Args:\n        path (Union[str, Path]): Path to save directory.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"model\", \"kernel\", \"likelihood\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\", \"train_X\", \"train_y\", \"input_scaler\", \"output_scaler\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module, torch.Tensor))\n    }\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.optimizer_cls.__class__.__name__ if self.scheduler_cls is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.kernel, self.likelihood, self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, \n                     self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n    torch.save([self.train_X, self.train_y], path / f\"train.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.ExactGP","title":"<code>ExactGP</code>","text":"<p>               Bases: <code>ExactGP</code></p> <p>A custom GPyTorch Exact Gaussian Process model using a constant mean and a user-specified kernel.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>Kernel defining the covariance structure of the GP.</p> required <code>train_x</code> <code>Tensor</code> <p>Training inputs of shape (n_samples, n_features).</p> required <code>train_y</code> <code>Tensor</code> <p>Training targets of shape (n_samples,).</p> required <code>likelihood</code> <code>Likelihood</code> <p>Likelihood function (e.g., GaussianLikelihood).</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>class ExactGP(gpytorch.models.ExactGP): \n    \"\"\"\n    A custom GPyTorch Exact Gaussian Process model using a constant mean and a user-specified kernel.\n\n    Args:\n        kernel (gpytorch.kernels.Kernel): Kernel defining the covariance structure of the GP.\n        train_x (torch.Tensor): Training inputs of shape (n_samples, n_features).\n        train_y (torch.Tensor): Training targets of shape (n_samples,).\n        likelihood (gpytorch.likelihoods.Likelihood): Likelihood function (e.g., GaussianLikelihood).\n    \"\"\"\n    def __init__(self, kernel, train_x, train_y, likelihood):\n        super(ExactGP, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x): \n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n</code></pre>"},{"location":"api/Bayesian/deep_ens/","title":"uqregressors.bayesian.deep_ens","text":"<p>Deep Ensembles are implemented as in Lakshimnarayanan et al. 2017. </p>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens--deep-ensembles","title":"Deep Ensembles","text":"<p>This module implements Deep Ensemble Regressors for regression of a one dimensional output. </p> Key features are <ul> <li>Customizable neural network architecture </li> <li>Prediction Intervals based on Gaussian assumption </li> <li>Parallel training of ensemble members with Joblib</li> <li>Customizable optimizer and loss function</li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor","title":"<code>DeepEnsembleRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Deep Ensemble Regressor with uncertainty estimation using neural networks.</p> <p>Trains an ensemble of MLP models to predict both mean and variance for regression tasks, and provides predictive uncertainty intervals.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the regressor for config files.</p> <code>'Deep_Ensemble_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of ensemble members.</p> <code>5</code> <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes for each MLP.</p> <code>[64, 64]</code> <code>alpha</code> <code>float</code> <p>Significance level for prediction intervals (e.g., 0.1 for 90% interval).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>If True, returned predictions require gradients.</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>Name of activation function to use (e.g., 'ReLU').</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimizer.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>Optimizer</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Additional kwargs for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>_LRScheduler or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Additional kwargs for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable</code> <p>Loss function accepting (preds, targets).</p> <code>None</code> <code>device</code> <code>str or device</code> <p>Device to run training on ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to use Weights &amp; Biases logging.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>WandB project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>WandB run name prefix.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to train ensemble members.</p> <code>1</code> <code>random_seed</code> <code>int or None</code> <p>Seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to scale input and output data.</p> <code>True</code> <code>input_scaler</code> <code>object or None</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>object or None</code> <p>Scaler for target values.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>List of tuning loggers.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>models</code> <code>list</code> <p>List of trained PyTorch MLP models.</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of input features.</p> <code>_loggers</code> <code>list</code> <p>Training loggers for each model.</p> <code>training_logs</code> <p>Logs from training.</p> <code>tuning_logs</code> <p>Logs from hyperparameter tuning.</p> <code>fitted</code> <code>bool</code> <p>Whether fit has been successfully called</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>class DeepEnsembleRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Deep Ensemble Regressor with uncertainty estimation using neural networks.\n\n    Trains an ensemble of MLP models to predict both mean and variance for regression tasks,\n    and provides predictive uncertainty intervals.\n\n    Args:\n        name (str): Name of the regressor for config files.\n        n_estimators (int): Number of ensemble members.\n        hidden_sizes (list of int): List of hidden layer sizes for each MLP.\n        alpha (float): Significance level for prediction intervals (e.g., 0.1 for 90% interval).\n        requires_grad (bool): If True, returned predictions require gradients.\n        activation_str (str): Name of activation function to use (e.g., 'ReLU').\n        learning_rate (float): Learning rate for optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (torch.optim.Optimizer): Optimizer class.\n        optimizer_kwargs (dict): Additional kwargs for optimizer.\n        scheduler_cls (torch.optim.lr_scheduler._LRScheduler or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Additional kwargs for scheduler.\n        loss_fn (callable): Loss function accepting (preds, targets).\n        device (str or torch.device): Device to run training on ('cpu' or 'cuda').\n        use_wandb (bool): Whether to use Weights &amp; Biases logging.\n        wandb_project (str or None): WandB project name.\n        wandb_run_name (str or None): WandB run name prefix.\n        n_jobs (int): Number of parallel jobs to train ensemble members.\n        random_seed (int or None): Seed for reproducibility.\n        scale_data (bool): Whether to scale input and output data.\n        input_scaler (object or None): Scaler for input features.\n        output_scaler (object or None): Scaler for target values.\n        tuning_loggers (list): List of tuning loggers.\n\n    Attributes:\n        models (list): List of trained PyTorch MLP models.\n        input_dim (int): Dimensionality of input features.\n        _loggers (list): Training loggers for each model.\n        training_logs: Logs from training.\n        tuning_logs: Logs from hyperparameter tuning.\n        fitted (bool): Whether fit has been successfully called\n    \"\"\"\n    def __init__(\n        self,\n        name = \"Deep_Ensemble_Regressor\",\n        n_estimators=5,\n        hidden_sizes=[64, 64],\n        alpha=0.1,\n        requires_grad=False,\n        activation_str=\"ReLU\",\n        learning_rate=1e-3,\n        epochs=200,\n        batch_size=32,\n        optimizer_cls=torch.optim.Adam,\n        optimizer_kwargs=None,\n        scheduler_cls=None,\n        scheduler_kwargs=None,\n        loss_fn=None,\n        device=\"cpu\",\n        use_wandb=False,\n        wandb_project=None,\n        wandb_run_name=None,\n        n_jobs=1,\n        random_seed=None,\n        scale_data=True, \n        input_scaler=None,\n        output_scaler=None, \n        tuning_loggers = [],\n    ):\n        self.name=name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.activation_str = activation_str\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.nll_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n        self.models = []\n        self.input_dim = None\n\n        self.scale_data = scale_data\n\n        if scale_data: \n            self.input_scaler = input_scaler or TorchStandardScaler()\n            self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n        self.fitted = False\n\n    def nll_loss(self, preds, y): \n        \"\"\"\n        Negative log-likelihood loss assuming Gaussian outputs.\n\n        Args:\n            preds (torch.Tensor): Predicted means and variances, shape (batch_size, 2).\n            y (torch.Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (torch.Tensor): Scalar loss value.\n        \"\"\"\n        means = preds[:, 0]\n        variances = preds[:, 1]\n        precision = 1 / variances\n        squared_error = (y.view(-1) - means) ** 2\n        nll = 0.5 * (torch.log(variances) + precision * squared_error)\n        return nll.mean()\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, idx): \n        \"\"\"\n        Train a single ensemble member.\n\n        Args:\n            X_tensor (torch.Tensor): Input tensor.\n            y_tensor (torch.Tensor): Target tensor.\n            input_dim (int): Number of input features.\n            idx (int): Index of the model (for seeding and logging).\n\n        Returns:\n            (Tuple[MLP, Logger]): (trained model, logger instance)\n        \"\"\"\n        X_tensor = X_tensor.to(self.device)\n        y_tensor = y_tensor.to(self.device)\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + idx)\n            np.random.seed(self.random_seed + idx)\n\n        activation = get_activation(self.activation_str)\n        model = MLP(input_dim, self.hidden_sizes, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{idx}\"\n        )\n\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss.item()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n        logger.finish()\n        return model, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (DeepEnsembleRegressor): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, i)\n            for i in range(self.n_estimators)\n        )\n\n        self.models, self._loggers = zip(*results)\n\n        self.fitted = True\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = [] \n\n        for model in self.models: \n            model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n        preds = torch.stack(preds)\n\n        means = preds[:, :, 0]\n        variances = preds[:, :, 1]\n\n        mean = means.mean(dim=0)\n        variance = torch.mean(variances + means ** 2, dim=0) - mean ** 2\n        std = variance.sqrt()\n\n        std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n\n        lower = mean - std * std_mult \n        upper = mean + std * std_mult \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze() \n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained ensemble to disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to save the model and metadata.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \n                         \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved ensemble regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (DeepEnsembleRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = MLP(model.input_dim, config[\"hidden_sizes\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n        model.fitted = fitted\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>DeepEnsembleRegressor</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (DeepEnsembleRegressor): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, i)\n        for i in range(self.n_estimators)\n    )\n\n    self.models, self._loggers = zip(*results)\n\n    self.fitted = True\n    return self\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved ensemble regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>DeepEnsembleRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved ensemble regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (DeepEnsembleRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = MLP(model.input_dim, config[\"hidden_sizes\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n    model.fitted = fitted\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.nll_loss","title":"<code>nll_loss(preds, y)</code>","text":"<p>Negative log-likelihood loss assuming Gaussian outputs.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted means and variances, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss value.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def nll_loss(self, preds, y): \n    \"\"\"\n    Negative log-likelihood loss assuming Gaussian outputs.\n\n    Args:\n        preds (torch.Tensor): Predicted means and variances, shape (batch_size, 2).\n        y (torch.Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (torch.Tensor): Scalar loss value.\n    \"\"\"\n    means = preds[:, 0]\n    variances = preds[:, 1]\n    precision = 1 / variances\n    squared_error = (y.view(-1) - means) ** 2\n    nll = 0.5 * (torch.log(variances) + precision * squared_error)\n    return nll.mean()\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = [] \n\n    for model in self.models: \n        model.eval()\n        pred = model(X_tensor)\n        preds.append(pred)\n\n    preds = torch.stack(preds)\n\n    means = preds[:, :, 0]\n    variances = preds[:, :, 1]\n\n    mean = means.mean(dim=0)\n    variance = torch.mean(variances + means ** 2, dim=0) - mean ** 2\n    std = variance.sqrt()\n\n    std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n\n    lower = mean - std * std_mult \n    upper = mean + std * std_mult \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze() \n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.save","title":"<code>save(path)</code>","text":"<p>Save the trained ensemble to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to save the model and metadata.</p> required Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained ensemble to disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to save the model and metadata.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \n                     \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple multi-layer perceptron which outputs a mean and a positive variance per input sample.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes.</p> required <code>activation</code> <code>Module</code> <p>Activation function class (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple multi-layer perceptron which outputs a mean and a positive variance per input sample.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): List of hidden layer sizes.\n        activation (torch.nn.Module): Activation function class (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, activation):\n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            input_dim = h\n        output_layer = nn.Linear(hidden_sizes[-1], 2)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        outputs = self.model(x)\n        means = outputs[:, 0]\n        unscaled_variances = outputs[:, 1]\n        scaled_variance = F.softplus(unscaled_variances) + 1e-6\n        scaled_outputs = torch.cat((means.unsqueeze(dim=1), scaled_variance.unsqueeze(dim=1)), dim=1)\n\n        return scaled_outputs\n</code></pre>"},{"location":"api/Bayesian/dropout/","title":"uqregressors.bayesian.dropout","text":"<p>Monte Carlo Dropout is implemented as in Gal and Ghahramani 2016. </p>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout--monte-carlo-dropout","title":"Monte Carlo Dropout","text":"<p>This module implements a Monte Carlo (MC) Dropout Regressor for regression on a one dimensional output with uncertainty quantification. It estimates predictive uncertainty by performing multiple stochastic forward passes through a dropout-enabled neural network.</p> Key features are <ul> <li>Customizable neural network architecture </li> <li>Aleatoric uncertainty included with hyperparameter tau</li> <li>Prediction Intervals based on Gaussian assumption </li> <li>Customizable optimizer and loss function</li> <li>Optional Input/Output Normalization</li> </ul> <p>Warning</p> <p>Using hyperparameter optimization to optimize the aleatoric uncertainty hyperparameter tau is often necessary to obtain correct predictive intervals!</p>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor","title":"<code>MCDropoutRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>MC Dropout Regressor with uncertainty estimation using neural networks. </p> <p>This class trains a dropout MLP and takes stochastic forward passes to provide predictive uncertainty intervals. It makes a Gaussian assumption on the output distribution, and often requires tuning of the hyperparameter tau</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model instance.</p> <code>'MC_Dropout_Regressor'</code> <code>hidden_sizes</code> <code>List[int]</code> <p>Hidden layer sizes for the MLP.</p> <code>[64, 64]</code> <code>dropout</code> <code>float</code> <p>Dropout rate to apply after each hidden layer.</p> <code>0.1</code> <code>tau</code> <code>float</code> <p>Precision parameter (used in predictive variance).</p> <code>1000000.0</code> <code>use_paper_weight_decay</code> <code>bool</code> <p>Whether to use paper's theoretical weight decay.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Significance level (1 - confidence level) for prediction intervals.</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether to track gradients in prediction output.</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>Activation function name (e.g., \"ReLU\", \"Tanh\").</p> <code>'ReLU'</code> <code>prior_length_scale</code> <code>float</code> <p>Prior length scale for weight decay (1e-2 in paper implementation).</p> <code>0.01</code> <code>n_samples</code> <code>int</code> <p>Number of stochastic forward passes for prediction.</p> <code>100</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>Optimizer</code> <p>PyTorch optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Optional kwargs to pass to optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>Optional[Callable]</code> <p>Optional learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Optional kwargs for the scheduler.</p> <code>None</code> <code>loss_fn</code> <code>Callable</code> <p>Loss function for training (default: MSE).</p> <code>mse_loss</code> <code>device</code> <code>str</code> <p>Device to run training/prediction on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>If True, logs training to Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str</code> <p>W&amp;B project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str</code> <p>W&amp;B run name.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>Seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to standardize inputs and outputs.</p> <code>True</code> <code>input_scaler</code> <code>Optional[TorchStandardScaler]</code> <p>Custom input scaler.</p> <code>None</code> <code>output_scaler</code> <code>Optional[TorchStandardScaler]</code> <p>Custom output scaler.</p> <code>None</code> <code>tuning_loggers</code> <code>List[Logger]</code> <p>External loggers returned from hyperparameter tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>MLP</code> <p>Trained PyTorch MLP model.</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of input features.</p> <code>_loggers</code> <code>Logger</code> <p>Training logger.</p> <code>training_logs</code> <p>Logs from training.</p> <code>tuning_logs</code> <p>Logs from hyperparameter tuning.</p> <code>fitted</code> <code>bool</code> <p>Whether fit has successfully been called.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>class MCDropoutRegressor(BaseEstimator, RegressorMixin):\n    \"\"\" \n    MC Dropout Regressor with uncertainty estimation using neural networks. \n\n    This class trains a dropout MLP and takes stochastic forward passes to provide predictive uncertainty intervals.\n    It makes a Gaussian assumption on the output distribution, and often requires tuning of the hyperparameter tau\n\n    Args:\n        name (str): Name of the model instance.\n        hidden_sizes (List[int]): Hidden layer sizes for the MLP.\n        dropout (float): Dropout rate to apply after each hidden layer.\n        tau (float): Precision parameter (used in predictive variance).\n        use_paper_weight_decay (bool): Whether to use paper's theoretical weight decay.\n        alpha (float): Significance level (1 - confidence level) for prediction intervals.\n        requires_grad (bool): Whether to track gradients in prediction output.\n        activation_str (str): Activation function name (e.g., \"ReLU\", \"Tanh\").\n        prior_length_scale (float): Prior length scale for weight decay (1e-2 in paper implementation).\n        n_samples (int): Number of stochastic forward passes for prediction.\n        learning_rate (float): Learning rate for the optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (Optimizer): PyTorch optimizer class.\n        optimizer_kwargs (dict): Optional kwargs to pass to optimizer.\n        scheduler_cls (Optional[Callable]): Optional learning rate scheduler class.\n        scheduler_kwargs (dict): Optional kwargs for the scheduler.\n        loss_fn (Callable): Loss function for training (default: MSE).\n        device (str): Device to run training/prediction on (\"cpu\" or \"cuda\").\n        use_wandb (bool): If True, logs training to Weights &amp; Biases.\n        wandb_project (str): W&amp;B project name.\n        wandb_run_name (str): W&amp;B run name.\n        random_seed (Optional[int]): Seed for reproducibility.\n        scale_data (bool): Whether to standardize inputs and outputs.\n        input_scaler (Optional[TorchStandardScaler]): Custom input scaler.\n        output_scaler (Optional[TorchStandardScaler]): Custom output scaler.\n        tuning_loggers (List[Logger]): External loggers returned from hyperparameter tuning.\n\n    Attributes:\n        model (MLP): Trained PyTorch MLP model.\n        input_dim (int): Dimensionality of input features.\n        _loggers (Logger): Training logger.\n        training_logs: Logs from training.\n        tuning_logs: Logs from hyperparameter tuning.\n        fitted (bool): Whether fit has successfully been called.\n    \"\"\"\n    def __init__(\n        self,\n        name=\"MC_Dropout_Regressor\",\n        hidden_sizes=[64, 64],\n        dropout=0.1,\n        tau=1.0e6,\n        use_paper_weight_decay=True,\n        prior_length_scale=1e-2,\n        alpha=0.1,\n        requires_grad=False, \n        activation_str=\"ReLU\",\n        n_samples=100,\n        learning_rate=1e-3,\n        epochs=200,\n        batch_size=32,\n        optimizer_cls=torch.optim.Adam,\n        optimizer_kwargs=None,\n        scheduler_cls=None,\n        scheduler_kwargs=None,\n        loss_fn=torch.nn.functional.mse_loss,\n        device=\"cpu\",\n        use_wandb=False,\n        wandb_project=None,\n        wandb_run_name=None,\n        random_seed=None,\n        scale_data=True, \n        input_scaler=None,\n        output_scaler=None, \n        tuning_loggers = []\n    ):\n        self.name=name\n        self.hidden_sizes = hidden_sizes\n        self.dropout = dropout\n        self.tau = tau\n        self.use_paper_weight_decay = use_paper_weight_decay\n        self.prior_length_scale = prior_length_scale\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.activation_str = activation_str\n        self.n_samples = n_samples\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n\n        self.device = device\n        self.model = None\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n        self.random_seed = random_seed\n        self.input_dim = None\n\n        self.scale_data = scale_data\n        self.input_scaler = input_scaler or TorchStandardScaler()\n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = [] \n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers \n        self.tuning_logs = None\n        self.fitted = False\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the MC Dropout model on training data.\n\n        Args:\n            X (array-like): Training features of shape (n_samples, n_features).\n            y (array-like): Target values of shape (n_samples,).\n        Returns: \n            (MCDropoutRegressor): Fitted model.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        if self.use_paper_weight_decay: \n            l = self.prior_length_scale \n            N = len(X_tensor)\n            p = 1 - self.dropout \n            weight_decay = (p * l ** 2) / (2 * N * self.tau)\n            self.optimizer_kwargs[\"weight_decay\"] = weight_decay\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        model, logger = self._fit_single_model(X_tensor, y_tensor)\n        self._loggers.append(logger)\n        self.fitted = True\n        return self \n\n    def _fit_single_model(self, X_tensor, y_tensor):\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n            \"batch_size\": self.batch_size,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        activation = get_activation(self.activation_str)\n\n        model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n        self.model = model.to(self.device)\n\n        optimizer = self.optimizer_cls(\n            self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self.model.train()\n        for epoch in range(self.epochs):\n            epoch_loss = 0.0\n            for xb, yb in dataloader: \n                optimizer.zero_grad()\n                preds = self.model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss\n\n            if scheduler is not None:\n                scheduler.step()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n        logger.finish()\n\n        return self, logger\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        self.model.train()\n        preds = []\n        with torch.no_grad():\n            for _ in range(self.n_samples):\n                preds.append(self.model(X_tensor))\n        preds = torch.stack(preds)\n        mean = preds.mean(dim=0)\n        variance = torch.var(preds, dim=0) + 1 / self.tau \n        std = variance.sqrt()\n        std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n        lower = mean - std * std_mult\n        upper = mean + std * std_mult\n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean).squeeze()\n            lower = self.output_scaler.inverse_transform(lower).squeeze()\n            upper = self.output_scaler.inverse_transform(upper).squeeze()\n\n        else: \n            mean = mean.squeeze() \n            lower = lower.squeeze() \n            upper = upper.squeeze() \n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save model weights, config, and scalers to disk.\n\n        Args:\n            path (str or Path): Directory to save model components.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \n                         \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved MC dropout regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (MCDropoutRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n\n        model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n        model.fitted = fitted\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the MC Dropout model on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>array - like</code> <p>Target values of shape (n_samples,).</p> required <p>Returns:      (MCDropoutRegressor): Fitted model.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the MC Dropout model on training data.\n\n    Args:\n        X (array-like): Training features of shape (n_samples, n_features).\n        y (array-like): Target values of shape (n_samples,).\n    Returns: \n        (MCDropoutRegressor): Fitted model.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    if self.use_paper_weight_decay: \n        l = self.prior_length_scale \n        N = len(X_tensor)\n        p = 1 - self.dropout \n        weight_decay = (p * l ** 2) / (2 * N * self.tau)\n        self.optimizer_kwargs[\"weight_decay\"] = weight_decay\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    model, logger = self._fit_single_model(X_tensor, y_tensor)\n    self._loggers.append(logger)\n    self.fitted = True\n    return self \n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved MC dropout regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>MCDropoutRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved MC dropout regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (MCDropoutRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n\n    model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n    model.fitted = fitted\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    self.model.train()\n    preds = []\n    with torch.no_grad():\n        for _ in range(self.n_samples):\n            preds.append(self.model(X_tensor))\n    preds = torch.stack(preds)\n    mean = preds.mean(dim=0)\n    variance = torch.var(preds, dim=0) + 1 / self.tau \n    std = variance.sqrt()\n    std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n    lower = mean - std * std_mult\n    upper = mean + std * std_mult\n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean).squeeze()\n        lower = self.output_scaler.inverse_transform(lower).squeeze()\n        upper = self.output_scaler.inverse_transform(upper).squeeze()\n\n    else: \n        mean = mean.squeeze() \n        lower = lower.squeeze() \n        upper = upper.squeeze() \n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.save","title":"<code>save(path)</code>","text":"<p>Save model weights, config, and scalers to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model components.</p> required Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save model weights, config, and scalers to disk.\n\n    Args:\n        path (str or Path): Directory to save model components.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \n                     \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation):\n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            layers.append(nn.Dropout(dropout))\n            input_dim = h\n        layers.append(nn.Linear(hidden_sizes[-1], 1))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"api/Bayesian/gp/","title":"uqregressors.bayesian.gaussian_process","text":"<p>This is a compatibility wrapper of the scikit-learn Gaussian Process Regressor  such that predictions return intervals rather than means and/or standard deviations. </p>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor","title":"<code>GPRegressor</code>","text":"<p>A wrapper for scikit-learn's GaussianProcessRegressor with prediction intervals.</p> <p>This class provides a simplified interface to fit a Gaussian Process (GP) regressor, make predictions with uncertainty intervals, and save/load the model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'GP_Regressor'</code> <code>kernel</code> <code>Kernel</code> <p>Kernel to use for the GP model.</p> <code>RBF()</code> <code>alpha</code> <code>float</code> <p>Significance level for the prediction interval.</p> <code>0.1</code> <code>gp_kwargs</code> <code>dict</code> <p>Additional keyword arguments for GaussianProcessRegressor.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>kernel</code> <code>Kernel</code> <p>Kernel to use in the GP model.</p> <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals (e.g., 0.1 for 90% CI).</p> <code>gp_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the GaussianProcessRegressor.</p> <code>model</code> <code>GaussianProcessRegressor</code> <p>Fitted scikit-learn GP model.</p> <code>fitted</code> <code>bool</code> <p>Whether fit has been successfully called.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>class GPRegressor: \n    \"\"\"\n    A wrapper for scikit-learn's GaussianProcessRegressor with prediction intervals.\n\n    This class provides a simplified interface to fit a Gaussian Process (GP) regressor,\n    make predictions with uncertainty intervals, and save/load the model configuration.\n\n    Args:\n            name (str): Name of the model.\n            kernel (sklearn.gaussian_process.kernels.Kernel): Kernel to use for the GP model.\n            alpha (float): Significance level for the prediction interval.\n            gp_kwargs (dict, optional): Additional keyword arguments for GaussianProcessRegressor.\n\n    Attributes:\n        name (str): Name of the model.\n        kernel (sklearn.gaussian_process.kernels.Kernel): Kernel to use in the GP model.\n        alpha (float): Significance level for confidence intervals (e.g., 0.1 for 90% CI).\n        gp_kwargs (dict): Additional keyword arguments for the GaussianProcessRegressor.\n        model (GaussianProcessRegressor): Fitted scikit-learn GP model.\n        fitted (bool): Whether fit has been successfully called. \n    \"\"\"\n    def __init__(self, name=\"GP_Regressor\", kernel = RBF(), \n                 alpha=0.1, \n                 gp_kwargs=None):\n\n        self.name = name\n        self.kernel = kernel \n        self.alpha = alpha \n        self.gp_kwargs = gp_kwargs or {}\n        self.model = None\n        self.fitted = False\n\n    def fit(self, X, y): \n        \"\"\"\n        Fits the GP model to the input data.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n            y (np.ndarray): Target values of shape (n_samples,).\n        Returns: \n            (GPRegressor): Fitted model.\n        \"\"\"\n        model = GaussianProcessRegressor(kernel=self.kernel, **self.gp_kwargs)\n        model.fit(X, y)\n        self.model = model\n        self.fitted = True\n        return self \n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        preds, std = self.model.predict(X, return_std=True)\n        z_score = st.norm.ppf(1 - self.alpha / 2)\n        mean = preds\n        lower = mean - z_score * std\n        upper = mean + z_score * std\n        return mean, lower, upper\n\n    def save(self, path): \n        \"\"\"\n        Saves the model and its configuration to disk.\n\n        Args:\n            path (Union[str, Path]): Directory where model and config will be saved.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True) \n\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"kernel\", \"model\"]\n            and not callable(v)\n            and not isinstance(v, ())\n        }\n        config[\"kernel\"] = self.kernel.__class__.__name__\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"model.pkl\", 'wb') as file: \n            pickle.dump(self, file)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False): \n        \"\"\"\n        Loads a previously saved GPRegressor from disk.\n\n        Args:\n            path (Union[str, Path]): Path to the directory containing the saved model.\n            device (str, optional): Unused, included for compatibility. Defaults to \"cpu\".\n            load_logs (bool, optional): Unused, included for compatibility. Defaults to False.\n\n        Returns:\n            (GPRegressor): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        with open(path / \"model.pkl\", 'rb') as file: \n            model = pickle.load(file)\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fits the GP model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values of shape (n_samples,).</p> required <p>Returns:      (GPRegressor): Fitted model.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fits the GP model to the input data.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,).\n    Returns: \n        (GPRegressor): Fitted model.\n    \"\"\"\n    model = GaussianProcessRegressor(kernel=self.kernel, **self.gp_kwargs)\n    model.fit(X, y)\n    self.model = model\n    self.fitted = True\n    return self \n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Loads a previously saved GPRegressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory containing the saved model.</p> required <code>device</code> <code>str</code> <p>Unused, included for compatibility. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Unused, included for compatibility. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GPRegressor</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False): \n    \"\"\"\n    Loads a previously saved GPRegressor from disk.\n\n    Args:\n        path (Union[str, Path]): Path to the directory containing the saved model.\n        device (str, optional): Unused, included for compatibility. Defaults to \"cpu\".\n        load_logs (bool, optional): Unused, included for compatibility. Defaults to False.\n\n    Returns:\n        (GPRegressor): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    with open(path / \"model.pkl\", 'rb') as file: \n        model = pickle.load(file)\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    preds, std = self.model.predict(X, return_std=True)\n    z_score = st.norm.ppf(1 - self.alpha / 2)\n    mean = preds\n    lower = mean - z_score * std\n    upper = mean + z_score * std\n    return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.save","title":"<code>save(path)</code>","text":"<p>Saves the model and its configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Directory where model and config will be saved.</p> required Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def save(self, path): \n    \"\"\"\n    Saves the model and its configuration to disk.\n\n    Args:\n        path (Union[str, Path]): Directory where model and config will be saved.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True) \n\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"kernel\", \"model\"]\n        and not callable(v)\n        and not isinstance(v, ())\n    }\n    config[\"kernel\"] = self.kernel.__class__.__name__\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"model.pkl\", 'wb') as file: \n        pickle.dump(self, file)\n</code></pre>"},{"location":"api/Conformal/conformal_ens/","title":"uqregressors.conformal.conformal_ens","text":"<p>This method employs normalized conformal prediction as described in Tibshirani, 2023.  The difficulty measure, \\(\\sigma\\), used for normalization is taken to be the standard deviation of the predictions of all models in an ensemble, while the ensemble mean  is returned as the mean prediction. </p>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens--normalized-conformal-ensemble","title":"Normalized Conformal Ensemble","text":"<p>This module implements normalized conformal ensemble prediction in a split conformal context for regression on a one dimensional output </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Customizable dropout to increase ensemble diversity</li> <li>Prediction intervals without distributional assumptions  </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor","title":"<code>ConformalEnsRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Conformal Ensemble Regressor for uncertainty estimation in regression tasks. </p> <p>This class trains an ensemble of MLP models, and applies normalized conformal prediction on a split calibration set to calibrate prediction intervals. </p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model. </p> <code>'Conformal_Ens_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of models to train. </p> <code>5</code> <code>hidden_sizes</code> <code>list</code> <p>sizes of the hidden layers for each quantile regressor. </p> <code>[64, 64]</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level). </p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient, determines output type.</p> <code>False</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers. </p> <code>None</code> <code>pred_with_dropout</code> <code>bool</code> <p>Whether dropout should be applied at test time, dropout must be non-Null</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function. </p> <code>'ReLU'</code> <code>cal_size</code> <code>float</code> <p>Proportion of training samples to use for calibration, between 0 and 1.  </p> <code>0.2</code> <code>gamma</code> <code>float</code> <p>Stability constant added to difficulty score . </p> <code>0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>mse_loss</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>n_jobs</code> <code>float</code> <p>Number of parallel jobs for training.</p> <code>1</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>models</code> <code>list[QuantNN]</code> <p>A list of the models in the ensemble.</p> <code>residuals</code> <code>Tensor</code> <p>The combined residuals on the calibration set. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member. </p> <code>fitted</code> <code>bool</code> <p>Whether fit has been successfully called.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>class ConformalEnsRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Conformal Ensemble Regressor for uncertainty estimation in regression tasks. \n\n    This class trains an ensemble of MLP models, and applies normalized conformal prediction on a split\n    calibration set to calibrate prediction intervals. \n\n    Args: \n        name (str): Name of the model. \n        n_estimators (int): Number of models to train. \n        hidden_sizes (list): sizes of the hidden layers for each quantile regressor. \n        alpha (float): Miscoverage rate (1 - confidence level). \n        requires_grad (bool): Whether inputs should require gradient, determines output type.\n        dropout (float or None): Dropout rate for the neural network layers. \n        pred_with_dropout (bool): Whether dropout should be applied at test time, dropout must be non-Null\n        activation_str (str): String identifier of the activation function. \n        cal_size (float): Proportion of training samples to use for calibration, between 0 and 1.  \n        gamma (float): Stability constant added to difficulty score . \n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        n_jobs (float): Number of parallel jobs for training.\n        random_seed (int or None): Random seed for reproducibility.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        models (list[QuantNN]): A list of the models in the ensemble.\n        residuals (Tensor): The combined residuals on the calibration set. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n        fitted (bool): Whether fit has been successfully called. \n    \"\"\"\n    def __init__(self, \n                 name=\"Conformal_Ens_Regressor\",\n                 n_estimators=5, \n                 hidden_sizes=[64, 64], \n                 alpha=0.1, \n                 requires_grad=False,\n                 dropout=None,\n                 pred_with_dropout=False,\n                 activation_str=\"ReLU\",\n                 cal_size = 0.2, \n                 gamma = 0,\n                 learning_rate=1e-3,\n                 epochs=200,\n                 batch_size=32,\n                 optimizer_cls=torch.optim.Adam,\n                 optimizer_kwargs=None,\n                 scheduler_cls=None,\n                 scheduler_kwargs=None,\n                 loss_fn=nn.functional.mse_loss,\n                 device=\"cpu\", \n                 use_wandb=False, \n                 wandb_project=None, \n                 wandb_run_name=None, \n                 n_jobs=1, \n                 random_seed=None, \n                 scale_data=True, \n                 input_scaler=None,\n                 output_scaler=None,\n                 tuning_loggers = []\n    ): \n        self.name = name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.dropout = dropout\n        self.pred_with_dropout = pred_with_dropout\n        self.activation_str = activation_str\n        self.cal_size = cal_size\n        self.gamma = gamma\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self.input_dim = None\n        self.conformity_scores = None\n        self.conformity_score = None\n        self.models = []\n        self.residuals = []\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n        self.fitted = False\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, train_idx, cal_idx, model_idx): \n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + model_idx)\n            np.random.seed(self.random_seed + model_idx)\n\n        activation = get_activation(self.activation_str)\n        model = MLP(input_dim, self.hidden_sizes, self.dropout, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(model_idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{model_idx}\"\n        )\n\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss.item()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n        if self.pred_with_dropout: \n            model.train()\n        else: \n            model.eval()\n\n        test_X = X_tensor[cal_idx]\n        cal_preds = model(test_X)\n\n        logger.finish()\n        return model, cal_preds, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (ConformalEnsRegressor): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        train_idx, cal_idx = self._train_test_split(X_tensor, 0.2, self.random_seed)\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n            for i in range(self.n_estimators)\n        )\n\n        self.models = [result[0] for result in results]\n        cal_preds = torch.stack([result[1] for result in results]).squeeze()\n        self._loggers = [result[2] for result in results]\n\n        mean_cal_preds = torch.mean(cal_preds, dim=0).squeeze()\n        var_cal_preds = torch.var(cal_preds, dim=0).squeeze()\n        std_cal_preds = var_cal_preds.sqrt()\n        self.residuals = torch.abs(mean_cal_preds - y_tensor[cal_idx].squeeze())\n\n        self.conformity_scores = self.residuals / (std_cal_preds + self.gamma)\n        self.fitted = True \n        return self \n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"    \n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n+1)) \n        q = min(q, n-1) \n\n        res_quantile = n-q\n        self.conformity_score = torch.topk(self.conformity_scores, res_quantile).values[-1]\n\n        preds = []\n\n        with torch.no_grad(): \n            for model in self.models: \n                if self.pred_with_dropout: \n                    model.train()\n                else: \n                    model.eval()\n                pred = model(X_tensor)\n                preds.append(pred)\n\n        preds = torch.stack(preds)[:, :, 0]\n        mean = torch.mean(preds, dim=0)\n        variances = torch.var(preds, dim=0)\n        stds = variances.sqrt()\n        conformal_widths = self.conformity_score * (stds + self.gamma) \n        lower = mean - conformal_widths \n        upper = mean + conformal_widths \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained model and associated configuration to disk.\n\n        Args:\n            path (str or Path): Directory to save model files.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"residuals\", \"conformity_score\", \"conformity_scores\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        # Save residuals and conformity score\n        torch.save({\n            \"residuals\": self.residuals.cpu(),\n            \"conformity_score\": self.conformity_score, \n            \"conformity_scores\": self.conformity_scores\n        }, path / \"extras.pt\")\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved KFoldCQR model from disk.\n\n        Args:\n            path (str or Path): Directory containing saved model files.\n            device (str): Device to load the model on (\"cpu\" or \"cuda\").\n            load_logs (bool): Whether to also load training logs.\n\n        Returns:\n            (ConformalEnsRegressor): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = MLP(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        # Load extras\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.residuals = extras.get(\"residuals\", None)\n            model.conformity_score = extras.get(\"conformity_score\", None)\n            model.conformity_scores = extras.get(\"conformity_scores\", None)\n        else:\n            model.residuals = None\n            model.conformity_score = None\n            model.conformity_scores = None\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n        model.fitted = fitted\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n\n    def _train_test_split(self, X, cal_size, seed=None):\n        \"\"\"\n        For internal use in calibration splitting only, \n        see uqregressors/utils/torch_sklearn_utils for a global version\n        \"\"\"\n        if seed is not None: \n            torch.manual_seed(seed)\n\n        n = len(X)\n        n_cal = int(np.ceil(cal_size * n))\n        all_idx = np.arange(n)\n        cal_idx = np.random.randint(n, size=n_cal)\n        mask = np.ones(n, dtype=bool)\n        mask[cal_idx] = False \n        train_idx = all_idx[mask] \n        return train_idx, cal_idx\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>ConformalEnsRegressor</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (ConformalEnsRegressor): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    train_idx, cal_idx = self._train_test_split(X_tensor, 0.2, self.random_seed)\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n        for i in range(self.n_estimators)\n    )\n\n    self.models = [result[0] for result in results]\n    cal_preds = torch.stack([result[1] for result in results]).squeeze()\n    self._loggers = [result[2] for result in results]\n\n    mean_cal_preds = torch.mean(cal_preds, dim=0).squeeze()\n    var_cal_preds = torch.var(cal_preds, dim=0).squeeze()\n    std_cal_preds = var_cal_preds.sqrt()\n    self.residuals = torch.abs(mean_cal_preds - y_tensor[cal_idx].squeeze())\n\n    self.conformity_scores = self.residuals / (std_cal_preds + self.gamma)\n    self.fitted = True \n    return self \n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved KFoldCQR model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory containing saved model files.</p> required <code>device</code> <code>str</code> <p>Device to load the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to also load training logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>ConformalEnsRegressor</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved KFoldCQR model from disk.\n\n    Args:\n        path (str or Path): Directory containing saved model files.\n        device (str): Device to load the model on (\"cpu\" or \"cuda\").\n        load_logs (bool): Whether to also load training logs.\n\n    Returns:\n        (ConformalEnsRegressor): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = MLP(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    # Load extras\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.residuals = extras.get(\"residuals\", None)\n        model.conformity_score = extras.get(\"conformity_score\", None)\n        model.conformity_scores = extras.get(\"conformity_scores\", None)\n    else:\n        model.residuals = None\n        model.conformity_score = None\n        model.conformity_scores = None\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n    model.fitted = fitted\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"    \n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n+1)) \n    q = min(q, n-1) \n\n    res_quantile = n-q\n    self.conformity_score = torch.topk(self.conformity_scores, res_quantile).values[-1]\n\n    preds = []\n\n    with torch.no_grad(): \n        for model in self.models: \n            if self.pred_with_dropout: \n                model.train()\n            else: \n                model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n    preds = torch.stack(preds)[:, :, 0]\n    mean = torch.mean(preds, dim=0)\n    variances = torch.var(preds, dim=0)\n    stds = variances.sqrt()\n    conformal_widths = self.conformity_score * (stds + self.gamma) \n    lower = mean - conformal_widths \n    upper = mean + conformal_widths \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.save","title":"<code>save(path)</code>","text":"<p>Save the trained model and associated configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model files.</p> required Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained model and associated configuration to disk.\n\n    Args:\n        path (str or Path): Directory to save model files.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"residuals\", \"conformity_score\", \"conformity_scores\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    # Save residuals and conformity score\n    torch.save({\n        \"residuals\": self.residuals.cpu(),\n        \"conformity_score\": self.conformity_score, \n        \"conformity_scores\": self.conformity_scores\n    }, path / \"extras.pt\")\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__()\n        layers = []\n        for h in hidden_sizes: \n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim=h \n        output_layer = nn.Linear(hidden_sizes[-1], 1)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x): \n        return self.model(x)\n</code></pre>"},{"location":"api/Conformal/cqr/","title":"uqregressors.conformal.cqr","text":"<p>This class implements split conformal quantile regression as described by Romano et al. 2018</p> <p>Tip</p> <p>The quantiles of the underlying quantile regressor can be tuned with the parameters tau_lo and tau_hi as in the paper. This can often result in more efficient intervals. </p>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr--conformalized-quantile-regression-cqr","title":"Conformalized Quantile Regression (CQR)","text":"<p>This module implements CQR in a split conformal context for regression on a one dimensional output </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Tunable quantiles of the underyling quantile regressor</li> <li>Prediction intervals without distributional assumptions  </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor","title":"<code>ConformalQuantileRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Conformalized Quantile Regressor for uncertainty estimation in regression tasks.</p> <p>This class trains one quantile neural network and conformalizes it with split conformal prediction</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'Conformal_Quantile_Regressor'</code> <code>hidden_sizes</code> <code>list</code> <p>Sizes of the hidden layers for each quantile regressor.</p> <code>[64, 64]</code> <code>cal_size</code> <code>float</code> <p>Proportion of training samples to use for calibration, between 0 and 1. </p> <code>0.2</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient.</p> <code>False</code> <code>tau_lo</code> <code>float</code> <p>Lower quantile, defaults to alpha/2.</p> <code>None</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function.</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>quantiles</code> <code>Tensor</code> <p>The lower and upper quantiles for prediction.</p> <code>residuals</code> <code>Tensor</code> <p>The residuals on the calibration set. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member. </p> <code>fitted</code> <code>bool</code> <p>Whether fit has been successfully called.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>class ConformalQuantileRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Conformalized Quantile Regressor for uncertainty estimation in regression tasks.\n\n    This class trains one quantile neural network and conformalizes it with split conformal prediction\n\n    Args:\n        name (str): Name of the model.\n        hidden_sizes (list): Sizes of the hidden layers for each quantile regressor.\n        cal_size (float): Proportion of training samples to use for calibration, between 0 and 1. \n        dropout (float or None): Dropout rate for the neural network layers.\n        alpha (float): Miscoverage rate (1 - confidence level).\n        requires_grad (bool): Whether inputs should require gradient.\n        tau_lo (float): Lower quantile, defaults to alpha/2.\n        activation_str (str): String identifier of the activation function.\n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        quantiles (Tensor): The lower and upper quantiles for prediction.\n        residuals (Tensor): The residuals on the calibration set. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n        fitted (bool): Whether fit has been successfully called. \n    \"\"\"\n    def __init__(\n            self, \n            name=\"Conformal_Quantile_Regressor\",\n            hidden_sizes = [64, 64],\n            cal_size = 0.2, \n            dropout = None, \n            alpha = 0.1, \n            requires_grad = False, \n            tau_lo = None, \n            activation_str=\"ReLU\",\n            learning_rate=1e-3,\n            epochs=200, \n            batch_size=32,\n            optimizer_cls = torch.optim.Adam, \n            optimizer_kwargs=None, \n            scheduler_cls=None, \n            scheduler_kwargs=None, \n            loss_fn=None, \n            device=\"cpu\", \n            use_wandb=False, \n            wandb_project=None,\n            wandb_run_name=None,\n            scale_data=True, \n            input_scaler=None,\n            output_scaler=None, \n            random_seed=None,\n            tuning_loggers = []\n    ):\n        self.name = name\n        self.hidden_sizes = hidden_sizes \n        self.cal_size = cal_size \n        self.dropout = dropout \n        self.alpha = alpha \n        self.requires_grad = requires_grad\n        self.tau_lo = tau_lo or alpha / 2 \n        self.activation_str = activation_str \n        self.learning_rate = learning_rate \n        self.epochs = epochs \n        self.batch_size = batch_size \n        self.optimizer_cls = optimizer_cls \n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.quantile_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.random_seed = random_seed\n\n        self.quantiles = torch.tensor([self.tau_lo, 1-self.tau_lo], device=self.device)\n\n        self.residuals = [] \n        self.conformal_width = None \n        self.input_dim = None\n\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n        self.fitted = False\n\n    def quantile_loss(self, preds, y): \n        \"\"\"\n        Quantile loss used for training the quantile regressor.\n\n        Args:\n            preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n            y (Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (Tensor): Scalar loss.\n        \"\"\"\n        error = y.view(-1, 1) - preds \n        return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the conformal quantile regressor model on training data. \n\n        Args:\n            X (array-like): Training features of shape (n_samples, n_features).\n            y (array-like): Target values of shape (n_samples,).\n        \"\"\"\n        X, y = validate_and_prepare_inputs(X, y, device=self.device)\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X = self.input_scaler.fit_transform(X)\n            y = self.output_scaler.fit_transform(y.reshape(-1, 1))\n\n        X_train, X_cal, y_train, y_cal = train_test_split(X, y, test_size=self.cal_size, random_state=self.random_seed, device=self.device, shuffle=True)\n\n        input_dim = X.shape[1]\n        self.input_dim = input_dim \n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n            \"batch_size\": self.batch_size,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        activation = get_activation(self.activation_str)\n\n        self.model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n        self.model.to(self.device)\n\n        optimizer = self.optimizer_cls(\n            self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self.model.train()\n        for epoch in range(self.epochs):\n            epoch_loss = 0.0\n            for xb, yb in dataloader: \n                optimizer.zero_grad()\n                preds = self.model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss\n\n            if scheduler is not None:\n                scheduler.step()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n        self.model.eval()\n        oof_preds = self.model(X_cal)\n        loss_matrix = (oof_preds - y_cal) * torch.tensor([1, -1], device=self.device)\n        self.residuals = torch.max(loss_matrix, dim=1).values\n\n        logger.finish()\n        self._loggers.append(logger)\n        self.fitted = True\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        self.model.eval()\n\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n + 1))\n        q = min(q, n-1)\n        res_quantile = n-q\n\n        self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = self.model(X_tensor)\n        lower_cq = preds[:, 0].unsqueeze(dim=1)\n        upper_cq = preds[:, 1].unsqueeze(dim=1)\n        lower = lower_cq - self.conformal_width \n        upper = upper_cq + self.conformal_width \n        mean = (lower + upper) / 2 \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean).squeeze()\n            lower = self.output_scaler.inverse_transform(lower).squeeze()\n            upper = self.output_scaler.inverse_transform(upper).squeeze()\n        else: \n            mean = mean.squeeze() \n            lower = lower.squeeze() \n            upper = upper.squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path): \n        \"\"\"\n        Save model weights, config, and scalers to disk.\n\n        Args:\n            path (str or Path): Directory to save model components.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"model\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \"output_scaler\", \"quantiles\", \n                         \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n        torch.save({\n            \"conformal_width\": self.conformal_width, \n            \"residuals\": self.residuals,\n            \"quantiles\": self.quantiles\n        }, path / \"extras.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False): \n        \"\"\"\n        Load a saved MC dropout regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (ConformalQuantileRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n\n        model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.residuals = extras.get(\"residuals\", None)\n            model.conformal_width = extras.get(\"conformal_width\", None)\n            model.quantiles = extras.get(\"quantiles\", None)\n        else:\n            model.residuals = None\n            model.conformal_width = None\n            model.quantiles = None\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n        model.fitted = fitted\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the conformal quantile regressor model on training data. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>array - like</code> <p>Target values of shape (n_samples,).</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the conformal quantile regressor model on training data. \n\n    Args:\n        X (array-like): Training features of shape (n_samples, n_features).\n        y (array-like): Target values of shape (n_samples,).\n    \"\"\"\n    X, y = validate_and_prepare_inputs(X, y, device=self.device)\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X = self.input_scaler.fit_transform(X)\n        y = self.output_scaler.fit_transform(y.reshape(-1, 1))\n\n    X_train, X_cal, y_train, y_cal = train_test_split(X, y, test_size=self.cal_size, random_state=self.random_seed, device=self.device, shuffle=True)\n\n    input_dim = X.shape[1]\n    self.input_dim = input_dim \n\n    config = {\n        \"learning_rate\": self.learning_rate,\n        \"epochs\": self.epochs,\n        \"batch_size\": self.batch_size,\n    }\n\n    logger = Logger(\n        use_wandb=self.use_wandb,\n        project_name=self.wandb_project,\n        run_name=self.wandb_run_name,\n        config=config,\n    )\n\n    activation = get_activation(self.activation_str)\n\n    self.model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n    self.model.to(self.device)\n\n    optimizer = self.optimizer_cls(\n        self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n    )\n\n    scheduler = None\n    if self.scheduler_cls is not None:\n        scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n    dataset = TensorDataset(X_train, y_train)\n    dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n    self.model.train()\n    for epoch in range(self.epochs):\n        epoch_loss = 0.0\n        for xb, yb in dataloader: \n            optimizer.zero_grad()\n            preds = self.model(xb)\n            loss = self.loss_fn(preds, yb)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss\n\n        if scheduler is not None:\n            scheduler.step()\n\n        if epoch % (self.epochs / 20) == 0:\n            logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n    self.model.eval()\n    oof_preds = self.model(X_cal)\n    loss_matrix = (oof_preds - y_cal) * torch.tensor([1, -1], device=self.device)\n    self.residuals = torch.max(loss_matrix, dim=1).values\n\n    logger.finish()\n    self._loggers.append(logger)\n    self.fitted = True\n    return self\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved MC dropout regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>ConformalQuantileRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False): \n    \"\"\"\n    Load a saved MC dropout regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (ConformalQuantileRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n\n    model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.residuals = extras.get(\"residuals\", None)\n        model.conformal_width = extras.get(\"conformal_width\", None)\n        model.quantiles = extras.get(\"quantiles\", None)\n    else:\n        model.residuals = None\n        model.conformal_width = None\n        model.quantiles = None\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n    model.fitted = fitted\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    self.model.eval()\n\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n + 1))\n    q = min(q, n-1)\n    res_quantile = n-q\n\n    self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = self.model(X_tensor)\n    lower_cq = preds[:, 0].unsqueeze(dim=1)\n    upper_cq = preds[:, 1].unsqueeze(dim=1)\n    lower = lower_cq - self.conformal_width \n    upper = upper_cq + self.conformal_width \n    mean = (lower + upper) / 2 \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean).squeeze()\n        lower = self.output_scaler.inverse_transform(lower).squeeze()\n        upper = self.output_scaler.inverse_transform(upper).squeeze()\n    else: \n        mean = mean.squeeze() \n        lower = lower.squeeze() \n        upper = upper.squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.quantile_loss","title":"<code>quantile_loss(preds, y)</code>","text":"<p>Quantile loss used for training the quantile regressor.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted quantiles, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def quantile_loss(self, preds, y): \n    \"\"\"\n    Quantile loss used for training the quantile regressor.\n\n    Args:\n        preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n        y (Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (Tensor): Scalar loss.\n    \"\"\"\n    error = y.view(-1, 1) - preds \n    return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.save","title":"<code>save(path)</code>","text":"<p>Save model weights, config, and scalers to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model components.</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def save(self, path): \n    \"\"\"\n    Save model weights, config, and scalers to disk.\n\n    Args:\n        path (str or Path): Directory to save model components.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"model\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \"output_scaler\", \"quantiles\", \n                     \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n    torch.save({\n        \"conformal_width\": self.conformal_width, \n        \"residuals\": self.residuals,\n        \"quantiles\": self.quantiles\n    }, path / \"extras.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float or None</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float or None): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__() \n        layers = [] \n        for h in hidden_sizes: \n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim = h \n        layers.append(nn.Linear(hidden_sizes[-1], 2))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x): \n        return self.model(x)\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/","title":"uqregressors.conformal.k_fold_cqr","text":"<p>This class implements conformal quantile regression in a K-fold manner to obtain uncertainty estimates which are often conservative, but use the entire dataset available.  This can result in large improvements over split conformal quantile regression, particularly in cases where the dataset is sparse. </p> <p>Tip</p> <p>The quantiles of the underlying quantile regressor can be tuned with the parameters tau_lo and tau_hi as in the paper. This can often result in more efficient intervals. </p> <p>Note</p> <p>K-fold Conformal Quantile Regression can be overly conservative in prediction intervals, particularly in sparse data settings or when the underlying estimator has high variance.</p>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr--k-fold-cqr","title":"K-Fold-CQR","text":"<p>This module implements conformal quantile regression in a K-fold manner for regression of a one dimensional output. </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Tunable quantiles of the underyling regressors</li> <li>Prediction intervals without distributional assumptions </li> <li>Parallel training of ensemble models with Joblib </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR","title":"<code>KFoldCQR</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>K-Fold Conformalized Quantile Regressor for uncertainty estimation in regression tasks.</p> <p>This class trains an ensemble of quantile neural networks using K-Fold cross-validation, and applies conformal prediction to calibrate prediction intervals.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'K_Fold_CQR_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of K-Fold models to train.</p> <code>5</code> <code>hidden_sizes</code> <code>list</code> <p>Sizes of the hidden layers for each quantile regressor.</p> <code>[64, 64]</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient.</p> <code>False</code> <code>tau_lo</code> <code>float</code> <p>Lower quantile, defaults to alpha/2.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for training.</p> <code>1</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function.</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>quantiles</code> <code>Tensor</code> <p>The lower and upper quantiles for prediction.</p> <code>models</code> <code>list[QuantNN]</code> <p>A list of the models in the ensemble.</p> <code>residuals</code> <code>Tensor</code> <p>The combined residuals on the calibration sets. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member. </p> <code>fitted</code> <code>bool</code> <p>Whether fit has been successfully called.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>class KFoldCQR(BaseEstimator, RegressorMixin): \n    \"\"\"\n    K-Fold Conformalized Quantile Regressor for uncertainty estimation in regression tasks.\n\n    This class trains an ensemble of quantile neural networks using K-Fold cross-validation,\n    and applies conformal prediction to calibrate prediction intervals.\n\n    Args:\n        name (str): Name of the model.\n        n_estimators (int): Number of K-Fold models to train.\n        hidden_sizes (list): Sizes of the hidden layers for each quantile regressor.\n        dropout (float or None): Dropout rate for the neural network layers.\n        alpha (float): Miscoverage rate (1 - confidence level).\n        requires_grad (bool): Whether inputs should require gradient.\n        tau_lo (float): Lower quantile, defaults to alpha/2.\n        n_jobs (int): Number of parallel jobs for training.\n        activation_str (str): String identifier of the activation function.\n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        quantiles (Tensor): The lower and upper quantiles for prediction.\n        models (list[QuantNN]): A list of the models in the ensemble.\n        residuals (Tensor): The combined residuals on the calibration sets. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n        fitted (bool): Whether fit has been successfully called. \n    \"\"\"\n    def __init__(\n            self, \n            name=\"K_Fold_CQR_Regressor\",\n            n_estimators=5,\n            hidden_sizes=[64, 64], \n            dropout = None,\n            alpha=0.1, \n            requires_grad=False,\n            tau_lo = None, \n            n_jobs=1, \n            activation_str=\"ReLU\",\n            learning_rate=1e-3,\n            epochs=200,\n            batch_size=32,\n            optimizer_cls=torch.optim.Adam,\n            optimizer_kwargs=None,\n            scheduler_cls=None,\n            scheduler_kwargs=None,\n            loss_fn=None,\n            device=\"cpu\",\n            use_wandb=False,\n            wandb_project=None,\n            wandb_run_name=None,\n            scale_data = True, \n            input_scaler = None, \n            output_scaler = None,\n            random_seed=None, \n            tuning_loggers = []\n    ):\n        self.name = name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.dropout = dropout\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.tau_lo = tau_lo or alpha / 2 \n        self.activation_str = activation_str\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.quantile_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n        self.quantiles = torch.tensor([self.tau_lo, 1-self.tau_lo], device=self.device)\n        self.models = []\n        self.residuals = []\n        self.conformal_width = None\n        self.input_dim = None\n        if self.n_estimators == 1: \n            raise ValueError(\"n_estimators set to 1. To use a single Quantile Regressor, use a non-ensembled Quantile Regressor class\")\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n        self.fitted = False \n\n    def quantile_loss(self, preds, y): \n        \"\"\"\n        Quantile loss used for training the quantile regressors.\n\n        Args:\n            preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n            y (Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (Tensor): Scalar loss.\n        \"\"\"\n        error = y.view(-1, 1) - preds\n        return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, train_idx, cal_idx, model_idx): \n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + model_idx)\n            np.random.seed(self.random_seed + model_idx)\n\n        activation = get_activation(self.activation_str)\n        model = QuantNN(input_dim, self.hidden_sizes, self.dropout, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        X_train = X_tensor.detach()[train_idx]\n        y_train = y_tensor.detach()[train_idx]\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(model_idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{model_idx}\"\n        )\n\n        model.train()\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss \n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n        model.eval()\n        test_X = X_tensor[cal_idx]\n        test_y = y_tensor[cal_idx]\n        oof_preds = model(test_X)\n        loss_matrix =(oof_preds - test_y) * torch.tensor([1.0, -1.0], device=self.device)\n        residuals = torch.max(loss_matrix, dim=1).values\n        logger.finish()\n        return model, residuals, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (KFoldCQR): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n\n        if self.scale_data:\n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        kf = TorchKFold(n_splits=self.n_estimators, shuffle=True)\n\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n            for i, (train_idx, cal_idx) in enumerate(kf.split(X_tensor))\n        )\n\n        self.models = [result[0] for result in results]\n        self.residuals = torch.cat([result[1] for result in results], dim=0).ravel()\n        self._loggers = [result[2] for result in results]\n\n        self.fitted = True\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n + 1))\n        q = min(q, n-1)\n\n        res_quantile = n-q\n\n        self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = [] \n\n        with torch.no_grad(): \n            for model in self.models: \n                model.eval()\n                pred = model(X_tensor)\n                preds.append(pred)\n\n        preds = torch.stack(preds)\n\n        means = torch.mean(preds, dim=2) \n        mean = torch.mean(means, dim=0)\n\n        lower_cq = torch.mean(preds[:, :, 0], dim=0)\n        upper_cq = torch.mean(preds[:, :, 1], dim=0)\n\n        lower = lower_cq - self.conformal_width\n        upper = upper_cq + self.conformal_width\n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained model and associated configuration to disk.\n\n        Args:\n            path (str or Path): Directory to save model files.\n        \"\"\"\n        if not self.fitted: \n            raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"quantiles\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        # Save residuals and conformity score\n        torch.save({\n            \"conformal_width\": self.conformal_width, \n            \"residuals\": self.residuals,\n            \"quantiles\": self.quantiles,\n        }, path / \"extras.pt\")\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                        self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved KFoldCQR model from disk.\n\n        Args:\n            path (str or Path): Directory containing saved model files.\n            device (str): Device to load the model on (\"cpu\" or \"cuda\").\n            load_logs (bool): Whether to also load training logs.\n\n        Returns:\n            (KFoldCQR): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        fitted = config.pop(\"fitted\", False)\n        model = cls(**config)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = QuantNN(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        # Load extras\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.conformal_width = extras.get(\"conformal_width\", None)\n            model.residuals = extras.get(\"residuals\", None)\n            model.quantiles = extras.get(\"quantiles\", None)\n        else:\n            model.conformal_width = None\n            model.residuals = None\n            model.quantiles = None\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler\n        model.output_scaler = output_scaler\n        model.fitted = fitted\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>KFoldCQR</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (KFoldCQR): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n\n    if self.scale_data:\n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    kf = TorchKFold(n_splits=self.n_estimators, shuffle=True)\n\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n        for i, (train_idx, cal_idx) in enumerate(kf.split(X_tensor))\n    )\n\n    self.models = [result[0] for result in results]\n    self.residuals = torch.cat([result[1] for result in results], dim=0).ravel()\n    self._loggers = [result[2] for result in results]\n\n    self.fitted = True\n    return self\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved KFoldCQR model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory containing saved model files.</p> required <code>device</code> <code>str</code> <p>Device to load the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to also load training logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>KFoldCQR</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved KFoldCQR model from disk.\n\n    Args:\n        path (str or Path): Directory containing saved model files.\n        device (str): Device to load the model on (\"cpu\" or \"cuda\").\n        load_logs (bool): Whether to also load training logs.\n\n    Returns:\n        (KFoldCQR): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    fitted = config.pop(\"fitted\", False)\n    model = cls(**config)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = QuantNN(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    # Load extras\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.conformal_width = extras.get(\"conformal_width\", None)\n        model.residuals = extras.get(\"residuals\", None)\n        model.quantiles = extras.get(\"quantiles\", None)\n    else:\n        model.conformal_width = None\n        model.residuals = None\n        model.quantiles = None\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler\n    model.output_scaler = output_scaler\n    model.fitted = fitted\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before predict().\")\n\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n + 1))\n    q = min(q, n-1)\n\n    res_quantile = n-q\n\n    self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = [] \n\n    with torch.no_grad(): \n        for model in self.models: \n            model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n    preds = torch.stack(preds)\n\n    means = torch.mean(preds, dim=2) \n    mean = torch.mean(means, dim=0)\n\n    lower_cq = torch.mean(preds[:, :, 0], dim=0)\n    upper_cq = torch.mean(preds[:, :, 1], dim=0)\n\n    lower = lower_cq - self.conformal_width\n    upper = upper_cq + self.conformal_width\n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.quantile_loss","title":"<code>quantile_loss(preds, y)</code>","text":"<p>Quantile loss used for training the quantile regressors.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted quantiles, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def quantile_loss(self, preds, y): \n    \"\"\"\n    Quantile loss used for training the quantile regressors.\n\n    Args:\n        preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n        y (Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (Tensor): Scalar loss.\n    \"\"\"\n    error = y.view(-1, 1) - preds\n    return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.save","title":"<code>save(path)</code>","text":"<p>Save the trained model and associated configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model files.</p> required Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained model and associated configuration to disk.\n\n    Args:\n        path (str or Path): Directory to save model files.\n    \"\"\"\n    if not self.fitted: \n        raise ValueError(\"Model not yet fit. Please call fit() before save().\")\n\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"quantiles\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    # Save residuals and conformity score\n    torch.save({\n        \"conformal_width\": self.conformal_width, \n        \"residuals\": self.residuals,\n        \"quantiles\": self.quantiles,\n    }, path / \"extras.pt\")\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                    self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.QuantNN","title":"<code>QuantNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple quantile neural network that estimates the lower and upper quantile when trained with a pinball loss function. </p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features </p> required <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes</p> required <code>dropout</code> <code>None or float</code> <p>The dropout probability - None if no dropout</p> required <code>activation</code> <code>Module</code> <p>Activation function class (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>class QuantNN(nn.Module): \n    \"\"\"\n    A simple quantile neural network that estimates the lower and upper quantile when trained\n    with a pinball loss function. \n\n    Args: \n        input_dim (int): Number of input features \n        hidden_sizes (list of int): List of hidden layer sizes\n        dropout (None or float): The dropout probability - None if no dropout\n        activation (torch.nn.Module): Activation function class (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim = h\n        output_layer = nn.Linear(hidden_sizes[-1], 2)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"api/Utils/activations/","title":"uqregressors.utils.activations","text":""},{"location":"api/Utils/activations/#uqregressors.utils.activations--activations","title":"activations","text":""},{"location":"api/Utils/activations/#uqregressors.utils.activations.get_activation","title":"<code>get_activation(name)</code>","text":"<p>A simple method to return neural network activations (Pytorch modules) from their name (string)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The activation function to return </p> required <p>Returns:</p> Type Description <code>Module</code> <p>The activation function as a torch module</p> Source code in <code>uqregressors\\utils\\activations.py</code> <pre><code>def get_activation(name: str):\n    \"\"\"\n    A simple method to return neural network activations (Pytorch modules) from their name (string)\n\n    Args: \n        name (str): The activation function to return \n\n    Returns: \n        (Torch.nn.Module): The activation function as a torch module\n    \"\"\"\n    name = name.lower()\n    activations = {\n        \"relu\": nn.ReLU,\n        \"leaky_relu\": nn.LeakyReLU,\n        \"tanh\": nn.Tanh,\n        \"sigmoid\": nn.Sigmoid,\n        \"gelu\": nn.GELU,\n        \"elu\": nn.ELU,\n        \"selu\": nn.SELU,\n        \"none\": nn.Identity,\n    }\n    if name not in activations:\n        raise ValueError(f\"Unsupported activation: {name}\")\n    return activations[name]\n</code></pre>"},{"location":"api/Utils/data_loader/","title":"uqregressors.utils.data_loader","text":""},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader--data_loader","title":"data_loader","text":"<p>A collection of methods meant to help with dataset loading and cleaning. </p> The most useful user-facing methods are <ul> <li>load_unformatted_dataset</li> <li>clean_dataset </li> <li>validate_dataset</li> </ul>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.clean_dataset","title":"<code>clean_dataset(X, y)</code>","text":"<p>A simple helper method to drop missing or NaN values and reshape y to the correct size</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Input features (n_samples, n_features)</p> required <code>y</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Output targets (n_samples,)</p> required <p>Returns:</p> Name Type Description <code>X_clean</code> <code>ndarray</code> <p>Input features cleaned (n_samples, n_features)</p> <code>y_clean</code> <code>ndarray</code> <p>Output targets cleaned (n_samples, 1)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def clean_dataset(X, y): \n    \"\"\"\n    A simple helper method to drop missing or NaN values and reshape y to the correct size\n\n    Args: \n        X (Union[np.ndarray, pd.DataFrame, pd.Series]): Input features (n_samples, n_features)\n        y (Union[np.ndarray, pd.DataFrame, pd.Series]): Output targets (n_samples,)\n\n    Returns: \n        X_clean (np.ndarray): Input features cleaned (n_samples, n_features)\n        y_clean (np.ndarray): Output targets cleaned (n_samples, 1)\n    \"\"\"\n    X_df = pd.DataFrame(X)\n    y_series = pd.Series(y) if isinstance(y, (np.ndarray, list)) else pd.Series(y.values)\n\n    combined = pd.concat([X_df, y_series], axis=1)\n    combined_clean = combined.dropna()\n\n    X_clean = combined_clean.iloc[:, :-1].astype(np.float32).values\n    y_clean = combined_clean.iloc[:, -1].astype(np.float32).values.reshape(-1, 1)\n\n    return X_clean, y_clean\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.load_arff","title":"<code>load_arff(path)</code>","text":"<p>ARFF file loader.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the ARFF file.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Parsed ARFF data as a DataFrame.</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def load_arff(path):\n    \"\"\"\n    ARFF file loader.\n\n    Args:\n        path (str): Path to the ARFF file.\n\n    Returns:\n        df (pd.DataFrame): Parsed ARFF data as a DataFrame.\n    \"\"\"\n    attributes = []\n    data = []\n    reading_data = False\n\n    with open(path, 'r') as file:\n        for line in file:\n            line = line.strip()\n            if not line or line.startswith('%'):\n                continue\n            if line.lower().startswith('@attribute'):\n                # Example: @attribute age numeric\n                parts = line.split()\n                if len(parts) &gt;= 2:\n                    attributes.append(parts[1])\n            elif line.lower() == '@data':\n                reading_data = True\n            elif reading_data:\n                # Data line\n                row = [x.strip().strip('\"') for x in line.split(',')]\n                data.append(row)\n\n    df = pd.DataFrame(data, columns=attributes)\n    df = df.apply(pd.to_numeric, errors='coerce')  # convert to floats where possible\n    return df.dropna()\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.load_unformatted_dataset","title":"<code>load_unformatted_dataset(path, target_column=None, drop_columns=None)</code>","text":"<p>Load and standardize a dataset from a file. Note that the last column is always assumed to be the target.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset file (CSV, XLSX, ARFF, etc.)</p> required <code>target_column</code> <code>Union[str, int, None]</code> <p>Name or index of the target column. If not provided, it is assumed the last column</p> <code>None</code> <code>drop_columns</code> <code>list</code> <p>Columns to drop (e.g., indices, column names).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Input features (n_samples, n_features)</p> <code>y</code> <code>ndarray</code> <p>Target values (n_samples,)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def load_unformatted_dataset(path, target_column=None, drop_columns=None):\n    \"\"\"\n    Load and standardize a dataset from a file. Note that the last column is always assumed to be the target.\n\n    Args:\n        path (str): Path to the dataset file (CSV, XLSX, ARFF, etc.)\n        target_column (Union[str, int, None]): Name or index of the target column. If not provided, it is assumed the last column\n        drop_columns (list): Columns to drop (e.g., indices, column names).\n\n    Returns:\n        X (np.ndarray): Input features (n_samples, n_features)\n        y (np.ndarray): Target values (n_samples,)\n    \"\"\"\n\n    ext = os.path.splitext(path)[-1].lower()\n\n    if ext == \".csv\":\n        try:\n            df = pd.read_csv(path)\n            if df.shape[1] &lt;= 1:\n                raise ValueError(\"Only one column detected; trying semicolon delimiter.\")\n        except Exception:\n            df = pd.read_csv(path, sep=';')\n    elif ext == \".xlsx\" or ext == \".xls\":\n        df = pd.read_excel(path)\n    elif ext == \".arff\":\n        data = load_arff(path)\n        df = pd.DataFrame(data)\n        # Decode bytes to str if needed\n        for col in df.select_dtypes([object]):\n            df[col] = df[col].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n    elif ext == \".txt\":\n        # Try common delimiters: comma, tab, space\n        for delim in [',', '\\t', r'\\s+']:\n            try:\n                df = pd.read_csv(path, sep=delim, engine='python', header=None)\n                if df.shape[1] &lt; 2:\n                    continue  # unlikely to be valid\n                break\n            except Exception:\n                continue\n        else:\n            raise ValueError(f\"Could not parse .txt file: {path}\")\n    else:\n        raise ValueError(f\"Unsupported file extension: {ext}\")\n\n    df = df.dropna()\n\n    if drop_columns:\n        df.drop(columns=drop_columns, inplace=True)\n\n    if target_column is None:\n        target_column = df.columns[-1]  # default: last column\n\n    y = df[target_column].values.astype(np.float32)\n    X = df.drop(columns=[target_column]).values.astype(np.float32)\n\n    return X, y\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_X_input","title":"<code>validate_X_input(X, input_dim=None, device='cpu', requires_grad=False)</code>","text":"<p>Convert X to a torch.Tensor for inference. Called by regressors before the predict method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data to convert, should have shape (n_samples, n_features)</p> required <code>device</code> <code>str</code> <p>Target device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>requires_grad</code> <code>bool</code> <p>Whether the tensor should track gradients.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction inputs of shape (n_samples, n_features)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_X_input(X, input_dim = None, device=\"cpu\", requires_grad=False):\n    \"\"\"\n    Convert X to a torch.Tensor for inference. Called by regressors before the predict method. \n\n    Args: \n        X (array-like): Input data to convert, should have shape (n_samples, n_features)\n        device (str): Target device ('cpu' or 'cuda').\n        requires_grad (bool): Whether the tensor should track gradients.\n\n    Returns:\n        (torch.Tensor): Prediction inputs of shape (n_samples, n_features)\n    \"\"\"\n    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n        X = X.values\n    elif isinstance(X, list):\n        X = np.array(X)\n    elif isinstance(X, torch.Tensor):\n        pass\n    elif not isinstance(X, np.ndarray):\n        raise TypeError(f\"Unsupported type for X: {type(X)}\")\n\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        elif X.ndim != 2:\n            raise ValueError(f\"X must be 2D. Got shape {X.shape}\")\n        X = torch.tensor(X, dtype=torch.float32)\n\n    if not isinstance(X, torch.Tensor):\n        raise TypeError(\"X could not be converted to a torch.Tensor\")\n\n    if input_dim is not None: \n        if X.shape[1] != input_dim: \n            raise ValueError(f\"Based on the training samples, the number of features of X should be {input_dim}. Got {X.shape[1] = }\")\n\n    X = X.to(device)\n    if requires_grad:\n        X.requires_grad_()\n\n    return X\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_and_prepare_inputs","title":"<code>validate_and_prepare_inputs(X, y, device='cpu', requires_grad=False)</code>","text":"<p>Convert X and y into compatible torch.Tensors for training. Called by regressors before the fit method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Feature matrix. Supports np.ndarray, pd.DataFrame, list, or torch.Tensor.</p> required <code>y</code> <code>array - like</code> <p>Target vector. Supports np.ndarray, pd.Series, list, or torch.Tensor.</p> required <code>device</code> <code>str</code> <p>Device to place tensors on (e.g., 'cpu' or 'cuda').</p> <code>'cpu'</code> <code>requires_grad</code> <code>bool</code> <p>Whether the X tensor should require gradients (for gradient-based inference).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_tensor</code> <code>Tensor</code> <p>Input features of shape (n_samples, n_features)</p> <code>y_tensor</code> <code>Tensor</code> <p>Output targets of shape (n_samples, 1)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_and_prepare_inputs(X, y, device=\"cpu\", requires_grad=False):\n    \"\"\"\n    Convert X and y into compatible torch.Tensors for training. Called by regressors before the fit method. \n\n    Args:\n        X (array-like): Feature matrix. Supports np.ndarray, pd.DataFrame, list, or torch.Tensor.\n        y (array-like): Target vector. Supports np.ndarray, pd.Series, list, or torch.Tensor.\n        device (str): Device to place tensors on (e.g., 'cpu' or 'cuda').\n        requires_grad (bool): Whether the X tensor should require gradients (for gradient-based inference).\n\n    Returns: \n        X_tensor (torch.Tensor): Input features of shape (n_samples, n_features)\n        y_tensor (torch.Tensor): Output targets of shape (n_samples, 1)\n    \"\"\"\n    # --- Convert X ---\n    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n        X = X.values\n    elif isinstance(X, list):\n        X = np.array(X)\n    elif isinstance(X, torch.Tensor):\n        pass  # leave as is\n    elif not isinstance(X, np.ndarray):\n        raise TypeError(f\"Unsupported type for X: {type(X)}\")\n\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        elif X.ndim != 2:\n            raise ValueError(f\"X must be 2D. Got shape {X.shape}\")\n        X = torch.tensor(X, dtype=torch.float32)\n\n    if not isinstance(X, torch.Tensor):\n        raise TypeError(\"X could not be converted to a torch.Tensor\")\n\n    # --- Convert y ---\n    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n        y = y.values\n    elif isinstance(y, list):\n        y = np.array(y)\n    elif isinstance(y, torch.Tensor):\n        pass\n    elif not isinstance(y, np.ndarray):\n        raise TypeError(f\"Unsupported type for y: {type(y)}\")\n\n    if isinstance(y, np.ndarray):\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n        elif y.ndim == 2 and y.shape[1] != 1:\n            raise ValueError(\"y must be 1D or 2D with shape (n, 1)\")\n        y = torch.tensor(y, dtype=torch.float32)\n\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"y could not be converted to a torch.Tensor\")\n\n    # --- Final checks ---\n    if y.ndim == 1:\n        y = y.unsqueeze(1)\n    elif y.ndim == 2 and y.shape[1] != 1:\n        raise ValueError(f\"Expected y to have shape (n_samples,) or (n_samples, 1), but got {y.shape}\")\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(f\"X and y must have the same number of samples. Got {X.shape[0]} and {y.shape[0]}\")\n\n    X = X.to(device)\n    y = y.to(device)\n\n    if requires_grad:\n        X.requires_grad_()\n\n    return X, y\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_dataset","title":"<code>validate_dataset(X, y, name='unnamed')</code>","text":"<p>A simple helper method to validate that a dataset is ready for regression.  Raises errors if X and y are not of the correct shape, or if the dataset contains NaNs or missing values.  If a dataset fails this method, try to apply the clean_dataset method first, and try again. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Input features (n_samples, n_features)</p> required <code>y</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Output targets (n_samples,)</p> required Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_dataset(X, y, name=\"unnamed\"): \n    \"\"\"\n    A simple helper method to validate that a dataset is ready for regression. \n    Raises errors if X and y are not of the correct shape, or if the dataset contains NaNs or missing values. \n    If a dataset fails this method, try to apply the clean_dataset method first, and try again. \n\n    Args: \n        X (Union[np.ndarray, pd.DataFrame, pd.Series]): Input features (n_samples, n_features)\n        y (Union[np.ndarray, pd.DataFrame, pd.Series]): Output targets (n_samples,)\n    \"\"\"\n    print(f\"Summary for: {name} dataset\")\n    print(\"=\" * (21 + len(name)))\n\n    if isinstance(X, pd.DataFrame): \n        X = X.values \n    if isinstance(y, (pd.Series, pd.DataFrame)): \n        y = y.values \n\n    if X.ndim != 2: \n        raise ValueError(\"X must be a 2D array (n_samples, n_features)\")\n    if y.ndim == 2 and y.shape[1] != 1: \n        raise ValueError(\"y must be 1D or a 2D column vector with shape (n_samples, 1)\")\n    if y.ndim &gt; 2: \n        raise ValueError(\"y must be 1D or 2D with a single output\")\n\n    n_samples, n_features = X.shape \n\n    if y.shape[0] != n_samples: \n        raise ValueError(\"X and y must have the same number of samples\")\n\n    if np.isnan(X).any() or np.isnan(y).any(): \n        raise ValueError(\"Dataset contains NaNs or missing values.\")\n\n    if not np.issubdtype(X.dtype, np.floating):\n        raise ValueError(\"X must contain only float values (use float32 or float64)\")\n\n    print(f\"Number of samples: {n_samples}\")\n    print(f\"Number of features: {n_features}\")\n    print(f\"Output shape: {y.shape}\")\n    print(\"Dataset validation passed.\\n\") \n</code></pre>"},{"location":"api/Utils/file_manager/","title":"uqregressors.utils.file_manager","text":""},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager--file_manager","title":"file_manager","text":"<p>Handles saving paths, including saving and loading models and plots. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\n&gt;&gt;&gt; from uqregressors.metrics.metrics import compute_all_metrics\n&gt;&gt;&gt; from uqregressors.utils.file_manager import FileManager\n</code></pre> <pre><code>&gt;&gt;&gt; # Instantiate File Manager\n&gt;&gt;&gt; BASE_PATH = \"C:/.uqregressors\"\n&gt;&gt;&gt; fm = FileManager(BASE_PATH)  # Replace with desired base path\n</code></pre> <pre><code>&gt;&gt;&gt; # Fit a model and compute metrics\n&gt;&gt;&gt; reg = DeepEnsembleRegressor()\n&gt;&gt;&gt; reg.fit(X_train, y_train)\n&gt;&gt;&gt; mean, lower, upper = reg.predict(X_test)\n&gt;&gt;&gt; metrics = compute_all_metrics(\n...     mean, lower, upper, y_test, reg.alpha\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Save model and metrics\n&gt;&gt;&gt; save_path = fm.save_model(\n...     name=\"Deep_Ens\",\n...     model=reg,\n...     metrics=metrics,\n...     X_train=X_train,\n...     y_train=y_train,\n...     X_test=X_test,\n...     y_test=y_test\n... )\n&gt;&gt;&gt; # Will save to: BASE_PATH/models/Deep_Ens\n</code></pre> <pre><code>&gt;&gt;&gt; # Alternatively, specify full path directly\n&gt;&gt;&gt; save_path = fm.save_model(path=\"SAVE_PATH\", model=reg, ...)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load model and metrics\n&gt;&gt;&gt; load_dict = fm.load_model(\n...     reg.__class__, save_path, load_logs=True\n... )\n&gt;&gt;&gt; metrics = load_dict[\"metrics\"]\n&gt;&gt;&gt; loaded_model = load_dict[\"model\"]\n&gt;&gt;&gt; X_test = load_dict[\"X_test\"]\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager","title":"<code>FileManager</code>","text":"<p>FileManager class to handle paths and saving.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Base directory to save files to. Defaults to creating a folder \".uqregressors\" within the Users home path. </p> <code>home() / '.uqregressors'</code> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>Path</code> <p>The base directory as a Path object.</p> <code>model_dir</code> <code>Path</code> <p>The directory \"models\" within the base_dir, where models will be saved and loaded.</p> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>class FileManager:\n    \"\"\"\n    FileManager class to handle paths and saving.\n\n    Args: \n        base_dir (str): Base directory to save files to. Defaults to creating a folder \".uqregressors\" within the Users home path. \n\n    Attributes: \n        base_dir (Path): The base directory as a Path object.\n        model_dir (Path): The directory \"models\" within the base_dir, where models will be saved and loaded.\n    \"\"\"\n    def __init__(self, base_dir=Path.home() / \".uqregressors\"):\n        self.base_dir = Path(base_dir)\n        self.model_dir = self.base_dir / \"models\"\n        self.model_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_timestamped_path(self, model_class_name: str) -&gt; Path:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        return self.model_dir / f\"{model_class_name}_{timestamp}\"\n\n    def get_named_path(self, name: str) -&gt; Path:\n        return self.model_dir / name\n\n    def save_model(\n        self, model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None\n    ) -&gt; Path:\n        \"\"\"\n        Saves a model, along with metrics, and training and testing data\n\n        Args: \n            model (BaseEstimator): The regressor to save. Note that it must implement the save method.\n            name (str): The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.\n            path (str): The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,\n                        a directory with the model class and timestamp is created. \n            metrics (dict): A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.\n            X_train (array-like): Training features. \n            y_train (array-like): Training targets. \n            X_test (array-like): Testing features. \n            y_test (array-like): Testing targets.\n        \"\"\"\n        if name is not None:\n            if path is not None:\n                warnings.warn(f\"Both name and path given. Using named path: {path}\")\n            else: \n                path = self.get_named_path(name)\n        elif path is None:\n            path = self.get_timestamped_path(model.__class__.__name__)\n        else:\n            path = Path(path)\n\n        path.mkdir(parents=True, exist_ok=True)\n\n        if not hasattr(model, \"save\") or not callable(model.save):\n            raise AttributeError(f\"{model.__class__.__name__} must implement `save(path)`\")\n        model.save(path)\n\n        if metrics:\n            with open(path / \"metrics.json\", \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n        for name, array in [(\"X_train\", X_train), (\"y_train\", y_train), (\"X_test\", X_test), (\"y_test\", y_test)]:\n            if array is not None:\n                np.save(path / f\"{name}.npy\", np.array(array))\n\n        print(f\"Model and additional artifacts saved to: {path}\")\n        return path\n\n    def load_model(self, model_class, path=None, name=None, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Loads a model and associated metadata from path\n\n        Args: \n            model_class (BaseEstimator): The class of the model to be loaded. This should match with the class of model which was saved. \n            path (str): The path to the directory in which the model and associated metadata is stored. If not given, name must be given. \n            name (str): The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. \n            device (str): The device, \"cpu\" or \"cuda\" to load the model with. \n            load_logs (bool): Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.\n\n        Returns: \n            (dict): Dictionary of loaded objects with the following keys: \n\n                    model: The loaded model,\n\n                    metrics: The loaded metrics or None if there is no metrics.json file, \n\n                    X_train: The loaded training features or None if there is no X_train.npy file, \n\n                    y_train: The loaded training targets or None if there is no y_train.npy file,\n\n                    X_test: The loaded testing features or None if there is no X_test.npy file, \n\n                    y_test: The loaded testing targets or None if there is no y_test.npy file.\n        \"\"\"\n        if name:\n            path = self.get_named_path(name)\n        elif path:\n            path = Path(path)\n        else:\n            raise ValueError(\"Either `name` or `path` must be specified.\")\n\n        if not path.exists():\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n\n        if not hasattr(model_class, \"load\") or not callable(model_class.load):\n            raise AttributeError(f\"{model_class.__name__} must implement `load(path)`\")\n\n        from torch.serialization import safe_globals\n        with safe_globals([np._core.multiarray._reconstruct, np.ndarray, np.dtype]):\n            model = model_class.load(path, device=device, load_logs=load_logs)\n\n        def try_load(name):\n            f = path / f\"{name}.npy\"\n            return np.load(f) if f.exists() else None\n\n        metrics = None\n        metrics_path = path / \"metrics.json\"\n        if metrics_path.exists():\n            with open(metrics_path) as f:\n                metrics = json.load(f)\n\n        return {\n            \"model\": model,\n            \"metrics\": metrics,\n            \"X_train\": try_load(\"X_train\"),\n            \"y_train\": try_load(\"y_train\"),\n            \"X_test\": try_load(\"X_test\"),\n            \"y_test\": try_load(\"y_test\"),\n        }\n\n    def save_plot(self, fig, model_path, filename=\"plot.png\", show=True, subdir=\"plots\"):\n        \"\"\"\n        A helper method to save plots to a subdirectory within the directory in which the model would be saved. \n\n        Args: \n            fig (matplotlib.figure.Figure): The figure to be saved. \n            model_path (str): The directory in which to create a \"plots\" subdirectory where the image will be saved. \n            filename (str): The filename of the plot to be saved, including the file extension.\n            show (bool): Whether to display the plot after saving it. \n            subdir (str): The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .\n        \"\"\"\n\n        path = Path(model_path)\n        plot_dir = path / subdir\n        plot_dir.mkdir(parents=True, exist_ok=True)\n        save_path = plot_dir / filename\n\n        plt.figure(fig.number)\n        fig.savefig(save_path, bbox_inches='tight')\n\n        if show:\n            plt.show(fig)\n        plt.close(fig)\n        print(f\"Plot saved to {save_path}\")\n        return save_path\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.load_model","title":"<code>load_model(model_class, path=None, name=None, device='cpu', load_logs=False)</code>","text":"<p>Loads a model and associated metadata from path</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>BaseEstimator</code> <p>The class of the model to be loaded. This should match with the class of model which was saved. </p> required <code>path</code> <code>str</code> <p>The path to the directory in which the model and associated metadata is stored. If not given, name must be given. </p> <code>None</code> <code>name</code> <code>str</code> <p>The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. </p> <code>None</code> <code>device</code> <code>str</code> <p>The device, \"cpu\" or \"cuda\" to load the model with. </p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of loaded objects with the following keys: </p> <pre><code>model: The loaded model,\n\nmetrics: The loaded metrics or None if there is no metrics.json file,\n\nX_train: The loaded training features or None if there is no X_train.npy file,\n\ny_train: The loaded training targets or None if there is no y_train.npy file,\n\nX_test: The loaded testing features or None if there is no X_test.npy file,\n\ny_test: The loaded testing targets or None if there is no y_test.npy file.\n</code></pre> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def load_model(self, model_class, path=None, name=None, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Loads a model and associated metadata from path\n\n    Args: \n        model_class (BaseEstimator): The class of the model to be loaded. This should match with the class of model which was saved. \n        path (str): The path to the directory in which the model and associated metadata is stored. If not given, name must be given. \n        name (str): The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. \n        device (str): The device, \"cpu\" or \"cuda\" to load the model with. \n        load_logs (bool): Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.\n\n    Returns: \n        (dict): Dictionary of loaded objects with the following keys: \n\n                model: The loaded model,\n\n                metrics: The loaded metrics or None if there is no metrics.json file, \n\n                X_train: The loaded training features or None if there is no X_train.npy file, \n\n                y_train: The loaded training targets or None if there is no y_train.npy file,\n\n                X_test: The loaded testing features or None if there is no X_test.npy file, \n\n                y_test: The loaded testing targets or None if there is no y_test.npy file.\n    \"\"\"\n    if name:\n        path = self.get_named_path(name)\n    elif path:\n        path = Path(path)\n    else:\n        raise ValueError(\"Either `name` or `path` must be specified.\")\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Path {path} does not exist\")\n\n    if not hasattr(model_class, \"load\") or not callable(model_class.load):\n        raise AttributeError(f\"{model_class.__name__} must implement `load(path)`\")\n\n    from torch.serialization import safe_globals\n    with safe_globals([np._core.multiarray._reconstruct, np.ndarray, np.dtype]):\n        model = model_class.load(path, device=device, load_logs=load_logs)\n\n    def try_load(name):\n        f = path / f\"{name}.npy\"\n        return np.load(f) if f.exists() else None\n\n    metrics = None\n    metrics_path = path / \"metrics.json\"\n    if metrics_path.exists():\n        with open(metrics_path) as f:\n            metrics = json.load(f)\n\n    return {\n        \"model\": model,\n        \"metrics\": metrics,\n        \"X_train\": try_load(\"X_train\"),\n        \"y_train\": try_load(\"y_train\"),\n        \"X_test\": try_load(\"X_test\"),\n        \"y_test\": try_load(\"y_test\"),\n    }\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.save_model","title":"<code>save_model(model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None)</code>","text":"<p>Saves a model, along with metrics, and training and testing data</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>The regressor to save. Note that it must implement the save method.</p> required <code>name</code> <code>str</code> <p>The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,         a directory with the model class and timestamp is created. </p> <code>None</code> <code>metrics</code> <code>dict</code> <p>A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.</p> <code>None</code> <code>X_train</code> <code>array - like</code> <p>Training features. </p> <code>None</code> <code>y_train</code> <code>array - like</code> <p>Training targets. </p> <code>None</code> <code>X_test</code> <code>array - like</code> <p>Testing features. </p> <code>None</code> <code>y_test</code> <code>array - like</code> <p>Testing targets.</p> <code>None</code> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def save_model(\n    self, model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None\n) -&gt; Path:\n    \"\"\"\n    Saves a model, along with metrics, and training and testing data\n\n    Args: \n        model (BaseEstimator): The regressor to save. Note that it must implement the save method.\n        name (str): The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.\n        path (str): The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,\n                    a directory with the model class and timestamp is created. \n        metrics (dict): A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.\n        X_train (array-like): Training features. \n        y_train (array-like): Training targets. \n        X_test (array-like): Testing features. \n        y_test (array-like): Testing targets.\n    \"\"\"\n    if name is not None:\n        if path is not None:\n            warnings.warn(f\"Both name and path given. Using named path: {path}\")\n        else: \n            path = self.get_named_path(name)\n    elif path is None:\n        path = self.get_timestamped_path(model.__class__.__name__)\n    else:\n        path = Path(path)\n\n    path.mkdir(parents=True, exist_ok=True)\n\n    if not hasattr(model, \"save\") or not callable(model.save):\n        raise AttributeError(f\"{model.__class__.__name__} must implement `save(path)`\")\n    model.save(path)\n\n    if metrics:\n        with open(path / \"metrics.json\", \"w\") as f:\n            json.dump(metrics, f, indent=2)\n\n    for name, array in [(\"X_train\", X_train), (\"y_train\", y_train), (\"X_test\", X_test), (\"y_test\", y_test)]:\n        if array is not None:\n            np.save(path / f\"{name}.npy\", np.array(array))\n\n    print(f\"Model and additional artifacts saved to: {path}\")\n    return path\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.save_plot","title":"<code>save_plot(fig, model_path, filename='plot.png', show=True, subdir='plots')</code>","text":"<p>A helper method to save plots to a subdirectory within the directory in which the model would be saved. </p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to be saved. </p> required <code>model_path</code> <code>str</code> <p>The directory in which to create a \"plots\" subdirectory where the image will be saved. </p> required <code>filename</code> <code>str</code> <p>The filename of the plot to be saved, including the file extension.</p> <code>'plot.png'</code> <code>show</code> <code>bool</code> <p>Whether to display the plot after saving it. </p> <code>True</code> <code>subdir</code> <code>str</code> <p>The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .</p> <code>'plots'</code> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def save_plot(self, fig, model_path, filename=\"plot.png\", show=True, subdir=\"plots\"):\n    \"\"\"\n    A helper method to save plots to a subdirectory within the directory in which the model would be saved. \n\n    Args: \n        fig (matplotlib.figure.Figure): The figure to be saved. \n        model_path (str): The directory in which to create a \"plots\" subdirectory where the image will be saved. \n        filename (str): The filename of the plot to be saved, including the file extension.\n        show (bool): Whether to display the plot after saving it. \n        subdir (str): The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .\n    \"\"\"\n\n    path = Path(model_path)\n    plot_dir = path / subdir\n    plot_dir.mkdir(parents=True, exist_ok=True)\n    save_path = plot_dir / filename\n\n    plt.figure(fig.number)\n    fig.savefig(save_path, bbox_inches='tight')\n\n    if show:\n        plt.show(fig)\n    plt.close(fig)\n    print(f\"Plot saved to {save_path}\")\n    return save_path\n</code></pre>"},{"location":"api/Utils/logging/","title":"uqregressors.utils.logging","text":""},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger","title":"<code>Logger</code>","text":"<p>Base Logging class.</p> <p>Parameters:</p> Name Type Description Default <code>use_wandb</code> <code>bool</code> <p>Whether to use weights and biases for logging (Experimental feature, not validated yet).</p> <code>False</code> <code>project_name</code> <code>str</code> <p>The logger project name.</p> <code>None</code> <code>run_name</code> <code>str</code> <p>The logger run name for a given training run. </p> <code>None</code> <code>config</code> <code>dict</code> <p>Dictionary of relevant training parameters, only used if weights and biases is used.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the logger.</p> <code>None</code> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>class Logger:\n    \"\"\"\n    Base Logging class.\n\n    Args: \n        use_wandb (bool): Whether to use weights and biases for logging (Experimental feature, not validated yet).\n        project_name (str): The logger project name.\n        run_name (str): The logger run name for a given training run. \n        config (dict): Dictionary of relevant training parameters, only used if weights and biases is used.\n        name (str): Name of the logger.\n    \"\"\"\n    def __init__(self, use_wandb=False, project_name=None, run_name=None, config=None, name=None):\n        self.use_wandb = use_wandb and _wandb_available\n        self.logs = []\n\n        if self.use_wandb:\n            wandb.init(\n                project=project_name or \"default_project\",\n                name=run_name,\n                config=config or {},\n            )\n            self.wandb = wandb\n        else:\n            self.logger = logging.getLogger(name or f\"Logger-{os.getpid()}\")\n            self.logger.setLevel(logging.INFO)\n\n            if LOGGING_CONFIG[\"print\"] and not self.logger.handlers:\n                ch = logging.StreamHandler()\n                formatter = logging.Formatter(\"[%(name)s] %(message)s\")\n                ch.setFormatter(formatter)\n                self.logger.addHandler(ch)\n\n    def log(self, data: dict):\n        \"\"\"\n        Writes a dictionary to a stored internal log.\n        \"\"\"\n        if self.use_wandb:\n            self.wandb.log(data)\n        else:\n            msg = \", \".join(f\"{k}={v}\" for k, v in data.items())\n            self.logs.append(msg)\n            self.logger.info(msg)\n\n    def save_to_file(self, path, subdir=\"logs\", idx=0, name=\"\"): \n        \"\"\"\n        Saves logs to the logs subdirectory when model.save is called.\n        \"\"\"\n        log_dir = Path(path) / subdir \n        log_dir.mkdir(parents=True, exist_ok=True)\n        with open(log_dir / f\"{name}_{str(idx)}.log\", \"w\", encoding=\"utf-8\") as f: \n            f.write(\"\\n\".join(self.logs))\n\n\n    def finish(self):\n        \"\"\"\n        Finish method for weights and biases logging.\n        \"\"\"\n        if self.use_wandb:\n            self.wandb.finish()\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.finish","title":"<code>finish()</code>","text":"<p>Finish method for weights and biases logging.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def finish(self):\n    \"\"\"\n    Finish method for weights and biases logging.\n    \"\"\"\n    if self.use_wandb:\n        self.wandb.finish()\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.log","title":"<code>log(data)</code>","text":"<p>Writes a dictionary to a stored internal log.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def log(self, data: dict):\n    \"\"\"\n    Writes a dictionary to a stored internal log.\n    \"\"\"\n    if self.use_wandb:\n        self.wandb.log(data)\n    else:\n        msg = \", \".join(f\"{k}={v}\" for k, v in data.items())\n        self.logs.append(msg)\n        self.logger.info(msg)\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.save_to_file","title":"<code>save_to_file(path, subdir='logs', idx=0, name='')</code>","text":"<p>Saves logs to the logs subdirectory when model.save is called.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def save_to_file(self, path, subdir=\"logs\", idx=0, name=\"\"): \n    \"\"\"\n    Saves logs to the logs subdirectory when model.save is called.\n    \"\"\"\n    log_dir = Path(path) / subdir \n    log_dir.mkdir(parents=True, exist_ok=True)\n    with open(log_dir / f\"{name}_{str(idx)}.log\", \"w\", encoding=\"utf-8\") as f: \n        f.write(\"\\n\".join(self.logs))\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.set_logging_config","title":"<code>set_logging_config(print=True)</code>","text":"<p>Sets global logging printing configuration. </p> <p>Parameters:</p> Name Type Description Default <code>print</code> <code>bool</code> <p>If False, disables printing to the terminal for all future Logger instances</p> <code>True</code> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def set_logging_config(print=True): \n    \"\"\"\n    Sets global logging printing configuration. \n\n    Args: \n        print (bool): If False, disables printing to the terminal for all future Logger instances\n    \"\"\"\n    LOGGING_CONFIG[\"print\"] = print\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/","title":"uqregressors.utils.torch_sklearn_utils","text":""},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils--torch_sklearn_utils","title":"torch_sklearn_utils","text":"<p>A collection of sklearn utility functions refactored to work with pytorch tensors. </p> The key functions are <ul> <li>TorchStandardScaler (class)</li> <li>TorchKFold (class)</li> <li>train_test_split (function)</li> </ul> <p>Warning</p> <p>TorchKFold returns the indices of each K-Fold, while train_test_split returns the values in each split.</p>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchKFold","title":"<code>TorchKFold</code>","text":"<p>A class meant to split the data into K-folds for conformalization or cross validation. </p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>The number of folds for data splitting.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting. </p> <code>False</code> <code>random_state</code> <code>int or None</code> <p>Controls shuffling for reproducibility.</p> <code>None</code> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>class TorchKFold:\n    \"\"\"\n    A class meant to split the data into K-folds for conformalization or cross validation. \n\n    Args: \n        n_splits (int): The number of folds for data splitting.\n        shuffle (bool): Whether to shuffle the data before splitting. \n        random_state (int or None): Controls shuffling for reproducibility.\n    \"\"\"\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        if n_splits &lt; 2:\n            raise ValueError(\"n_splits must be at least 2.\")\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def split(self, X):\n        \"\"\"\n        Yield train/test indices for each fold.\n\n        Args:\n            X (torch.Tensor, np.ndarray, or list): Input data with shape (n_samples, ...)\n\n        Yields:\n            (tuple[torch.LongTensor, torch.LongTensor]): train_idx, val_idx; the indices of the training and validation sets for each of the splits. \n        \"\"\"\n        if isinstance(X, torch.Tensor):\n            n_samples = X.shape[0]\n        else:\n            X = np.asarray(X)\n            n_samples = len(X)\n\n        indices = np.arange(n_samples)\n\n        if self.shuffle:\n            rng = np.random.default_rng(self.random_state)\n            rng.shuffle(indices)\n\n        fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n        fold_sizes[:n_samples % self.n_splits] += 1\n        current = 0\n\n        for fold_size in fold_sizes:\n            val_idx = indices[current:current + fold_size]\n            train_idx = np.concatenate([indices[:current], indices[current + fold_size:]])\n            current += fold_size\n\n            yield (\n                torch.from_numpy(train_idx).long(),\n                torch.from_numpy(val_idx).long()\n            )\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchKFold.split","title":"<code>split(X)</code>","text":"<p>Yield train/test indices for each fold.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>torch.Tensor, np.ndarray, or list</code> <p>Input data with shape (n_samples, ...)</p> required <p>Yields:</p> Type Description <code>tuple[LongTensor, LongTensor]</code> <p>train_idx, val_idx; the indices of the training and validation sets for each of the splits.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def split(self, X):\n    \"\"\"\n    Yield train/test indices for each fold.\n\n    Args:\n        X (torch.Tensor, np.ndarray, or list): Input data with shape (n_samples, ...)\n\n    Yields:\n        (tuple[torch.LongTensor, torch.LongTensor]): train_idx, val_idx; the indices of the training and validation sets for each of the splits. \n    \"\"\"\n    if isinstance(X, torch.Tensor):\n        n_samples = X.shape[0]\n    else:\n        X = np.asarray(X)\n        n_samples = len(X)\n\n    indices = np.arange(n_samples)\n\n    if self.shuffle:\n        rng = np.random.default_rng(self.random_state)\n        rng.shuffle(indices)\n\n    fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n    fold_sizes[:n_samples % self.n_splits] += 1\n    current = 0\n\n    for fold_size in fold_sizes:\n        val_idx = indices[current:current + fold_size]\n        train_idx = np.concatenate([indices[:current], indices[current + fold_size:]])\n        current += fold_size\n\n        yield (\n            torch.from_numpy(train_idx).long(),\n            torch.from_numpy(val_idx).long()\n        )\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler","title":"<code>TorchStandardScaler</code>","text":"<p>Standardized scaling to 0 mean values with unit variance.</p> <p>Attributes:</p> Name Type Description <code>mean_</code> <code>float</code> <p>The mean of the data, subtracted from the data during scaling. </p> <code>std_</code> <code>float</code> <p>The standard deviation of the data, by which the data is divided during scaling.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>class TorchStandardScaler:\n    \"\"\"\n    Standardized scaling to 0 mean values with unit variance.\n\n    Attributes: \n        mean_ (float): The mean of the data, subtracted from the data during scaling. \n        std_ (float): The standard deviation of the data, by which the data is divided during scaling.\n    \"\"\"\n    def __init__(self):\n        self.mean_ = None\n        self.std_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fits the standard scaler. \n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (TorchStandardScaler): the scaler with updated mean_ and std_ attributes. \n        \"\"\"\n        self.mean_ = X.mean(dim=0, keepdim=True)\n        self.std_ = X.std(dim=0, unbiased=False, keepdim=True)\n        # Avoid division by zero\n        self.std_[self.std_ &lt; 1e-8] = 1.0\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transforms the standard scaler based on the attributes obtained with the fit method. \n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The scaled data\n        \"\"\"\n        return (X - self.mean_) / self.std_\n\n    def fit_transform(self, X): \n        \"\"\"\n        Performs the fit and transforms the data. A combination of the fit and transform methods.\n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The scaled data\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def inverse_transform(self, X_scaled):\n        \"\"\"\n        Transforms scaled data back to the original scale. \n\n        Args: \n            X_scaled (torch.Tensor): scaled data of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The unscaled data. \n        \"\"\"\n        return X_scaled * self.std_ + self.mean_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.fit","title":"<code>fit(X)</code>","text":"<p>Fits the standard scaler. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>TorchStandardScaler</code> <p>the scaler with updated mean_ and std_ attributes.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def fit(self, X):\n    \"\"\"\n    Fits the standard scaler. \n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (TorchStandardScaler): the scaler with updated mean_ and std_ attributes. \n    \"\"\"\n    self.mean_ = X.mean(dim=0, keepdim=True)\n    self.std_ = X.std(dim=0, unbiased=False, keepdim=True)\n    # Avoid division by zero\n    self.std_[self.std_ &lt; 1e-8] = 1.0\n    return self\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.fit_transform","title":"<code>fit_transform(X)</code>","text":"<p>Performs the fit and transforms the data. A combination of the fit and transform methods.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The scaled data</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def fit_transform(self, X): \n    \"\"\"\n    Performs the fit and transforms the data. A combination of the fit and transform methods.\n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The scaled data\n    \"\"\"\n    self.fit(X)\n    return self.transform(X)\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.inverse_transform","title":"<code>inverse_transform(X_scaled)</code>","text":"<p>Transforms scaled data back to the original scale. </p> <p>Parameters:</p> Name Type Description Default <code>X_scaled</code> <code>Tensor</code> <p>scaled data of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The unscaled data.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def inverse_transform(self, X_scaled):\n    \"\"\"\n    Transforms scaled data back to the original scale. \n\n    Args: \n        X_scaled (torch.Tensor): scaled data of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The unscaled data. \n    \"\"\"\n    return X_scaled * self.std_ + self.mean_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.transform","title":"<code>transform(X)</code>","text":"<p>Transforms the standard scaler based on the attributes obtained with the fit method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The scaled data</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Transforms the standard scaler based on the attributes obtained with the fit method. \n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The scaled data\n    \"\"\"\n    return (X - self.mean_) / self.std_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.train_test_split","title":"<code>train_test_split(X, y, test_size=0.2, device='cpu', random_state=None, shuffle=True)</code>","text":"<p>Split arrays or tensors into training and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Features to be split. </p> required <code>y</code> <code>array - like or Tensor</code> <p>Targets to be split. </p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split (between 0 and 1).</p> <code>0.2</code> <code>random_state</code> <code>int or None</code> <p>Controls the shuffling for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether or not to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>X_train, X_test, y_train, y_test; same type as inputs</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def train_test_split(X, y, test_size=0.2, device=\"cpu\", random_state=None, shuffle=True):\n    \"\"\"\n    Split arrays or tensors into training and test sets.\n\n    Args:\n        X (array-like or torch.Tensor): Features to be split. \n        y (array-like or torch.Tensor): Targets to be split. \n        test_size (float): Proportion of the dataset to include in the test split (between 0 and 1).\n        random_state (int or None): Controls the shuffling for reproducibility.\n        shuffle (bool): Whether or not to shuffle the data before splitting.\n\n    Returns:\n        (Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]): X_train, X_test, y_train, y_test; same type as inputs\n    \"\"\"\n    # Convert to numpy for easy indexing\n    if isinstance(X, torch.Tensor):\n        X_np = X.cpu().numpy()\n        is_tensor = True\n    else:\n        X_np = np.asarray(X)\n        is_tensor = False\n\n    if isinstance(y, torch.Tensor):\n        y_np = y.cpu().numpy()\n    else:\n        y_np = np.asarray(y)\n\n    # Check dimensions\n    if X_np.shape[0] != y_np.shape[0]:\n        raise ValueError(f\"X and y must have the same number of samples. Got {X_np.shape[0]} and {y_np.shape[0]}.\")\n\n    n_samples = X_np.shape[0]\n    n_test = int(n_samples * test_size)\n\n    if random_state is not None:\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = np.random.default_rng()\n\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng.shuffle(indices)\n\n    test_indices = indices[:n_test]\n    train_indices = indices[n_test:]\n\n    X_train, X_test = X_np[train_indices], X_np[test_indices]\n    y_train, y_test = y_np[train_indices], y_np[test_indices]\n\n    if is_tensor:\n        X_train = torch.tensor(X_train, dtype=X.dtype, device=device)\n        X_test = torch.tensor(X_test, dtype=X.dtype, device=device)\n        y_train = torch.tensor(y_train, dtype=y.dtype, device=device)\n        y_test = torch.tensor(y_test, dtype=y.dtype, device=device)\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"examples/conformal_coverage_validation/","title":"Validating Average Coverage of Conformalized Regressors","text":"<p>This notebook demonstrates how to validate that conformalized regressors in the UQRegressors library (ConformalEnsRegressor, ConformalQuantileRegressor, and KFoldCQR) satisfy the average coverage guarantee.</p> <p>We will:</p> <ol> <li>Load a real-world regression dataset</li> <li>Train each conformal regressor</li> <li>Compute the empirical coverage using the <code>coverage</code> metric</li> <li>Compare the empirical coverage to the nominal coverage (1 - alpha)</li> </ol>"},{"location":"examples/conformal_coverage_validation/#1-imports-and-setup","title":"1. Imports and Setup","text":"<p>We import the necessary modules and set up the random seed and device.</p> <pre><code>import numpy as np\nimport torch\nfrom uqregressors.conformal.conformal_ens import ConformalEnsRegressor\nfrom uqregressors.conformal.cqr import ConformalQuantileRegressor\nfrom uqregressors.conformal.k_fold_cqr import KFoldCQR\nfrom uqregressors.metrics.metrics import coverage\nfrom uqregressors.utils.torch_sklearn_utils import train_test_split\nfrom uqregressors.utils.logging import set_logging_config\nimport pandas as pd\nimport requests\nfrom io import StringIO\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nr_seed = 42\nnp.random.seed(r_seed)\ntorch.manual_seed(r_seed)\nset_logging_config(print=False)\n</code></pre>"},{"location":"examples/conformal_coverage_validation/#2-load-and-prepare-the-dataset","title":"2. Load and Prepare the Dataset","text":"<p>We use the UCI Protein Structure dataset, a standard benchmark for regression and uncertainty quantification.</p> <pre><code># Use the datasets listed in bayesian_datasets from validation.ipynb\nimport os\nfrom uqregressors.utils.data_loader import load_unformatted_dataset\nfrom sklearn.preprocessing import StandardScaler\n\n# Define dataset files (relative to a datasets/ folder in the repo)\n\ndatasets_bayesian = {\n    'concrete': 'concrete.xls',\n    'energy': 'energy_efficiency.xlsx',\n    #'kin8nm': 'kin8nm.arff',\n    #'power': 'power_plant.xlsx',\n    'wine': 'winequality-red.csv',\n}\n\nDATASET_PATH = os.path.abspath(os.path.join(os.getcwd(), 'datasets'))\n\ndatasets = {}\n\nfor name, file in datasets_bayesian.items():\n    X, y = load_unformatted_dataset(os.path.join(DATASET_PATH, file))\n    Xs = StandardScaler().fit_transform(X)\n    ys = StandardScaler().fit_transform(y.reshape(-1, 1)).flatten()\n    datasets[name] = (Xs, ys)\n</code></pre>"},{"location":"examples/conformal_coverage_validation/#3-set-nominal-coverage-level","title":"3. Set Nominal Coverage Level","text":"<p>We set the miscoverage rate \\(\\alpha = 0.1\\) for 90% nominal coverage.</p> <pre><code>alpha = 0.1\nnominal_coverage = 1 - alpha\nn_trials = 100\ntest_size = 0.3\n\nprint(f'Nominal coverage: {nominal_coverage:.2%}, Trials: {n_trials}')\n</code></pre> <pre><code>Nominal coverage: 90.00%, Trials: 100\n</code></pre>"},{"location":"examples/conformal_coverage_validation/#4-train-and-evaluate-conformalized-regressors","title":"4. Train and Evaluate Conformalized Regressors","text":"<p>We train each conformal regressor and compute the empirical coverage on the test set.</p> <pre><code>results = {name: {\n    'ConformalEnsRegressor': [],\n    'ConformalQuantileRegressor': [],\n    'KFoldCQR': []\n\n} for name in datasets}\n\n\nfor dataset_name, (X, y) in datasets.items():\n    print(f'\\nDataset: {dataset_name}')\n    for trial in range(n_trials):\n        r_seed = 1000 + trial\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=r_seed)\n\n        # 1. Conformalized Deep Ensemble\n\n        conformal_ens = ConformalEnsRegressor(\n            n_estimators=2,\n            hidden_sizes=[16],\n            alpha=alpha,\n            cal_size=0.5,\n            epochs=8,\n            batch_size=32,\n            device=device,\n            scale_data=True,\n            use_wandb=False,\n            random_seed=r_seed\n        )\n\n        conformal_ens.fit(X_train, y_train)\n        _, lower, upper = conformal_ens.predict(X_test)\n        emp_cov = coverage(lower, upper, y_test)\n        results[dataset_name]['ConformalEnsRegressor'].append(emp_cov)\n\n        # 2. Split Conformal Quantile Regressor\n\n        cqr = ConformalQuantileRegressor(\n            hidden_sizes=[16, 16],\n            cal_size=0.5,\n            alpha=alpha,\n            dropout=0.1,\n            epochs=8,\n            batch_size=32,\n            device=device,\n            scale_data=True,\n            use_wandb=False,\n            random_seed=r_seed,\n        )\n\n        cqr.fit(X_train, y_train)\n        _, lower, upper = cqr.predict(X_test)\n        emp_cov = coverage(lower, upper, y_test)\n        results[dataset_name]['ConformalQuantileRegressor'].append(emp_cov)\n\n        # 3. K-Fold Conformal Quantile Regressor\n        kfcqr = KFoldCQR(\n            hidden_sizes=[16, 16],\n            alpha=alpha,\n            dropout=0.1,\n            epochs=8,\n            batch_size=32,\n            n_estimators=2,\n            device=device,\n            scale_data=True,\n            use_wandb=False,\n            random_seed=r_seed,\n        )\n\n        kfcqr.fit(X_train, y_train)\n        _, lower, upper = kfcqr.predict(X_test)\n        emp_cov = coverage(lower, upper, y_test)\n        results[dataset_name]['KFoldCQR'].append(emp_cov)\n\n    print(f'  Done {n_trials} trials.')\n</code></pre> <pre><code>Dataset: concrete\n  Done 100 trials.\n\nDataset: energy\n  Done 100 trials.\n\nDataset: wine\n  Done 100 trials.\n</code></pre>"},{"location":"examples/conformal_coverage_validation/#5-summary-table","title":"5. Summary Table","text":"<p>We summarize the empirical coverage for each regressor and compare to the nominal coverage.</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Compute mean empirical coverage for each method and dataset\nmean_coverages = {dataset: {model: np.mean(covs) for model, covs in model_results.items()} for dataset, model_results in results.items()}\n\n# Print as a table\nsummary_rows = []\nfor dataset, model_means in mean_coverages.items():\n    for model, mean_cov in model_means.items():\n        summary_rows.append({'Dataset': dataset, 'Regressor': model, 'Mean Empirical Coverage': f'{mean_cov:.3f}'})\n\nsummary_df = pd.DataFrame(summary_rows)\n\nprint('Empirical Average Coverage Table:')\ndisplay(summary_df.pivot(index='Dataset', columns='Regressor', values='Mean Empirical Coverage'))\n\n# Define a color map for models\nmodel_colors = {\n    'ConformalEnsRegressor': 'tab:blue',\n    'ConformalQuantileRegressor': 'tab:orange',\n    'KFoldCQR': 'tab:green',\n}\n\n# Plot histograms and vertical lines for mean and nominal coverage (color-matched)\nfig, axes = plt.subplots(len(datasets), 1, figsize=(10, 5 * len(datasets)), sharey=True)\n\nif len(datasets) == 1:\n    axes = [axes]\n\nfor i, (ax, (dataset_name, model_results)) in enumerate(zip(axes, results.items())):\n    for model, covs in model_results.items():\n        color = model_colors.get(model, None)\n\n        ax.hist(covs, bins=8, alpha=0.3, label=model, edgecolor='black', color=color)\n\n        # Add vertical line for mean empirical coverage\n        mean_cov = np.mean(covs)\n        ax.axvline(mean_cov, linestyle='--', linewidth=2, color=color, label=f'{model} Mean: {mean_cov:.3f}')\n\n        # Add vertical line for nominal coverage in the same color\n        ax.axvline(nominal_coverage, color='r', linestyle='-', linewidth=2, alpha=0.7, label=f'{model} Nominal: {nominal_coverage:.2f}')\n\n    ax.set_title(dataset_name)\n    ax.set_xlabel('Empirical Coverage')\n    ax.set_xlim(0.7, 1.0)\n    ax.legend()\n\naxes[0].set_ylabel('Frequency')\n\nplt.suptitle('Empirical Coverage Distributions (100 trials per dataset)')\n\nplt.tight_layout(rect=[0, 0, 1, 0.95])\n\nplt.show()\n</code></pre> <pre><code>Empirical Average Coverage Table:\n</code></pre> Regressor ConformalEnsRegressor ConformalQuantileRegressor KFoldCQR Dataset concrete 0.899 0.904 0.918 energy 0.903 0.909 0.932 wine 0.898 0.900 0.910 <p></p>"},{"location":"examples/conformal_coverage_validation/#6-interpretation","title":"6. Interpretation","text":"<p>All three conformalized regressors achieve empirical coverage close to the nominal level (90% in this example), validating the average coverage guarantee of conformal prediction. Note that K-fold Conformal Quantile Regression can often be conservative. This occurs if the underlying regressors have high variance, resulting in the ensemble prediction being much better than that of individual regressors. Because the calibration set was based off of the residuals from single model predictions, K-Fold CQR can sometimes choose overly conservative intervals. </p>"},{"location":"examples/getting_started/","title":"Getting Started:","text":"<p>This is an example script to demonstrate the capabilities of UQregressors with some examples that you can copy and paste to start generating results in a matter of minutes. If you are considering whether to use this package and do not need a detailed implementation and explanation yet, please check the QuickStart examples page. </p> <p>There are five main capabilities of UQRegessors: </p> <ol> <li>Dataset loading and validation </li> <li>Regression using models of various types created with UQ capability</li> <li>Hyperparameter Tuning using bayesian optimization (wrapper around Optuna)</li> <li>Metrics for evaluating goodness of fit and quality of uncertainty intervals</li> <li>Visualization of metrics, goodness of fit, and quality of uncertainty intervals</li> </ol> <p>This script demonstrates basic usage of each of these five features by creating a data set from realizations of a 1-dimensional sine wave generator with some small added noise, the magnitude of which varies with the input coordinate. </p>"},{"location":"examples/getting_started/#dataset-generation-validation","title":"Dataset Generation / Validation","text":"<p>As an example for this notebook, we will draw samples from a sine function with small added noise that scales with x.  </p> <p>A dataset is considered to be a sequence of input values (<code>x</code>) of shape (n_samples, n_features), and a one dimensional target (<code>y</code>), which is contained in a 2D array of shape (n_samples, 1). </p> <p>We introduce the methods <code>clean_dataset</code> and <code>validate_dataset</code> to deal with missing values and to verify that the inputs and targets are shaped correctly and have the same number of samples. <code>validate_dataset</code> should be called each time a new dataset is loaded. If <code>validate_dataset</code> fails, we can call <code>clean_dataset</code> before in order to coerce <code>x</code> and <code>y</code> into the right form. Additionally, we generate a test set of data samples to evaluate on. </p> <pre><code>import numpy as np\nimport torch \nfrom uqregressors.utils.data_loader import clean_dataset, validate_dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns # For visualization\nplt.rcParams['font.size'] = 20\n\n# Set Random Seed for Reproducibility\nseed = 42 \nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrng = np.random.RandomState(seed)\n\n# Define a data generator function to generate targets from features\ndef true_function(x, beta=0.1):\n    noise = beta * x * np.random.standard_normal((len(x), 1))\n    return np.sin(2 * np.pi * x) + noise\n\nn_test = 250 \nn_train = 150\n\nX_test = np.linspace(0, 1, n_test).reshape(-1, 1)\ny_test = true_function(X_test)\ny_noiseless = true_function(X_test, beta=0)\n\nX_train = np.sort(rng.rand(n_train, 1))\ny_train = true_function(X_train).ravel() \n\n# clean_dataset drops missing or NaN values and reshapes X and y to 2D np arrays\nX_train, y_train = clean_dataset(X_train, y_train)\n\n# Confirm the shapes of X and y, and that there are no missing or NaN values\nvalidate_dataset(X_train, y_train, name=\"Synthetic Sine\")\n</code></pre> <pre><code>Summary for: Synthetic Sine dataset\n===================================\nNumber of samples: 150\nNumber of features: 1\nOutput shape: (150, 1)\nDataset validation passed.\n</code></pre> <p>We also define a plotting function that can be used to visualize regressor results: </p> <pre><code>sns.set(style=\"whitegrid\", font_scale=1.5)\ncolors = sns.color_palette(\"deep\")\n# Seaborn colors\ncolor_true = colors[3]    # blue\ncolor_train = colors[1]   # orange\ncolor_test = colors[2]    # green\ncolor_mean = colors[0]    # red\ncolor_interval = colors[0]  # purple or teal depending on palette\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, y_noiseless, color=color_true, linestyle='--', linewidth=2, label=\"True Function\")\nplt.scatter(X_train, y_train, color=color_train, alpha=0.9, s=30, label=\"Training Data\")\nplt.scatter(X_test, y_test, color=color_test, alpha=0.9, s=15, label=\"Testing Data\")\nplt.legend()\nplt.show()\n\ndef plot_uncertainty_results(mean, lower, upper, model_name): \n    plt.figure(figsize=(10, 6))\n\n    # Plot true function\n    plt.plot(X_test, y_noiseless, color=color_true, linestyle='--', linewidth=2, label=\"True Function\")\n\n    # Training and testing data\n    plt.scatter(X_train, y_train, color=color_train, alpha=0.9, s=30, label=\"Training Data\")\n    plt.scatter(X_test, y_test, color=color_test, alpha=0.9, s=15, label=\"Testing Data\")\n\n    # Predicted mean and uncertainty\n    plt.plot(X_test, mean, color=color_mean, linewidth=2, label=\"Predicted Mean\")\n    plt.fill_between(X_test.ravel(), lower, upper, color=color_interval, alpha=0.4, label=\"Uncertainty Interval\")\n\n    # Plot settings\n    plt.title(f\"{model_name} Uncertainty Test\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.legend(frameon=False)\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p></p>"},{"location":"examples/getting_started/#regressors","title":"Regressors","text":"<p>Regressors are models which predict <code>y</code> from <code>x</code>. Regressors follow the scikit-learn API, where they are first initialized with all relevant settings, then optimized to fit the training data with the <code>fit(X, y)</code> function. New predictions are made with the <code>predict(X)</code> method, which will return the Tuple <code>(mean, lower, upper)</code>, where each of these elements is a one dimensional array containing the mean prediction, the predicted lower bound, and the predicted upper bound. Confidence is controlled with the <code>alpha</code> parameter, where the confidence level is 1 - <code>alpha</code>. For example, to construct 95% confidence intervals, set <code>alpha=0.05</code>. </p> <p>Each regressor also has a <code>save</code> and <code>load</code> method, which stores the regressor parameters, along with any metrics, training, and testing data to disk. These functions are explored in detail in other example files. Each type of regressor currently implemented is fit to the sine function above, and visualized. </p> <p>These examples solely describe the implementation and some key parameters of the regressor types. A detailed description of each regressor type is available in the Regressor Details section of the documentation. </p>"},{"location":"examples/getting_started/#mc-dropout","title":"MC Dropout","text":"<pre><code>from uqregressors.bayesian.dropout import MCDropoutRegressor\nfrom uqregressors.utils.logging import set_logging_config\n\nset_logging_config(print=False) # Disable logging for all future regressors for cleanliness\n\ndropout = MCDropoutRegressor(\n    hidden_sizes=[100, 100],\n    dropout=0.1, # Dropout probability before each layer\n    alpha=0.1,  # 90% confidence\n    tau=1e6, # Aleatoric Uncertainty; should be tuned to provide accurate intervals\n    n_samples=100, # Number of forward passes during predictions\n    scale_data=True, # Internally standardizes the data before training and prediction\n    epochs=1000,\n    learning_rate=1e-3,\n    device=\"cpu\",  # use \"cuda\" if GPU available\n    use_wandb=False # Weights and biases logging as an experimental feature\n)\n\n# sklearn fit and predict API\ndropout.fit(X_train, y_train)\ndropout_sol = dropout.predict(X_test) # dropout_sol = (mean_prediction, lower_bound, upper_bound)\n\nplot_uncertainty_results(*dropout_sol, \"MC Dropout Regressor\")\n</code></pre>"},{"location":"examples/getting_started/#deep-ensemble","title":"Deep Ensemble","text":"<pre><code>from uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\n\ndeep_ens = DeepEnsembleRegressor(\n    n_estimators=5, # Number of estimators to use within the ensemble\n    hidden_sizes=[100, 100],\n    alpha=0.1,\n    scale_data=True,\n    epochs=1000,\n    learning_rate=1e-3,\n    device=\"cpu\", \n    n_jobs=1, # Experimental: Number of parallel jobs using joblib\n    use_wandb=False)\n\ndeep_ens.fit(X_train, y_train)\ndeep_ens_sol = deep_ens.predict(X_test)\n\nplot_uncertainty_results(*deep_ens_sol, \"Deep Ensemble Regressor\")\n</code></pre>"},{"location":"examples/getting_started/#standard-gaussian-process-regression-gpr","title":"Standard Gaussian Process Regression (GPR)","text":"<pre><code>from uqregressors.bayesian.gaussian_process import GPRegressor\nfrom sklearn.gaussian_process.kernels import RBF, WhiteKernel\n\ngp_kwargs = {\"normalize_y\": True}\ngpr = GPRegressor(\n    kernel= RBF(length_scale=0.2, length_scale_bounds=(0.05, 1)) + WhiteKernel(noise_level=1), # Kernel function supported by sklearn\n    alpha=0.1, \n    gp_kwargs=gp_kwargs # keyword arguments to sklearn's GPRegressor\n    )\n\ngpr.fit(X_train, y_train)\ngp_sol = gpr.predict(X_test)\nplot_uncertainty_results(*gp_sol, \"Gaussian Process Regressor\")\n</code></pre>"},{"location":"examples/getting_started/#bbmm-gaussian-process","title":"BBMM Gaussian Process","text":"<pre><code>from uqregressors.bayesian.bbmm_gp import BBMM_GP\nimport gpytorch\n\nbbmm_gp = BBMM_GP(kernel=gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), # gpytorch kernel\n                           likelihood=gpytorch.likelihoods.GaussianLikelihood(), # gpytorch likelihood\n                           alpha = 0.1,\n                           epochs=1000,\n                           learning_rate=1e-1,\n                           device=\"cpu\",\n                           use_wandb=False)\n\nbbmm_gp.fit(X_train, y_train)\nbbmm_gp_sol = bbmm_gp.predict(X_test)\nplot_uncertainty_results(*bbmm_gp_sol, \"BBMM Gaussian Process Regressor\")\n</code></pre>"},{"location":"examples/getting_started/#split-conformal-quantile-regression","title":"Split Conformal Quantile Regression","text":"<pre><code>from uqregressors.conformal.cqr import ConformalQuantileRegressor \n\ncqr = ConformalQuantileRegressor(hidden_sizes = [100, 100], \n                                 cal_size=0.2, # Proportion of training data to use for conformal calibration\n                                 alpha=0.1, \n                                 tau_lo=0.05, # Lower quantile the underlying regressor is trained for; can be tuned\n                                 dropout=None, # Dropout probability in the underlying neural network (only during training)\n                                 epochs=2500, \n                                 learning_rate=1e-3, \n                                 device=\"cpu\", \n                                 use_wandb=False \n                                 )\n\ncqr.fit(X_train, y_train)\ncqr_sol = cqr.predict(X_test)\n</code></pre> <pre><code>plot_uncertainty_results(*cqr_sol, \"Split Conformal Quantile Regression\")\n</code></pre>"},{"location":"examples/getting_started/#k-fold-conformal-quantile-regression","title":"K-fold Conformal Quantile Regression","text":"<pre><code>from uqregressors.conformal.k_fold_cqr import KFoldCQR\n\nk_fold_cqr = KFoldCQR(\n    n_estimators=5, # Number of models in the ensemble\n    hidden_sizes=[100, 100],\n    alpha=0.1, \n    tau_lo=0.05, # Lower quantile the underlying regressor is trained for; can be tuned\n    dropout=None,\n    epochs=2500,\n    learning_rate=1e-3,\n    device=\"cpu\",\n    n_jobs=1, # Experimental: number of parallel processes using joblib\n    use_wandb=False)\n\nk_fold_cqr.fit(X_train, y_train)\nk_fold_cqr_sol = k_fold_cqr.predict(X_test)\n</code></pre> <pre><code>plot_uncertainty_results(*k_fold_cqr_sol, \"K-Fold Conformal Quantile Regression\")\n</code></pre>"},{"location":"examples/getting_started/#normalized-conformalens","title":"Normalized ConformalEns","text":"<pre><code>from uqregressors.conformal.conformal_ens import ConformalEnsRegressor\n\nconformal_ens = ConformalEnsRegressor(\n    n_estimators=5, \n    hidden_sizes=[100, 100],\n    alpha=0.1,\n    cal_size=0.2,\n    epochs=1000,\n    gamma=0, # Normalization constant added for stability; can be tuned\n    dropout=None,\n    learning_rate=1e-3,\n    device=\"cpu\",\n    n_jobs=1,\n    use_wandb=False)\n\nconformal_ens.fit(X_train, y_train)\nconformal_ens_sol = conformal_ens.predict(X_test)\n\nplot_uncertainty_results(*conformal_ens_sol, \"Normalized Conformal Ensemble\")\n</code></pre>"},{"location":"examples/getting_started/#metrics","title":"Metrics","text":"<p>Several metrics can be calculated from the predicted and true values on the test set. If just computing metrics for one model, see the previous example script for usage of the method: <code>compute_all_metrics</code>. Otherwise, to graphically compare the metrics of several models, use the method <code>plot_metrics_comparisons</code>. For details on each of the metrics, see the metrics example. </p> <pre><code>from uqregressors.utils.file_manager import FileManager\nfrom uqregressors.plotting.plotting import plot_metrics_comparisons\nfrom pathlib import Path\n\nsns.set(style=\"whitegrid\", font_scale=1)\n\nsol_dict = {\"MC Dropout\": dropout_sol, \n            \"Deep Ensemble Regressor\": deep_ens_sol, \n            \"Standard GP\": gp_sol, \n            \"BBMM GP\": bbmm_gp_sol, \n            \"Split CQR\": cqr_sol, \n            \"K-fold-CQR\": k_fold_cqr_sol, \n            \"Normalized Conformal Ens\": conformal_ens_sol\n            }\n\nplot_metrics_comparisons(sol_dict, \n                         y_test, \n                         alpha=0.1, \n                         show=True, \n                         save_dir=Path.home()/\".uqregressors\"/\"metrics_curve_tests\", \n                         log_metrics=[], # Which metrics to display on a log scale\n                         filename=\"metrics_test.png\")\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\rmse_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\rmse_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\coverage_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\coverage_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\average_interval_width_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\average_interval_width_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\interval_score_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\interval_score_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\nll_gaussian_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\nll_gaussian_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\error_width_corr_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\error_width_corr_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_under_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_under_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\lowest_group_coverage_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\lowest_group_coverage_metrics_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/metrics_curve_tests')\n</code></pre>"},{"location":"examples/getting_started/#visualization","title":"Visualization","text":""},{"location":"examples/getting_started/#calibration-curves","title":"Calibration Curves","text":"<p>Generates a calibration curve for the model. This sweeps the predictions through a range of confidence levels and evaluates how close the coverage given by the predicted intervals is to the desired confidence level. The two methods useful here are <code>generate_cal_curve</code>, which outputs the data needed for plotting the calibration curve, and <code>plot_cal_curve</code>, which plots the calibration curve. </p> <pre><code>from uqregressors.plotting.plotting import generate_cal_curve, plot_cal_curve\nfrom pathlib import Path\n\n\"\"\"\nGenerate data with generate_cal_curve. If true, refit will re-train the \nmodel for each confidence level (only necessary for quantile regressors)\n\nReturns desired coverage, empirical coverage, and average interval width.\n\"\"\"\n\ndes_cov, emp_cov, avg_width = generate_cal_curve(dropout, X_test, y_test, \n                                                 refit=False, X_train=X_train, \n                                                 y_train=y_train)\n\n\nplot_cal_curve(des_cov, \n               emp_cov, \n               show=True, \n               save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n               filename=\"dropout_test.png\", \n               title=\"Calibration Curve: Dropout\")\n</code></pre> <pre><code>Model and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp_zma25_5\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpkntl7um3\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp8w3s_uc5\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpjix8a44u\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpad5mm5tn\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpr79nkajn\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpxwxo8guq\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp0y07es5r\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp0u9sh1vx\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpl6g1sfq1\\models\\MCDropoutRegressor_20250703_150234\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\nSaved calibration curve to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests/plots/dropout_test.png')\n</code></pre>"},{"location":"examples/getting_started/#predicted-vs-true-values","title":"Predicted vs. True Values","text":"<p>The method <code>plot_pred_vs_true</code> plots the predicted values against the true values in the test set, with the option to include the predicted confidence intervals.</p> <pre><code>from uqregressors.plotting.plotting import plot_pred_vs_true\n\nplot_pred_vs_true(*dropout_sol, \n                  y_test, \n                  samples=100, # Number of points randomly subsampled for plotting\n                  include_confidence=True, # Whether to include the confidence interval\n                  alpha=0.1, \n                  title=\"Predicted vs Actual: Dropout\",  \n                  save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n                  filename=\"dropout_test.png\", \n                  show=True)\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\nSaved calibration curve to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests/plots/dropout_test.png')\n</code></pre>"},{"location":"examples/getting_started/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>A simple example of using the <code>tune_hyperparams</code> method is used to tune the lower and upper quantiles of a split conformal quantile regressor using Bayesian Optimization. More detail on how to use the trial objects to suggest parameters for Bayesian Optimization is given in the Optuna documentation. </p> <pre><code>from uqregressors.tuning.tuning import tune_hyperparams, interval_width\n\n# Use Optuna to suggest parameters for the upper and lower quantiles of CQR\nparam_space = {\n    \"tau_lo\": lambda trial: trial.suggest_float(\"tau_lo\", 0.01, 0.1), # Parameter bounds\n}\n\n# Run hyperparameter tuning study\nopt_cqr, opt_score, study = tune_hyperparams(\n                                            regressor=cqr,\n                                            param_space=param_space,\n                                            X=X_train,\n                                            y=y_train,\n                                            score_fn=interval_width, # Can use custom scoring functions\n                                            greater_is_better=False, # Minimize score function\n                                            n_trials=5,\n                                            n_splits=3, # cross validation used if n_splits &gt; 1\n                                            verbose=False,\n                                            )\nopt_cqr_sol = opt_cqr.predict(X_test)\n\n# Plot predictions from the tuned method\nplot_uncertainty_results(*opt_cqr_sol, \"Tuned Quantile Split Conformal Quantile Regression\")\n\n# Plot metrics comparisons between the tuned and untuned models\nhyperparam_comparison_dict = {\"CQR_untuned\": cqr_sol, \n                              \"CQR_tuned\": opt_cqr_sol}\n</code></pre> <p>For this simple example, hyperparameter tuning of the quantiles will result in slightly smaller average interval width while maintaining coverage (note that the optimization was not run to convergence, so the interval width may not actually be smaller)</p> <pre><code>from uqregressors.plotting.plotting import plot_metrics_comparisons\nfrom pathlib import Path\n\nplot_metrics_comparisons(hyperparam_comparison_dict, y_test, alpha=0.1, show=True, \n                         save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n                         filename=\"dropout_test.png\", log_metrics=[], \n                         excluded_metrics=[\"rmse\", \"interval_score\", \"nll_gaussian\", \n                                           \"RMSCD_under\", \"RMSCD\", \"lowest_group_coverage\", \n                                           \"error_width_corr\"])\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\coverage_dropout_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\coverage_dropout_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\average_interval_width_dropout_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\average_interval_width_dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests')\n</code></pre>"},{"location":"examples/metrics/","title":"Uncertainty Quantification Metrics Demo","text":"<p>This notebook demonstrates and explains the most important metrics for evaluating regression models with uncertainty intervals. Each metric is defined mathematically and illustrated with a simple example.</p>"},{"location":"examples/metrics/#root-mean-squared-error-rmse","title":"Root Mean Squared Error (RMSE)","text":"<p>The Root Mean Squared Error (RMSE) measures the average magnitude of the errors between predicted means and true values. It is a standard metric for regression accuracy.</p> <p>Formula:</p> \\[ \\mathrm{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (\\hat{y}_i - y_i)^2} \\] <p>where \\(\\hat{y}_i\\) is the predicted mean and \\(y_i\\) is the true value.</p> <p>Interpretation: Lower RMSE indicates better point prediction accuracy.</p>"},{"location":"examples/metrics/#coverage","title":"Coverage","text":"<p>Coverage measures the proportion of true values that fall within the predicted uncertainty intervals. It is a key metric for interval calibration.</p> <p>Formula:</p> \\[ \\mathrm{Coverage} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}\\{l_i \\leq y_i \\leq u_i\\} \\] <p>where \\(l_i\\) and \\(u_i\\) are the lower and upper bounds of the prediction interval, and \\(\\mathbb{I}\\) is the indicator function.</p> <p>Interpretation: For a nominal \\(1-\\alpha\\) confidence level, coverage should be close to \\(1-\\alpha\\).</p>"},{"location":"examples/metrics/#average-interval-width","title":"Average Interval Width","text":"<p>The average interval width measures the mean width of the predicted uncertainty intervals.</p> <p>Formula:</p> \\[ \\mathrm{Average\\ Interval\\ Width} = \\frac{1}{n} \\sum_{i=1}^n (u_i - l_i) \\] <p>where \\(u_i\\) and \\(l_i\\) are the upper and lower bounds of the prediction interval.</p> <p>Interpretation: Lower interval width is better if coverage is maintained, as it indicates more precise uncertainty estimates.</p>"},{"location":"examples/metrics/#interval-score","title":"Interval Score","text":"<p>The interval score is a proper scoring rule for evaluating prediction intervals. It penalizes both the width of the interval and whether the true value falls outside the interval.</p> <p>Formula:</p> \\[ \\mathrm{Interval\\ Score}_i = (u_i - l_i) + \\frac{2}{\\alpha}(l_i - y_i)\\mathbb{I}\\{y_i &lt; l_i\\} + \\frac{2}{\\alpha}(y_i - u_i)\\mathbb{I}\\{y_i &gt; u_i\\} \\] <p>where \\(u_i\\) and \\(l_i\\) are the upper and lower bounds, \\(y_i\\) is the true value, \\(\\alpha\\) is the miscoverage rate, and \\(\\mathbb{I}\\) is the indicator function.</p> <p>Interpretation: Lower interval score is better. It rewards narrow intervals that contain the true value and penalizes intervals that miss the true value.</p>"},{"location":"examples/metrics/#negative-log-likelihood-nll","title":"Negative Log Likelihood (NLL)","text":"<p>The negative log likelihood (NLL) measures how well a probabilistic model predicts the observed data, assuming a Gaussian distribution for the predictions.</p> <p>Formula:</p> \\[ \\mathrm{NLL} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{1}{2} \\log(2\\pi \\sigma_i^2) + \\frac{(y_i - \\mu_i)^2}{2\\sigma_i^2} \\right] \\] <p>where \\(\\mu_i\\) is the predicted mean, \\(\\sigma_i^2\\) is the predicted variance, and \\(y_i\\) is the true value.</p> <p>Interpretation: Lower NLL indicates better probabilistic predictions.</p>"},{"location":"examples/metrics/#error-width-correlation","title":"Error Width Correlation","text":"<p>The error width correlation is the Pearson correlation coefficient between the absolute residuals and the predicted interval widths. It measures how well the predicted uncertainty adapts to the actual errors.</p> <p>Formula:</p> \\[ \\mathrm{Corr}(|y_i - \\hat{y}_i|, u_i - l_i) \\] <p>where \\(|y_i - \\hat{y}_i|\\) is the absolute error and \\(u_i - l_i\\) is the predicted interval width.</p> <p>The Pearson correlation coefficient between two variables \\(X\\) and \\(Y\\) is defined as:</p> \\[ \\rho_{X,Y} = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i=1}^n (X_i - \\bar{X})^2} \\sqrt{\\sum_{i=1}^n (Y_i - \\bar{Y})^2}} \\] <p>Interpretation: - A value near 1 means the model's uncertainty intervals adapt well to the actual errors (good calibration). - A value near 0 means little relationship between predicted uncertainty and actual error.</p>"},{"location":"examples/metrics/#group-conditional-coverage-metrics","title":"Group Conditional Coverage Metrics","text":"<p>These metrics evaluate how well the model's uncertainty intervals are calibrated across different regions of the data, by dividing the data into bins (groups) and computing coverage within each bin.</p> <p>Root Mean Square Coverage Deviation (RMSCD): - RMSCD measures the root mean square deviation between the empirical coverage in each bin and the nominal coverage \\((1-\\alpha)\\). - Lower RMSCD means coverage is more uniform across all bins (better conditional calibration). - Formula:</p> <p> </p> <p>where \\(\\hat{C}_b\\) is the empirical coverage in bin \\(b\\), and \\(B\\) is the number of bins.</p> <p>RMSCD_under: - RMSCD_under is the same as RMSCD, but only computed over bins where the empirical coverage is below the nominal coverage \\((1-\\alpha)\\). - This highlights regions where the model is under-covering (intervals are too narrow). - Formula:</p> <p> </p> <p>where \\(U\\) is the set of bins with \\(\\hat{C}_b &lt; 1-\\alpha\\).</p> <p>Lowest Group Coverage: - The lowest group coverage is the minimum empirical coverage across all bins. - This metric identifies the worst-case region in terms of coverage. - Formula:</p> <p> </p> <p>where \\(\\hat{C}_b\\) is the empirical coverage in bin \\(b\\).</p>"},{"location":"examples/metrics/#example-heteroskedastic-noise","title":"Example: Heteroskedastic Noise","text":"<p>We generate a simple sine wave with Heteroskedastic noise, and demonstrate how the metrics change when using a constant as opposed to adaptive method of uncertainty quantification. </p> <pre><code># Example: Generate synthetic regression data (sine wave with heteroskedastic noise) and prediction intervals\nimport numpy as np\nnp.random.seed(0)\n\nn = 200\nX = np.linspace(0, 1, n)\ny_true = np.sin(2 * np.pi * X) + np.random.normal(0, 0.5 * X / np.max(X), n)\n\n# Simulate predictions (mean is noiseless sine)\nmean_pred = np.sin(2 * np.pi * X)\n\n# --- Constant interval ---\n# Use a fixed width chosen to achieve ~90% coverage\nconst_width = 2.0  # initial guess, will calibrate below\nlower_const = mean_pred - const_width / 2\nupper_const = mean_pred + const_width / 2\n\n# --- Adaptive interval ---\n# Make interval width proportional to noise std (unknown in practice, but for demo)\nnoise_std = 0.5 * X / np.max(X)\nbase_width = 2 * 1.645 * np.mean(noise_std)  # scale to match nominal 90% coverage\nwidth_adapt = 2 * 1.645 * noise_std\nlower_adapt = mean_pred - width_adapt / 2\nupper_adapt = mean_pred + width_adapt / 2\n\n# Calibrate constant width to match adaptive coverage\nfrom scipy.optimize import minimize_scalar\ndef coverage_for_width(w):\n    l = mean_pred - w/2\n    u = mean_pred + w/2\n    return np.abs(np.mean((y_true &gt;= l) &amp; (y_true &lt;= u)) - 0.9)\nres = minimize_scalar(coverage_for_width, bounds=(0.1, 5), method='bounded')\nconst_width = res.x\nlower_const = mean_pred - const_width / 2\nupper_const = mean_pred + const_width / 2\n\n# Check actual coverages\ncov_const = np.mean((y_true &gt;= lower_const) &amp; (y_true &lt;= upper_const))\ncov_adapt = np.mean((y_true &gt;= lower_adapt) &amp; (y_true &lt;= upper_adapt))\nprint(f\"Constant interval coverage: {cov_const:.3f}\")\nprint(f\"Adaptive interval coverage: {cov_adapt:.3f}\")\n</code></pre> <pre><code>Constant interval coverage: 0.890\nAdaptive interval coverage: 0.895\n</code></pre> <pre><code># Visualize predictions and intervals\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 5))\nplt.plot(X, y_true, 'o', label='True values')\nplt.plot(X, mean_pred, 'r-', label='Predicted mean')\nplt.fill_between(X, lower_adapt, upper_adapt, color='orange', alpha=0.3, label='Adaptive Prediction interval')\nplt.fill_between(X, lower_const, upper_const, color='blue', alpha=0.2, label='Constant Prediction interval')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Synthetic Regression with Prediction Intervals')\nplt.legend()\nplt.show()\n</code></pre> <p></p> <pre><code># Compute and display UQ metrics using UQRegressors utilities\nfrom uqregressors.metrics.metrics import compute_all_metrics\nfrom uqregressors.plotting.plotting import plot_metrics_comparisons\n\n# Prepare solution_dict for plotting: method name -&gt; (mean, lower, upper)\nsolution_dict = {\n    'Constant': (mean_pred, lower_const, upper_const),\n    'Adaptive': (mean_pred, lower_adapt, upper_adapt)\n}\n\n# Compute and print all metrics for both\nalpha = 0.1\nfor method, (mean, lower, upper) in solution_dict.items():\n    metrics = compute_all_metrics(mean, lower, upper, y_true, alpha)\n    print(f\"{method} metrics:\")\n    for k, v in metrics.items():\n        print(f\"    {k}: {v:.4f}\")\n    print(f\"----------------\")\n# Plot metrics comparison\nplot_metrics_comparisons(solution_dict, y_true, alpha, show=True)\n</code></pre> <pre><code>Constant metrics:\n    rmse: 0.2931\n    coverage: 0.8900\n    average_interval_width: 0.9594\n    interval_score: 1.0326\n    nll_gaussian: 0.1916\n    error_width_corr: -0.0368\n    RMSCD: 0.0975\n    RMSCD_under: 0.1581\n    lowest_group_coverage: 0.6500\n----------------\nAdaptive metrics:\n    rmse: 0.2931\n    coverage: 0.8950\n    average_interval_width: 0.8225\n    interval_score: 0.8546\n    nll_gaussian: -0.3006\n    error_width_corr: 0.6207\n    RMSCD: 0.0975\n    RMSCD_under: 0.1323\n    lowest_group_coverage: 0.6500\n----------------\n</code></pre> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"examples/metrics/#analysis-of-results","title":"Analysis of results","text":"<p>Here we can see that although the target coverage is similar for both constant and adaptive width methods, the average interval width is slightly smaller for the adaptive method. This indicates that the adaptive method produces more informative intervals. Additionally, the correlation and log-likelihood metrics prioritize adaptivity of intervals, so both of those metrics are significantly better for the adaptive method. Note that the conditional coverage metrics in this case are similar for the two methods. However, this is due to the construction of the conditional coverage metrics in that they bin results based on their y-values, where the noise is truly heteroskedastic in their x-values. It is clear from the visualization that the adaptive method provides better conditional coverage, and this would be reflected in the metrics if binning was done by the input features as opposed to the output features. </p>"},{"location":"examples/quickstart/","title":"UQRegressors Quickstart","text":"<p>This notebook provides a fast introduction to the key features of UQRegressors for uncertainty-aware regression. In just a few cells, you'll see how to fit a model, make predictions with uncertainty, and visualize the results.</p> <p>There are five main capabilities of UQRegessors: </p> <ol> <li>Dataset loading and validation </li> <li>Regression using models of various types created with UQ capability</li> <li>Hyperparameter Tuning using bayesian optimization (wrapper around Optuna)</li> <li>Metrics for evaluating goodness of fit and quality of uncertainty intervals</li> <li>Visualization of metrics, goodness of fit, and quality of uncertainty intervals</li> </ol> <pre><code># 1. Import UQRegressors\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom uqregressors.bayesian.dropout import MCDropoutRegressor\nfrom uqregressors.metrics.metrics import compute_all_metrics\nfrom uqregressors.utils.torch_sklearn_utils import train_test_split\nfrom uqregressors.utils.logging import set_logging_config\n\nset_logging_config(print=False)\n</code></pre>"},{"location":"examples/quickstart/#loading-a-dataset","title":"Loading a Dataset","text":"<p>UQRegressors has several methods for loading datasets from various tabular forms, cleaning them, and validating them for use. More detail about Dataset Loading is available in the Getting Started example. For this example, we make a synthetic regression dataset with 8 features, 300 samples, and a small amount of noise. </p> <pre><code># 2. Load Example Dataset\nfrom sklearn.datasets import make_regression\n\n# Generate a small synthetic regression dataset\nX, y = make_regression(n_samples=300, n_features=8, noise=10.0, random_state=0)\n\n# Split into train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n</code></pre>"},{"location":"examples/quickstart/#training-a-regressor","title":"Training a Regressor","text":"<p>UQRegressors wraps PyTorch and supports a variety of Bayesian and Conformal approaches to uncertainty estimation. Each implementation is highly flexible, validated with published results, and straightforward to set up with a scikit-learn fit, predict interface. All of the supported regressor types are demonstrated in the Getting Started example. A wrapper around Optuna for hyperparameter tuning is also implemented. </p> <pre><code># 3. Train a Regressor with Uncertainty Estimation\nmcd = MCDropoutRegressor(\n    hidden_sizes=[64, 64],\n    dropout=0.05,\n    alpha=0.05,  # 95% prediction intervals\n    epochs=1000,\n    batch_size=32,\n    learning_rate=1e-2,\n    device=\"cpu\",\n    use_wandb=False\n)\nmcd.fit(X_train, y_train)\nprint(\"Model trained. Now, let's try tuning the hyperparameters.\")\n</code></pre> <pre><code>Model trained. Now, let's try tuning the hyperparameters.\n</code></pre> <pre><code># 3a. Hyperparameter Tuning: Optimize tau for maximum log likelihood\nfrom uqregressors.tuning.tuning import tune_hyperparams, log_likelihood\n\n# Define parameter search space for tau, more detail in Getting Started and Optuna docs\nparam_space = {\n    \"tau\": lambda trial: trial.suggest_float(\"tau\", 1e1, 1e3, log=True) # typically set a wider range of values\n}\n\n# Create a model with default tau (will be overwritten by tuning)\nmcd_tune = MCDropoutRegressor(\n    hidden_sizes=[64, 64],\n    dropout=0.05,\n    alpha=0.05,\n    epochs=200,  # Short epochs for tuning\n    batch_size=32,\n    learning_rate=1e-2,\n    device=\"cpu\",\n    use_wandb=False\n)\n\n# Run hyperparameter tuning\nopt_model, opt_score, study = tune_hyperparams(\n    regressor=mcd_tune,\n    param_space=param_space,\n    X=X_train,\n    y=y_train,\n    score_fn=log_likelihood,\n    greater_is_better=True,\n    n_trials=10,\n    n_splits=1,\n    verbose=False\n)\n\n# Use the best model for final training\nmcd = opt_model\nmcd.epochs = 1000  # Set to full epochs for final fit\n</code></pre> <pre><code>[I 2025-07-09 10:48:43,756] A new study created in memory with name: no-name-2a0494a3-f986-4c7d-83ed-200a877e857d\n\n\n\n  0%|          | 0/10 [00:00&lt;?, ?it/s]\n\n\n[I 2025-07-09 10:48:47,714] Trial 0 finished with value: -4.404375498086284 and parameters: {'tau': 83.7452964217357}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:48:51,550] Trial 1 finished with value: -4.751707269088777 and parameters: {'tau': 928.6359614276676}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:48:55,270] Trial 2 finished with value: -4.8170245913107665 and parameters: {'tau': 176.70514530587985}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:48:58,914] Trial 3 finished with value: -4.542158820858813 and parameters: {'tau': 547.914066622478}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:49:02,774] Trial 4 finished with value: -4.460069649380017 and parameters: {'tau': 64.14194109988905}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:49:06,609] Trial 5 finished with value: -4.90872205530352 and parameters: {'tau': 28.63746770769409}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:49:10,374] Trial 6 finished with value: -4.8856025313294715 and parameters: {'tau': 496.60809925075915}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:49:14,178] Trial 7 finished with value: -4.910923636676338 and parameters: {'tau': 18.332110490831788}. Best is trial 0 with value: -4.404375498086284.\n[I 2025-07-09 10:49:18,248] Trial 8 finished with value: -4.291508409718495 and parameters: {'tau': 173.5109790642976}. Best is trial 8 with value: -4.291508409718495.\n[I 2025-07-09 10:49:22,137] Trial 9 finished with value: -4.272518504981865 and parameters: {'tau': 124.55339703811985}. Best is trial 9 with value: -4.272518504981865.\n</code></pre>"},{"location":"examples/quickstart/#predicting-with-a-fitted-model","title":"Predicting with a fitted model","text":"<pre><code># 4. Make Predictions with Uncertainty\nmean, lower, upper = mcd.predict(X_test)\nprint(\"Predictions and uncertainty intervals computed.\")\n</code></pre> <pre><code>Predictions and uncertainty intervals computed.\n</code></pre>"},{"location":"examples/quickstart/#visualization-and-evaluation","title":"Visualization and evaluation","text":"<p>UQRegressors has several methods available to visualize the predictions made and to evaluate their quality (from both point and interval quality perspectives). More detail is available in the Getting Started and Metrics examples.</p> <pre><code># 5. Visualize Prediction Intervals using UQRegressors plotting utilities\nfrom uqregressors.plotting.plotting import plot_pred_vs_true, generate_cal_curve, plot_cal_curve\n\n# Predicted vs True plot\nplot_pred_vs_true(mean, lower, upper, y_test, samples=50, include_confidence=True, show=True, title=\"Predicted vs True with Uncertainty\")\n\n# Calibration curve\nalphas = np.linspace(0.01, 0.3, 10)\ndesired_coverage, coverages, avg_widths = generate_cal_curve(mcd, X_test, y_test, alphas=alphas, refit=False)\nplot_cal_curve(desired_coverage, coverages, show=True, title=\"Calibration Curve\")\n</code></pre> <pre><code>Model and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmph1e6v1yq\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpsi_7txd2\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpkrkrhj_8\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpal00q0gg\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp0c4eadgi\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpq8un3ti_\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpecq9ttws\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp3ma0y1i1\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmplqkmsepx\\models\\MCDropoutRegressor_20250709_104934\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpz50ue33v\\models\\MCDropoutRegressor_20250709_104934\n</code></pre> <p></p> <p></p> <pre><code># 6. Evaluate Prediction Quality\nmetrics = compute_all_metrics(mean, lower, upper, y_test, mcd.alpha)\nprint(\"\\nPrediction and Uncertainty Metrics:\")\nfor k, v in metrics.items():\n    print(f\"    {k}: {v}\")\n</code></pre> <pre><code>Prediction and Uncertainty Metrics:\n    rmse: 17.441155951585564\n    coverage: 0.9833333333333333\n    average_interval_width: 88.6605641523997\n    interval_score: 89.08682763819365\n    nll_gaussian: 4.273782332352531\n    error_width_corr: 0.44146893491167954\n    RMSCD: 0.06009252125773317\n    RMSCD_under: 0.11666666666666659\n    lowest_group_coverage: 0.8333333333333334\n</code></pre>"},{"location":"examples/saving_models/","title":"Example: Saving and Loading Models and Data with UQRegressors","text":"<p>This notebook demonstrates how to train a regression model, save the trained model and associated data to disk, and then load them back for further use or evaluation.</p> <p>The workflow includes:</p> <ol> <li>Generating synthetic data</li> <li>Training a Deep Ensemble regressor</li> <li>Saving the model, metrics, and datasets using the <code>FileManager</code> utility</li> <li>Loading the saved model and data</li> <li>Verifying that predictions from the loaded model match the original</li> </ol>"},{"location":"examples/saving_models/#import-required-libraries","title":"Import Required Libraries","text":"<p>We import the necessary modules from UQRegressors and scikit-learn. The <code>FileManager</code> utility handles saving and loading models and data, while <code>DeepEnsembleRegressor</code> is used as the example model.</p> <pre><code>import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\nfrom uqregressors.utils.file_manager import FileManager\nfrom sklearn.metrics import mean_squared_error\nfrom uqregressors.utils.logging import set_logging_config\nset_logging_config(print=False)\n</code></pre>"},{"location":"examples/saving_models/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>For demonstration purposes, we generate a simple synthetic regression dataset. The target variable is a nonlinear function of the features, with added Gaussian noise.</p> <pre><code># Function to generate synthetic data\ndef generate_data(n_samples=200, n_features=5):\n    X = np.random.randn(n_samples, n_features)\n    y = np.sin(X[:, 0]) + X[:, 1] ** 2 + np.random.normal(0, 0.1, size=n_samples)\n    return X, y\n\n# Generate data and split into train/test sets\nX, y = generate_data()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre>"},{"location":"examples/saving_models/#train-a-deep-ensemble-regressor","title":"Train a Deep Ensemble Regressor","text":"<p>We instantiate and train a <code>DeepEnsembleRegressor</code> on the training data. This model is an ensemble of neural networks, which provides both predictions and uncertainty estimates. For simplicity, we use a small number of epochs.</p> <pre><code># Create and train the regressor\nreg = DeepEnsembleRegressor(epochs=10, random_seed=42)\nreg.fit(X_train, y_train)\n\n# Predict on the test set\nmean_pred, lower, upper = reg.predict(X_test)\nmse = mean_squared_error(y_test, mean_pred)\nprint(f\"Test MSE: {mse:.4f}\")\n</code></pre> <pre><code>Test MSE: 1.5120\n</code></pre>"},{"location":"examples/saving_models/#save-the-model-metrics-and-datasets","title":"Save the Model, Metrics, and Datasets","text":"<p>We use the <code>FileManager</code> utility to save the trained model, evaluation metrics, and the train/test datasets to disk. This makes it easy to reload the model and data later for reproducibility or further analysis.</p> <pre><code># Initialize the FileManager and save everything\nfm = FileManager() # Can also be initialized with a 'BASE_DIR', e.g., FileManager(base_dir=\"C:/my_models\")\n# When not specified, 'BASE_DIR' defaults to a folder named 'models' in the user's home directory. \n\nsave_path = fm.save_model(\n    # Can include a custom path, e.g., save_path=\"C:/my_models/deep_ensemble_regressor\" \n    # or a custom name, e.g., save_name=\"deep_ensemble_regressor\", which will save in 'BASE_DIR'/models/'save_name'\n    reg,\n    metrics={\"mse\": mse},\n    X_train=X_train,\n    y_train=y_train,\n    X_test=X_test,\n    y_test=y_test,\n)\n</code></pre> <pre><code>Model and additional artifacts saved to: C:\\Users\\arsha\\.uqregressors\\models\\DeepEnsembleRegressor_20250709_115438\n</code></pre>"},{"location":"examples/saving_models/#load-the-model-metrics-and-datasets","title":"Load the Model, Metrics, and Datasets","text":"<p>We demonstrate how to load the saved model, metrics, and datasets using the <code>FileManager</code>. This allows you to resume work, evaluate, or make predictions without retraining.</p> <pre><code># Load everything back from disk\nload_dict = fm.load_model(DeepEnsembleRegressor, save_path, load_logs=True) # Returns a dictionary\nloaded_model = load_dict[\"model\"]\nX_test_loaded = load_dict[\"X_test\"]\ny_test_loaded = load_dict[\"y_test\"]\nmse_loaded = load_dict[\"metrics\"][\"mse\"]\n</code></pre> <pre><code>D:\\uqregressors\\src\\uqregressors\\bayesian\\deep_ens.py:412: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n</code></pre>"},{"location":"examples/saving_models/#predict-with-the-loaded-model-and-verify-results","title":"Predict with the Loaded Model and Verify Results","text":"<p>Finally, we use the loaded model to make predictions on the loaded test set and verify that the mean squared error matches the value saved earlier. This confirms that the model and data were saved and loaded correctly.</p> <pre><code># Predict with the loaded model and check MSE\nmean_pred_loaded, _, _ = loaded_model.predict(X_test_loaded)\nloaded_mse = mean_squared_error(y_test_loaded, mean_pred_loaded)\nprint(f\"Loaded MSE: {loaded_mse:.4f} (should match saved: {mse_loaded:.4f})\")\n</code></pre> <pre><code>Loaded MSE: 1.5120 (should match saved: 1.5120)\n</code></pre>"},{"location":"examples/validation/","title":"Validation of Regressor Implementations","text":"<p>This is an example script which replicates published results for the MC-dropout, Deep Ensemble, and Split Conformal Quantile Regression (CQR) methods. Additionally, average coverage is validated for the K-fold-CQR and normalized conformal ensemble methods. This script matches the experimental setup described in each of these papers using public datasets, runs one fit-predict trial, and verifies that the desired results land within the probable range reported in the published results. </p>"},{"location":"examples/validation/#utility-functions-for-running-regressor-tests","title":"Utility functions for running regressor tests:","text":"<pre><code>import numpy as np \nimport torch \nfrom uqregressors.utils.torch_sklearn_utils import train_test_split\nfrom uqregressors.tuning.tuning import tune_hyperparams, log_likelihood\nfrom uqregressors.utils.logging import set_logging_config\nfrom uqregressors.utils.file_manager import FileManager\nfrom uqregressors.bayesian.dropout import MCDropoutRegressor\nfrom uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\nfrom uqregressors.utils.data_loader import clean_dataset, validate_dataset\nfrom uqregressors.metrics.metrics import compute_all_metrics\nfrom uqregressors.utils.data_loader import load_unformatted_dataset\nfrom uqregressors.conformal.cqr import ConformalQuantileRegressor\nfrom uqregressors.conformal.k_fold_cqr import KFoldCQR\nfrom uqregressors.conformal.conformal_ens import ConformalEnsRegressor\nfrom pathlib import Path\nfrom copy import deepcopy\nimport optuna\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nr_seed = 42\nset_logging_config(print=False) # Disable logging for all future regressors for cleanliness\noptuna.logging.set_verbosity(optuna.logging.WARNING) # Disable hyperparameter logging for cleanliness\n\ndef test_regressor(model, X, y, dataset_name, test_size, seed=None, \n                   tuning_epochs=None, param_space=None, scoring_fn=None, greater=None,\n                   initial_params=None, n_trials=None, n_splits=1): \n    if seed is not None: \n        np.random.seed(seed)\n        torch.manual_seed(seed)\n\n    X, y = clean_dataset(X, y)\n    validate_dataset(X, y, name=dataset_name)\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n\n    # Hyperparameter Optimization: \n    if tuning_epochs is not None and param_space is not None: \n        epochs_copy = model.epochs\n        model.epochs = deepcopy(tuning_epochs)\n        opt_model, opt_score, study = tune_hyperparams(regressor=model, \n                                                        param_space=param_space,\n                                                        X=X_train, \n                                                        y=y_train,\n                                                        score_fn=scoring_fn, \n                                                        greater_is_better=greater,\n                                                        initial_params=initial_params, \n                                                        n_trials=n_trials, \n                                                        n_splits=n_splits, \n                                                        verbose=False\n                                                        )\n        model = opt_model \n        model.epochs = epochs_copy \n\n    # Modify learning rate for deep ensembles on energy and kin8nm datasets: \n    if type(model) is DeepEnsembleRegressor: \n        print (type(model))\n        if dataset_name in [\"energy\", \"kin8nm\"]: \n            print(\"Setting learning rate to 1e-2\")\n            model.learning_rate = 1e-2 \n\n        else: \n            print (\"Setting learning rate to 1e-1\")\n            model.learning_rate =1e-1\n\n    model.fit(X_train, y_train)\n    mean, lower, upper = model.predict(X_test)\n\n    metrics = compute_all_metrics(mean, lower, upper, y_test, model.alpha)\n    metrics[\"scale_factor\"] = np.mean(np.abs(y)).astype(np.float64)\n\n    return metrics \n\ndef run_regressor_test(model, datasets, seed, filename, test_size, \n                       BASE_SAVE_DIR=Path.home()/\".uqregressors\", \n                       tuning_epochs=None, param_space=None, scoring_fn=None, greater=None, \n                       initial_params=None, n_trials=None, n_splits=1): \n    DATASET_PATH = Path.cwd().absolute() / \"datasets\"\n    saved_results = []\n    for name, file in datasets.items(): \n        print(f\"\\n Loading dataset: {name}\")\n        X, y = load_unformatted_dataset(DATASET_PATH / file)\n\n        metrics = test_regressor(model, X, y, name, seed=seed, test_size=test_size, \n                                 tuning_epochs=tuning_epochs, param_space=param_space, \n                                 scoring_fn=scoring_fn, greater=greater, initial_params=initial_params, \n                                 n_trials=n_trials, n_splits=n_splits)\n        print(metrics)\n\n        fm = FileManager(BASE_SAVE_DIR)\n        save_path = fm.save_model(model, name=name + \"_\" + filename, metrics=metrics)\n        saved_results.append((model.__class__, name, save_path))\n    return saved_results\n\ndef print_results(paths): \n    fm = FileManager()\n    for cls, dataset_name, path in paths: \n        results = fm.load_model(cls, path=path, load_logs=False)\n        print (f\"Results for {dataset_name}\")\n        print(results[\"metrics\"])\n</code></pre>"},{"location":"examples/validation/#datasets","title":"Datasets","text":"<p>A variety of datasets are chosen which match the datasets for which there are published results. Extremely large datasets, and those where there is ambiguity about which target is being predicted are omitted. </p> <p>Information about the datasets used is available in: Hern\u00e1ndez Lobato and Adams, 2015.</p> <pre><code>datasets_bayesian = { \n    \"concrete\": \"concrete.xls\", \n    \"energy\": \"energy_efficiency.xlsx\", \n    \"kin8nm\": \"kin8nm.arff\", \n    \"power\": \"power_plant.xlsx\", \n    \"wine\": \"winequality-red.csv\", \n}\n\ndatasets_conformal = {\n    \"concrete\": \"concrete.xls\"\n}\n</code></pre> <pre><code>import matplotlib.pyplot as plt \nimport numpy as np \n\ndef plot_validation(pub, save_paths, name, metric_1, metric_2, metric_1_name, metric_2_name): \n    save_paths_dict = dict(zip([path[1] for path in save_paths], [(path[0], path[2]) for path in save_paths]))\n\n    MCD_exp = {}\n    fm = FileManager()\n    for key, (RMSE, RMSE_std, LL, LL_std) in pub.items(): \n        MCD_exp[key] = fm.load_model(model_class=save_paths_dict[key][0], path=save_paths_dict[key][1])[\"metrics\"]\n\n    datasets = list(pub.keys())\n    datasets.reverse()\n    y_pos = np.arange(len(datasets))\n\n    # Prepare RMSE and LL values\n    pub_rmse = np.array([pub[d][0] for d in datasets])\n    pub_rmse_err = np.array([1.96 * pub[d][1] for d in datasets])\n    exp_rmse = np.array([MCD_exp[d][metric_1] for d in datasets])\n\n    pub_ll = np.array([pub[d][2] for d in datasets])\n    pub_ll_err = np.array([1.96 * pub[d][3] for d in datasets])\n    if metric_2 is \"nll_gaussian\": \n        exp_ll = np.array([-MCD_exp[d][metric_2] for d in datasets])\n    else: \n        exp_ll = np.array([MCD_exp[d][metric_2] for d in datasets])\n\n    # Plot RMSE\n    fig, ax = plt.subplots(figsize=(8, 5))\n    ax.errorbar(pub_rmse, y_pos, xerr=pub_rmse_err, fmt='o', label='Published (95% CI)', color='blue', capsize=6)\n    ax.scatter(exp_rmse, y_pos, color='red', label='Experimental', zorder=5)\n    ax.set_yticks(y_pos)\n    ax.set_yticklabels(datasets)\n    ax.set_xlabel(metric_1_name)\n    ax.set_title(f\"{name}: {metric_1_name} Validation\")\n    ax.legend()\n    ax.grid(True)\n\n    # Plot Log Likelihood\n    fig2, ax2 = plt.subplots(figsize=(8, 5))\n    ax2.errorbar(pub_ll, y_pos, xerr=pub_ll_err, fmt='o', label='Published (95% CI)', color='blue', capsize=6)\n    ax2.scatter(exp_ll, y_pos, color='red', label='Experimental', zorder=5)\n    ax2.set_yticks(y_pos)\n    ax2.set_yticklabels(datasets)\n    ax2.set_xlabel(metric_2_name)\n    ax2.set_title(f\"{name}: {metric_2_name} Validation\")\n    ax2.legend()\n    ax2.grid(True)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"examples/validation/#mc-dropout-regressor","title":"MC dropout Regressor","text":"<p>First, results are generated and compared to the results published in Gal and Ghahramani, 2016. As described in the paper, a single layer neural network with 50 hidden units is used with 0.05 dropout probability, and parameters optimized for 400 epochs with minibatch sizes of 32 on the Adam optimizer. Before training, the aleatoric uncertainty parameter, \\(\\tau\\), is estimated with 30 iterations of Bayesian Optimization with initial prior length scale 0.01. </p> <pre><code>from uqregressors.tuning.tuning import log_likelihood, interval_score\n\n\ndropout = MCDropoutRegressor(\n    hidden_sizes = [50], \n    dropout=0.05, \n    use_paper_weight_decay=True, \n    prior_length_scale=1e-2,\n    alpha=0.05, \n    n_samples=100, \n    epochs=400, # Changed to 40 during hyperparameter tuning\n    batch_size=32, \n    learning_rate=1e-3, \n    device=device, \n    use_wandb=False\n)\n\n# Hyperparameter Tuning: \nparam_space = {\n    \"tau\": lambda trial: trial.suggest_float(\"tau\", 1e-2, 1e2, log=True)\n}\n\nMC_save_paths = run_regressor_test(dropout, datasets_bayesian, seed=r_seed, filename=\"dropout_validation\", test_size=0.2, \n                   tuning_epochs=40, param_space=param_space, scoring_fn=log_likelihood, greater=True, n_trials=40)\n\nprint_results(MC_save_paths)\n</code></pre> <pre><code>MCD_pub = {\n    \"concrete\": [5.23, 0.53, -3.04, 0.09], \n    \"energy\": [1.66, 0.19, -1.99, 0.09], \n    \"kin8nm\": [0.1, 0.005, 0.95, 0.03], \n    \"power\": [4.12, 0.03, -2.80, 0.05], \n    \"wine\": [0.64, 0.04, -0.93, 0.06]\n}\nplot_validation(MCD_pub, MC_save_paths, \"MC Dropout\", \"rmse\", \"nll_gaussian\", \"Root Mean Squared Error\", \"Gaussian Log Likelihood\")\n</code></pre> <p></p> <p></p>"},{"location":"examples/validation/#deep-ensemble-regressor","title":"Deep Ensemble Regressor","text":"<p>Next, validation is performed for the Deep Ensemble Regressor as in Lakshminarayanan et. al. 2017. A very similar experimental setup as that of MC Dropout is carried out using a Deep Ensemble with 5 estimators, each a single layer neural network with 50 hidden units, a batch size of 100, and a learning rate of either 1e-1 or 1e-2 on the Adam optimizer. </p> <pre><code>deep_ens = DeepEnsembleRegressor(\n    n_estimators=5, \n    hidden_sizes=[50], \n    n_jobs=2, \n    alpha=0.05, \n    batch_size=100, \n    learning_rate=1e-1, #Changed to 1e-2 for energy and kin8nm datasets \n    epochs=40, \n    device=device, \n    scale_data=True, \n    use_wandb=False\n)\n\ndeep_ens_save_paths = run_regressor_test(deep_ens, datasets_bayesian, seed=r_seed, filename=\"deep_ens_validation\", test_size=0.1)\n</code></pre> <pre><code>DE_pub = {\n    \"concrete\": [6.03, 0.58, -3.06, 0.18], \n    \"energy\": [2.09, 0.29, -1.38, 0.22], \n    \"kin8nm\": [0.09, 0.005, 1.2, 0.02], \n    \"power\": [4.11, 0.17, -2.79, 0.04], \n    \"wine\": [0.64, 0.04, -0.94, 0.12]\n}\nplot_validation(DE_pub, deep_ens_save_paths, \"Deep Ensemble\", \"rmse\", \"nll_gaussian\", \"Root Mean Squared Error\", \"Gaussian Log Likelihood\")\n</code></pre> <p></p> <p></p>"},{"location":"examples/validation/#split-conformal-prediction","title":"Split Conformal Prediction","text":"<p>Next, split conformal prediction is validated according to the results in Romano et. al. 2019. A split conformal quantile regressor with half of the training data in the calibration set, two hidden layers of 64 neurons each, a 0.1 dropout probability, and trained for 1000 epochs with a batch size of 64 using the standard quantile loss function is implemented as in the paper, and compared to the published results on the concrete compressive strength dataset. Note that a mean and standard deviation for the published results were estimated from the reported box and whisker plot, and scaled by the mean of the outputs. A slightly better average interval width than published was found, likely due to small differences in the training procedure and hyperparameter tunining functions. </p> <pre><code>from uqregressors.tuning.tuning import interval_width\ndevice = \"cuda\"\n\ncqr = ConformalQuantileRegressor(\n    hidden_sizes=[64, 64], \n    cal_size = 0.5, \n    alpha=0.1, \n    dropout=0.1, \n    epochs=1000, \n    batch_size=64, \n    learning_rate=5e-4, \n    optimizer_kwargs = {\"weight_decay\": 1e-6}, \n    device=device, \n    scale_data=True, \n    use_wandb=False\n)\n\nparam_space = {\n    \"tau_lo\": lambda trial: trial.suggest_float(\"tau_lo\", 0.03, 0.1),\n}\n\ncqr_save_paths = run_regressor_test(cqr, datasets_conformal, seed=r_seed, filename=\"cqr_validation\", test_size=0.2,\n                                    tuning_epochs=1000, param_space=param_space, scoring_fn=interval_width, greater=False, \n                                    n_trials=15, n_splits=3)\n</code></pre> <pre><code>conformal_pub = {\n    \"concrete\": [0.9, 0.01913, 17.189, 0.5479]\n}\nplot_validation(conformal_pub, cqr_save_paths, \"CQR\", \"coverage\", \"average_interval_width\", \"Coverage\", \"Avg. Interval Width\")\n</code></pre> <p></p> <p></p>"}]}