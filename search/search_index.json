{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"UQRegressors","text":"<p>UQRegressors is a Python package that provides machine learning regression models capable of generating prediction intervals for a user-specified confidence level. These models not only estimate the expected output but also quantify the uncertainty around each prediction. For instance, a model from UQRegressors trained to predict house prices could, given a new set of input features, return a 95% confidence interval\u2014indicating that the true price is expected to lie between a predicted lower and upper bound with 95% certainty. Several models from Bayesian and Conformal Prediction literature are implemented and validated on a PyTorch backend, which can be easily applied to regression problems through a scikit-learn <code>fit</code>, <code>predict</code> interface. </p> <p>Key Features</p> <ul> <li>Highly customizable parameters for each model</li> <li>Easy-to-use interface with built in dataset validation</li> <li>GPU compatibility with PyTorch backend </li> <li>Validated implementations with comparisons to published results </li> <li>Easy saving and loading of created models </li> <li>Wide variety of metrics available to assess quality of fit and prediction intervals</li> </ul>"},{"location":"#use-cases","title":"Use Cases","text":"<p>There are five main capabilities of UQRegessors: </p> <ol> <li>Dataset Loading and Validation </li> <li>Regression using models of various types created with UQ capability</li> <li>Hyperparameter Tuning using bayesian optimization (wrapper around Optuna)</li> <li>Metrics for evaluating goodness of fit and quality of uncertainty intervals</li> <li>Visualization of metrics, goodness of fit, and quality of uncertainty intervals</li> </ol> <p>For a simple demonstration of how these features could be used for your problem, check the \"Is UQRegressors right for me?\" example. For a more holistic view of UQRegressors' cabilities, look at the \"Getting Started\" example. </p>"},{"location":"#installation","title":"Installation","text":"<p>To install all core components of UQRegressors, run: <pre><code>pip install UQRegressors \n</code></pre></p>"},{"location":"#installing-pytorch","title":"Installing PyTorch","text":"<p>UQRegressors requires PyTorch, which must be installed separately to match your system's configuration.</p>"},{"location":"#cpu-only","title":"CPU-only:","text":"<pre><code>pip install torch torchvision torchaudio\n</code></pre>"},{"location":"#with-cuda-support-for-gpu","title":"With CUDA support for GPU:","text":"<p>Choose the appropriate command based on your CUDA version:</p> <ul> <li>CUDA 11.8: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></li> <li>CUDA 12.1: <pre><code>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre></li> </ul> <p>For other versions or platforms, check the official PyTorch installation page</p>"},{"location":"#what-next","title":"What Next?","text":""},{"location":"api/metrics/","title":"uqregressors.metrics.metrics","text":"<p>This script contains many metrics which can be used to compare the efficacy of different models. The methods are described along with their functions below.  </p>"},{"location":"api/metrics/#uqregressors.metrics.metrics.RMSCD","title":"<code>RMSCD(lower, upper, y_true, alpha, n_bins=10)</code>","text":"<p>Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins (see group_conditional_coverage).</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The root mean square coverage deviation from alpha.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def RMSCD(lower, upper, y_true, alpha, n_bins=10): \n    \"\"\"\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins (see group_conditional_coverage).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The root mean square coverage deviation from alpha.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true, alpha)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    return np.sqrt(np.mean((gcc[\"bin_coverages\"] - (1-alpha)) ** 2))\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.RMSCD_under","title":"<code>RMSCD_under(lower, upper, y_true, alpha, n_bins=10)</code>","text":"<p>Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated only over bins which do not meet nominal coverage (see RMSCD, group_conditional_coverage).</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The root mean square coverage deviation from alpha over bins which do not meet nominal coverage.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def RMSCD_under(lower, upper, y_true, alpha, n_bins=10):\n    \"\"\"\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated only over bins which do not meet nominal coverage (see RMSCD, group_conditional_coverage).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The root mean square coverage deviation from alpha over bins which do not meet nominal coverage.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true, alpha)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    miscovered_bins = gcc[\"bin_coverages\"][gcc[\"bin_coverages\"] &lt; (1-alpha)]\n    if len(miscovered_bins) == 0: \n        rmscd = 0.0\n    else: \n        rmscd = np.sqrt(np.mean((miscovered_bins - (1-alpha)) ** 2))\n    return rmscd\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.average_interval_width","title":"<code>average_interval_width(lower, upper, **kwargs)</code>","text":"<p>Computes the average interval width (distance between the predicted upper and lower bounds). </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Average distance between the upper and lower bound.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def average_interval_width(lower, upper, **kwargs):\n    \"\"\"\n    Computes the average interval width (distance between the predicted upper and lower bounds). \n\n    Args:\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n\n    Returns: \n        (float): Average distance between the upper and lower bound.\n    \"\"\"\n    _, lower, upper, _, _ = validate_inputs(lower, lower, upper, lower)\n    return np.mean(upper - lower)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.compute_all_metrics","title":"<code>compute_all_metrics(mean, lower, upper, y_true, alpha, n_bins=10, excluded_metrics=['group_conditional_coverage'])</code>","text":"<p>Compute all standard uncertainty quantification metrics and return as a dictionary. Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins. </p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[Tensor, ndarray]</code> <p>The mean predictions to compute metrics for, should be able to be flattened to one dimension.</p> required <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension. </p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as mean.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as mean.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into for conditional coverage metrics. </p> <code>10</code> <code>excluded_metrics</code> <code>list</code> <p>The key of any metrics to exclude from being returned.</p> <code>['group_conditional_coverage']</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the following metrics, except those named in excluded_metrics.</p> <p>rmse (float): Root Mean Square Error. </p> <p>coverage (float): Marginal coverage. </p> <p>average interval width (float): Average distance between upper and lower bound predictions.</p> <p>interval_score (float): Interval score between predictions and data. </p> <p>nll_gaussian (float): Average Negative Log Likelihood of data given predictions under Gaussian assumption.</p> <p>error_width_corr (float): Pearson correlation coefficient between true errors and predicted interval width. </p> <p>group_conditional_coverage (dict): Dictionary containing the mean and coverage of each bin when the outputs are split between several bins.</p> <p>RMSCD (float): Root mean square coverage deviation between the coverage conditional on output bin and the nominal coverage.</p> <p>RMSCD_under (float): Root mean square coverage deviation for all bins which undercover compared to nominal coverage.</p> <p>lowest_group_coverage (float): The lowest coverage of any bin into which the outputs were binned.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def compute_all_metrics(mean, lower, upper, y_true, alpha, n_bins=10, excluded_metrics=[\"group_conditional_coverage\"]):\n    \"\"\"\n    Compute all standard uncertainty quantification metrics and return as a dictionary.\n    Computes the Root Mean Square Coverage Deviation (RMSCD) evaluated over a given number of bins. \n\n    Args: \n        mean (Union[torch.Tensor, np.ndarray]): The mean predictions to compute metrics for, should be able to be flattened to one dimension.\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension. \n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as mean.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as mean.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n        n_bins (int): The number of bins to divide the outputs into for conditional coverage metrics. \n        excluded_metrics (list): The key of any metrics to exclude from being returned.\n\n    Returns: \n        (dict): dictionary containing the following metrics, except those named in excluded_metrics.\n\n            rmse (float): Root Mean Square Error. \n\n            coverage (float): Marginal coverage. \n\n            average interval width (float): Average distance between upper and lower bound predictions.\n\n            interval_score (float): Interval score between predictions and data. \n\n            nll_gaussian (float): Average Negative Log Likelihood of data given predictions under Gaussian assumption.\n\n            error_width_corr (float): Pearson correlation coefficient between true errors and predicted interval width. \n\n            group_conditional_coverage (dict): Dictionary containing the mean and coverage of each bin when the outputs are split between several bins.\n\n            RMSCD (float): Root mean square coverage deviation between the coverage conditional on output bin and the nominal coverage.\n\n            RMSCD_under (float): Root mean square coverage deviation for all bins which undercover compared to nominal coverage.\n\n            lowest_group_coverage (float): The lowest coverage of any bin into which the outputs were binned. \n    \"\"\"\n\n    mean, lower, upper, y_true, alpha = validate_inputs(mean, lower, upper, y_true, alpha)\n\n    metrics_dict = {\n        \"rmse\": rmse(mean, y_true, alpha=alpha),\n        \"coverage\": coverage(lower, upper, y_true, alpha=alpha),\n        \"average_interval_width\": average_interval_width(lower, upper, alpha=alpha),\n        \"interval_score\": interval_score(lower, upper, y_true, alpha),\n        \"nll_gaussian\": nll_gaussian(mean, lower, upper, y_true, alpha),\n        \"error_width_corr\": error_width_corr(mean, lower, upper, y_true), \n        \"group_conditional_coverage\": group_conditional_coverage(lower, upper, y_true, n_bins),\n        \"RMSCD\": RMSCD(lower, upper, y_true, alpha, n_bins),\n        \"RMSCD_under\": RMSCD_under(lower, upper, y_true, alpha, n_bins),\n        \"lowest_group_coverage\": lowest_group_coverage(lower, upper, y_true, n_bins)\n    }\n\n    return_dict = {}\n    for metric, value in metrics_dict.items(): \n        if metric not in excluded_metrics: \n            return_dict[metric] = value \n\n    return return_dict\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.coverage","title":"<code>coverage(lower, upper, y_true, **kwargs)</code>","text":"<p>Computes the coverage as a float between 0 and 1. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Coverage as a scalar between 0.0 and 1.0.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def coverage(lower, upper, y_true, **kwargs):\n    \"\"\"\n    Computes the coverage as a float between 0 and 1. \n\n    Args:\n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n\n    Returns: \n        (float): Coverage as a scalar between 0.0 and 1.0.\n    \"\"\"\n    _, lower, upper, y_true, _ = validate_inputs(lower, lower, upper, y_true)\n    covered = (y_true &gt;= lower) &amp; (y_true &lt;= upper)\n    return np.mean(covered)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.error_width_corr","title":"<code>error_width_corr(mean, lower, upper, y_true, **kwargs)</code>","text":"<p>Computes the Pearson correlation coefficient between true errors and the predicted interval width.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Correlation coefficient between residuals and predicted interval width, bounded in [-1, 1].</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def error_width_corr(mean, lower, upper, y_true, **kwargs): \n    \"\"\"\n    Computes the Pearson correlation coefficient between true errors and the predicted interval width.\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n\n    Returns: \n        (float): Correlation coefficient between residuals and predicted interval width, bounded in [-1, 1].\n    \"\"\"\n    mean, lower, upper, y_true, _ = validate_inputs(mean, lower, upper, y_true)\n    width = upper - lower \n    res = np.abs(mean - y_true)\n    corr = np.corrcoef(width, res)[0, 1]\n    return corr\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.group_conditional_coverage","title":"<code>group_conditional_coverage(lower, upper, y_true, n_bins=10)</code>","text":"<p>Divides the outputs into approximately equal bins, and computes the coverage in each bin. Returns a dictionary containing the mean of the  output in each bin, and the coverage in each bin. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to compute conditional coverage for.</p> <code>10</code> <p>Returns:</p> Type Description <code>dict</code> <p>dictionary containing the following keys: </p> <p>y_true_bin_means (np.ndarray): One dimensional array of the mean of the outputs within each bin.</p> <p>bin_coverages (np.ndarray): One dimensional array of the coverage of the predictions within each bin.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def group_conditional_coverage(lower, upper, y_true, n_bins = 10): \n    \"\"\"\n    Divides the outputs into approximately equal bins, and computes the coverage in each bin. Returns a dictionary containing the mean of the \n    output in each bin, and the coverage in each bin. \n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        n_bins (int): The number of bins to compute conditional coverage for.\n\n    Returns: \n        (dict): dictionary containing the following keys: \n\n            y_true_bin_means (np.ndarray): One dimensional array of the mean of the outputs within each bin.\n\n            bin_coverages (np.ndarray): One dimensional array of the coverage of the predictions within each bin.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    coverage_mask = (y_true &gt; lower) &amp; (y_true &lt; upper)\n    sort_ind = np.argsort(y_true)\n    y_true_sort = y_true[sort_ind]\n    coverage_mask_sort = coverage_mask[sort_ind]\n    split_y_true = np.array_split(y_true_sort, n_bins)\n    split_coverage_mask = np.array_split(coverage_mask_sort, n_bins)\n    bin_means = [np.mean(bin) for bin in split_y_true]\n    bin_coverages = [np.mean(bin) for bin in split_coverage_mask]\n    return {\"y_true_bin_means\": np.array(bin_means), \n            \"bin_coverages\": np.array(bin_coverages)}\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.interval_score","title":"<code>interval_score(lower, upper, y_true, alpha, **kwargs)</code>","text":"<p>Computes the interval score as given in Gneiting and Raftery, 2007.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Interval score.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def interval_score(lower, upper, y_true, alpha, **kwargs):\n    \"\"\"\n    Computes the interval score as given in [Gneiting and Raftery, 2007](https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf).\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n\n    Returns: \n        (float): Interval score.\n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    width = upper - lower\n    penalty_lower = (2 / alpha) * (lower - y_true) * (y_true &lt; lower)\n    penalty_upper = (2 / alpha) * (y_true - upper) * (y_true &gt; upper)\n    return np.mean(width + penalty_lower + penalty_upper)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.lowest_group_coverage","title":"<code>lowest_group_coverage(lower, upper, y_true, n_bins=10)</code>","text":"<p>Computes the coverage of the bin with lowest coverage when the outputs are divided into several bins and coverage is evaluated conditional on each bin. </p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>n_bins</code> <code>int</code> <p>The number of bins to divide the outputs into.</p> <code>10</code> <p>Returns:</p> Type Description <code>float</code> <p>The coverage of the least covered bin of outputs, float between 0 and 1.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def lowest_group_coverage(lower, upper, y_true, n_bins=10): \n    \"\"\"\n    Computes the coverage of the bin with lowest coverage when the outputs are divided into several bins and coverage is evaluated conditional on each bin. \n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        n_bins (int): The number of bins to divide the outputs into.\n\n    Returns: \n        (float): The coverage of the least covered bin of outputs, float between 0 and 1. \n    \"\"\"\n    _, lower, upper, y_true, alpha = validate_inputs(lower, lower, upper, y_true)\n    gcc = group_conditional_coverage(lower, upper, y_true, n_bins)\n    return np.min(gcc[\"bin_coverages\"])\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.nll_gaussian","title":"<code>nll_gaussian(mean, lower, upper, y_true, alpha, **kwargs)</code>","text":"<p>Computes the average negative log likelihood of the data given the predictions and assuming a Gaussian distribution of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>lower</code> <code>Union[ndarray, Tensor]</code> <p>The lower bound predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>upper</code> <code>Union[ndarray, Tensor]</code> <p>The upper bound predictions made by the model, should be the same shape as lower.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as lower.</p> required <code>alpha</code> <code>float</code> <p>1 - confidence, should be a float between 0 and 1. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Average negative log likelihood of the data given the predictions.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def nll_gaussian(mean, lower, upper, y_true, alpha, **kwargs):\n    \"\"\"\n    Computes the average negative log likelihood of the data given the predictions and assuming a Gaussian distribution of predictions.\n\n    Args: \n        lower (Union[np.ndarray, torch.Tensor]): The lower bound predictions made by the model, should be able to be flattened to 1 dimension.\n        upper (Union[np.ndarray, torch.Tensor]): The upper bound predictions made by the model, should be the same shape as lower.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as lower.\n        alpha (float): 1 - confidence, should be a float between 0 and 1. \n\n    Returns: \n        (float): Average negative log likelihood of the data given the predictions.\n    \"\"\"\n    mean, lower, upper, y_true, alpha = validate_inputs(mean, lower, upper, y_true, alpha)\n    z = norm.ppf(1 - alpha / 2)\n    std = (upper - lower) / (2 * z)\n    std = np.clip(std, 1e-6, None)\n\n    log_likelihoods = -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((y_true - mean) / std) ** 2\n    return -np.mean(log_likelihoods)\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.rmse","title":"<code>rmse(mean, y_true, **kwargs)</code>","text":"<p>Computes the root mean square error of the predictions compared to the targets.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[ndarray, Tensor]</code> <p>The mean predictions made by the model, should be able to be flattened to 1 dimension.</p> required <code>y_true</code> <code>Union[ndarray, Tensor]</code> <p>The targets to compare against, should be the same shape as mean. </p> required <p>Returns:</p> Type Description <code>float</code> <p>Scalar root mean squared error.</p> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def rmse(mean, y_true, **kwargs):\n    \"\"\"\n    Computes the root mean square error of the predictions compared to the targets.\n\n    Args: \n        mean (Union[np.ndarray, torch.Tensor]): The mean predictions made by the model, should be able to be flattened to 1 dimension.\n        y_true (Union[np.ndarray, torch.Tensor]): The targets to compare against, should be the same shape as mean. \n\n    Returns: \n        (float): Scalar root mean squared error.\n    \"\"\"\n    mean, _, _, y_true, _ = validate_inputs(mean, mean, mean, y_true)\n    return np.sqrt(np.mean((mean - y_true) ** 2))\n</code></pre>"},{"location":"api/metrics/#uqregressors.metrics.metrics.validate_inputs","title":"<code>validate_inputs(mean, lower, upper, y_true, alpha=0.5)</code>","text":"<p>Ensure inputs are converted to 1D numpy arrays and alpha is a float in (0, 1) for use in computing metrics.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>Union[Tensor, ndarray]</code> <p>The mean predictions to compute metrics for, should be able to be flattened to one dimension.</p> required <code>lower</code> <code>Union[Tensor, ndarray]</code> <p>The lower bound predictions to compute metrics for, should be the same shape as mean. </p> required <code>upper</code> <code>Union[Tensor, ndarray]</code> <p>The upper bound predictions to compute metrics for, should be the same shape as mean. </p> required <code>y_true</code> <code>Union[Tensor, ndarray]</code> <p>The targets to compute metrics with, should be the same shape as mean.</p> required <code>alpha</code> <code>float</code> <p>The desired confidence level, if relevannt, should be a float between 0 and 1.</p> <code>0.5</code> Source code in <code>uqregressors\\metrics\\metrics.py</code> <pre><code>def validate_inputs(mean, lower, upper, y_true, alpha=0.5):\n    \"\"\"\n    Ensure inputs are converted to 1D numpy arrays and alpha is a float in (0, 1) for use in computing metrics.\n\n    Args: \n        mean (Union[torch.Tensor, np.ndarray]): The mean predictions to compute metrics for, should be able to be flattened to one dimension.\n        lower (Union[torch.Tensor, np.ndarray]): The lower bound predictions to compute metrics for, should be the same shape as mean. \n        upper (Union[torch.Tensor, np.ndarray]): The upper bound predictions to compute metrics for, should be the same shape as mean. \n        y_true (Union[torch.Tensor, np.ndarray]): The targets to compute metrics with, should be the same shape as mean.\n        alpha (float): The desired confidence level, if relevannt, should be a float between 0 and 1. \n    \"\"\"\n\n    def to_1d_numpy(x):\n        if isinstance(x, torch.Tensor):\n            x = x.detach().cpu().numpy()\n        x = np.asarray(x)\n        if x.ndim != 1:\n            x = x.flatten()\n        return x\n\n    mean = to_1d_numpy(mean)\n    lower = to_1d_numpy(lower)\n    upper = to_1d_numpy(upper)\n    y_true = to_1d_numpy(y_true)\n\n    if not (0 &lt; float(alpha) &lt; 1):\n        raise ValueError(f\"alpha must be in (0, 1), got {alpha}\")\n\n    length = len(mean)\n    if not (len(lower) == len(upper) == len(y_true) == length):\n        raise ValueError(\"All input arrays must be of the same length.\")\n\n    return mean, lower, upper, y_true, float(alpha)\n</code></pre>"},{"location":"api/plotting/","title":"uqregressors.plotting.plotting","text":""},{"location":"api/plotting/#uqregressors.plotting.plotting--plotting","title":"Plotting","text":"<p>A collection of functions to visualize data generated by UQregressors. </p> The supported types of plots are <ul> <li>Calibration curves </li> <li>Predicted values vs. true values </li> <li>Bar chart of model comparisons based on metrics</li> </ul>"},{"location":"api/plotting/#uqregressors.plotting.plotting.generate_cal_curve","title":"<code>generate_cal_curve(model, X_test, y_test, alphas=np.linspace(0.7, 0.01, 10), refit=False, X_train=None, y_train=None)</code>","text":"<p>Generate the data for a calibration curve, which can be plotted with plot_cal_curve. </p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>The model for which to generate the calibration curve. </p> required <code>X_test</code> <code>array - like</code> <p>An array of testing features to generate the calibration curve for. </p> required <code>y_test</code> <code>array - like</code> <p>An array of testing targets to generate the calibration curve for. </p> required <code>alphas</code> <code>array - like</code> <p>The complement of the confidence intervals tested. If none, 10 alphas between 0.7 and 0.01 are linearly generated.</p> <code>linspace(0.7, 0.01, 10)</code> <code>refit</code> <code>bool</code> <p>Whether to re-fit the model for each alpha (useful for models like CQR where the underlying regressor depends on alpha).</p> <code>False</code> <code>X_train</code> <code>array - like</code> <p>Training features if refit is True. </p> <code>None</code> <code>y_train</code> <code>array - like</code> <p>Training targets if refit is True.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>(ndarray, ndarray, ndarray)</code> <p>The desired coverages, the empirical coverages, and the average interval widths for each alpha.</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def generate_cal_curve(model, X_test, y_test, alphas=np.linspace(0.7, 0.01, 10), refit=False, \n                       X_train=None, y_train=None):\n    \"\"\"\n    Generate the data for a calibration curve, which can be plotted with plot_cal_curve. \n\n    Args: \n        model (BaseEstimator): The model for which to generate the calibration curve. \n        X_test (array-like): An array of testing features to generate the calibration curve for. \n        y_test (array-like): An array of testing targets to generate the calibration curve for. \n        alphas (array-like): The complement of the confidence intervals tested. If none, 10 alphas between 0.7 and 0.01 are linearly generated.\n        refit (bool): Whether to re-fit the model for each alpha (useful for models like CQR where the underlying regressor depends on alpha).\n        X_train (array-like): Training features if refit is True. \n        y_train (array-like): Training targets if refit is True.\n\n    Returns: \n        Tuple(np.ndarray, np.ndarray, np.ndarray): The desired coverages, the empirical coverages, and the average interval widths for each alpha. \n    \"\"\"\n    if (refit == True) and (X_train is None or y_train is None): \n        raise ValueError(\"X_train and y_train must be given to generate a calibration curve with refit=True\")\n    alphas = np.array(alphas)\n    desired_coverage = 1 - alphas \n    coverages = np.zeros_like(desired_coverage)\n    avg_interval_widths = np.zeros_like(desired_coverage)\n\n    for i, alpha in enumerate(alphas): \n        # Clone model: \n        with tempfile.TemporaryDirectory() as tmpdirname: \n            fm = FileManager(tmpdirname)\n            saved_path = fm.save_model(model, name=None, path=None)\n            cloned_model = fm.load_model(model.__class__, path=saved_path)[\"model\"]\n        cloned_model.alpha = alpha \n        if refit == True: \n            cloned_model.fit(X_train, y_train)\n\n        mean, lower, upper = cloned_model.predict(X_test)\n        coverages[i] = coverage(lower, upper, y_test)\n        avg_interval_widths[i] = average_interval_width(lower, upper)\n\n    return desired_coverage, coverages, avg_interval_widths \n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_cal_curve","title":"<code>plot_cal_curve(desired_coverage, coverages, show=False, save_dir=None, filename='calibration_curve.png', title=None, figsize=(8, 5))</code>","text":"<p>Plot a calibration curve with data generated from uqregressors.plotting.plotting.generate_cal_curve. </p> <p>Parameters:</p> Name Type Description Default <code>desired_coverage</code> <code>array - like</code> <p>An array of the desired coverages for which the model was evaluated.</p> required <code>coverages</code> <code>array - like</code> <p>An array of the empirical coverages achieved by the model for each desired coverage. </p> required <code>show</code> <code>bool</code> <p>Whether to display the plot after generating it (True) or simply close (False).</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>If not None, the plot will be saved to the directory: save_dir/plots/filename. If associated with a model,              it is recommended that this directory is the directory in which the model is saved. </p> <code>None</code> <code>filename</code> <code>str</code> <p>The filename, including extension, to which the plots will be saved. </p> <code>'calibration_curve.png'</code> <code>title</code> <code>str</code> <p>The title included in the plot, if not None.</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure to be generated.</p> <code>(8, 5)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>If save_dir is not none, the path to which the file was saved is returned. Otherwise None is returned.</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_cal_curve(desired_coverage, coverages, show=False, save_dir=None, filename=\"calibration_curve.png\", title=None, figsize=(8, 5)): \n    \"\"\"\n    Plot a calibration curve with data generated from uqregressors.plotting.plotting.generate_cal_curve. \n\n    Args: \n        desired_coverage (array-like): An array of the desired coverages for which the model was evaluated.\n        coverages (array-like): An array of the empirical coverages achieved by the model for each desired coverage. \n        show (bool): Whether to display the plot after generating it (True) or simply close (False).\n        save_dir (str): If not None, the plot will be saved to the directory: save_dir/plots/filename. If associated with a model, \n                        it is recommended that this directory is the directory in which the model is saved. \n        filename (str): The filename, including extension, to which the plots will be saved. \n        title (str): The title included in the plot, if not None.\n        figsize (tuple): The size of the figure to be generated.\n\n    Returns: \n        (Union[str, None]): If save_dir is not none, the path to which the file was saved is returned. Otherwise None is returned. \n    \"\"\"\n    plt.figure(figsize=figsize)\n    sns.set_theme(style='whitegrid')\n    sns.lineplot(x=desired_coverage, y=coverages, marker='o', label='Empirical Coverage')\n    plt.plot([0, 1], [0, 1], 'k--', label='Ideal (y = x)')\n    plt.xlabel('Desired Coverage (1 - alpha)')\n    plt.ylabel('Empirical Coverage')\n    if title is not None: \n        plt.title(title)\n    plt.legend()\n    plt.gca().set_aspect('equal', adjustable='box')\n    plt.tight_layout()\n\n    if save_dir is not None: \n        fm = FileManager(save_dir)\n        save_path = fm.save_plot(plt.gcf(), save_dir, filename, show=show)\n\n        print (f\"Saved calibration curve to {save_path}\")\n        return save_path\n\n    else: \n        return None\n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_metrics_comparisons","title":"<code>plot_metrics_comparisons(solution_dict, y_test, alpha, excluded_metrics=[], show=False, save_dir=None, filename='.png', log_metrics=['rmse', 'interval_score', 'average_interval_width'], figsize=(8, 5))</code>","text":"<p>Generate bar charts which compare several models on the basis of all available metrics. </p> <p>Parameters:</p> Name Type Description Default <code>solution_dict</code> <code>dict[str</code> <p>Tuple[np.ndarray, np.ndarray, np.ndarray]]): A dictionary containing the names of the methods to plot as the keys and  a tuple containing the mean, lower, and upper predictions of the model on the test set as the values. </p> required <code>y_test</code> <code>array - like</code> <p>The true values of the targets to compare against. </p> required <code>alpha</code> <code>float</code> <p>1 - the confidence level of predictions. Should be a float between 0 and 1. </p> required <code>excluded_metrics</code> <code>list[str]</code> <p>The names of metrics to exclude. See uqregressors.metrics.metrics.compute_all_metrics for a list of possible keys </p> <code>[]</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default: False.</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>Directory to save the figure. If None, the figure is not saved.</p> <code>None</code> <code>filename</code> <code>str</code> <p>File name for the plot. Default: \"pred_vs_true.png\".</p> <code>'.png'</code> <code>log_metrics</code> <code>list</code> <p>A list containing the keys of metrics to display on a log scale. </p> <code>['rmse', 'interval_score', 'average_interval_width']</code> <code>figsize</code> <code>tuple</code> <p>Desired figure size. </p> <code>(8, 5)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The save path to the directory in which plots were saved if save_dir is True, otherwise None</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_metrics_comparisons(solution_dict, y_test, alpha, excluded_metrics=[], show=False, save_dir=None, filename=\".png\", log_metrics = [\"rmse\", \"interval_score\", \"average_interval_width\"], figsize=(8, 5)): \n    \"\"\"\n    Generate bar charts which compare several models on the basis of all available metrics. \n\n    Args: \n        solution_dict (dict[str: Tuple[np.ndarray, np.ndarray, np.ndarray]]): A dictionary containing the names of the methods to plot as the keys and \n            a tuple containing the mean, lower, and upper predictions of the model on the test set as the values. \n        y_test (array-like): The true values of the targets to compare against. \n        alpha (float): 1 - the confidence level of predictions. Should be a float between 0 and 1. \n        excluded_metrics (list[str]): The names of metrics to exclude. See uqregressors.metrics.metrics.compute_all_metrics for a list of possible keys \n        show (bool): Whether to display the plot. Default: False.\n        save_dir (str, optional): Directory to save the figure. If None, the figure is not saved.\n        filename (str): File name for the plot. Default: \"pred_vs_true.png\".\n        log_metrics (list): A list containing the keys of metrics to display on a log scale. \n        figsize (tuple): Desired figure size. \n\n    Returns: \n        (Union[str, None]): The save path to the directory in which plots were saved if save_dir is True, otherwise None\n    \"\"\"\n\n    better_direction = {\n        \"rmse\": \"lower is better\",\n        \"nll_gaussian\": \"lower is better\",\n        \"interval_score\": \"lower is better\",\n        \"coverage\": \"closer to {:.2f} is better\".format(1 - alpha),\n        \"average_interval_width\": \"lower is better\",\n        \"error_width_corr\":\"higher is better\", \n        \"RMSCD\": \"lower is better\", \n        \"RMSCD_under\": \"lower is better\", \n        \"lowest_group_coverage\": \"higher is better\"\n    }\n\n    rows = [] \n    for method, (mean, lower, upper) in solution_dict.items(): \n        metrics = compute_all_metrics(mean, lower, upper, y_test, alpha, excluded_metrics=excluded_metrics + [\"group_conditional_coverage\"])\n        metrics[\"method\"] = method \n        rows.append(metrics)\n        rows.append(metrics)\n    metrics_df = pd.DataFrame(rows)\n\n    metrics = [col for col in metrics_df.columns if col!=\"method\"]\n\n    for metric in metrics: \n        plt.figure(figsize=figsize)\n        sns.barplot(data=metrics_df, x=\"method\", y=metric)\n        plt.xticks(rotation=45)\n\n        if metric in log_metrics:\n            plt.yscale(\"log\")\n\n        if metric == \"coverage\": \n            plt.axhline(1 - alpha, color=\"red\", linestyle=\"--\", label=\"Nominal\")\n            plt.legend() \n\n        title = f\"{metric} ({better_direction.get(metric, '')})\"\n        plt.title(title)\n        plt.ylabel(metric)\n        plt.xlabel(\"Method\")\n        plt.tight_layout()\n\n        if save_dir is not None: \n            fm = FileManager(save_dir)\n            save_path = fm.save_plot(plt.gcf(), save_dir, metric.strip() + \"_\" + filename, show=show)\n\n            print (f\"Saved model comparison to {save_path}\")\n\n    if save_dir is not None: \n        return save_dir\n    else: \n        return save_path\n</code></pre>"},{"location":"api/plotting/#uqregressors.plotting.plotting.plot_pred_vs_true","title":"<code>plot_pred_vs_true(mean, lower, upper, y_true, samples=None, include_confidence=True, show=False, save_dir=None, filename='pred_vs_true.png', title=None, alpha=None, figsize=(8, 8))</code>","text":"<p>Plot predicted vs true values with optional confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>mean</code> <code>array - like</code> <p>Predicted mean values.</p> required <code>lower</code> <code>array - like</code> <p>Lower bound of prediction intervals.</p> required <code>upper</code> <code>array - like</code> <p>Upper bound of prediction intervals.</p> required <code>y_true</code> <code>array - like</code> <p>True target values.</p> required <code>samples</code> <code>int</code> <p>Number of samples to plot. Defaults to all.</p> <code>None</code> <code>include_confidence</code> <code>bool</code> <p>Whether to plot error bars. Default: True.</p> <code>True</code> <code>show</code> <code>bool</code> <p>Whether to display the plot. Default: False.</p> <code>False</code> <code>save_dir</code> <code>str</code> <p>Directory to save the figure. If None, the figure is not saved.</p> <code>None</code> <code>filename</code> <code>str</code> <p>File name for the plot. Default: \"pred_vs_true.png\".</p> <code>'pred_vs_true.png'</code> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Confidence level (e.g., 0.1 for 90% interval).</p> <code>None</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure to be generated. </p> <code>(8, 8)</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>The save path if the plot should be saved, otherwise None</p> Source code in <code>uqregressors\\plotting\\plotting.py</code> <pre><code>def plot_pred_vs_true(mean, lower, upper, y_true, samples=None, include_confidence=True, show=False, save_dir=None, filename=\"pred_vs_true.png\", title=None, alpha=None, figsize=(8, 8)):\n    \"\"\"\n    Plot predicted vs true values with optional confidence intervals.\n\n    Args:\n        mean (array-like): Predicted mean values.\n        lower (array-like): Lower bound of prediction intervals.\n        upper (array-like): Upper bound of prediction intervals.\n        y_true (array-like): True target values.\n        samples (int): Number of samples to plot. Defaults to all.\n        include_confidence (bool): Whether to plot error bars. Default: True.\n        show (bool): Whether to display the plot. Default: False.\n        save_dir (str): Directory to save the figure. If None, the figure is not saved.\n        filename (str): File name for the plot. Default: \"pred_vs_true.png\".\n        title (str): Title of the plot.\n        alpha (float): Confidence level (e.g., 0.1 for 90% interval).\n        figsize (tuple): Size of the figure to be generated. \n\n    Returns: \n        (Union[str, None]): The save path if the plot should be saved, otherwise None\n    \"\"\"\n    mean = np.asarray(mean)\n    lower = np.asarray(lower)\n    upper = np.asarray(upper)\n    y_true = np.asarray(y_true)\n\n    n = len(y_true)\n    idx = np.arange(n)\n    if samples is not None:\n        samples = min(samples, n)\n        idx = np.random.choice(n, samples, replace=False)\n\n    fig, ax = plt.subplots(figsize=figsize)\n    if include_confidence:\n        ax.errorbar(y_true[idx], mean[idx], \n                    yerr=[mean[idx] - lower[idx], upper[idx] - mean[idx]], \n                    fmt='o', ecolor='gray', alpha=0.75, capsize=3, label=f\"Predictions w/ CI\")\n    else:\n        ax.scatter(y_true[idx], mean[idx], alpha=0.75, label=\"Predictions\")\n\n    ax.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--', label=\"y = x\")\n    ax.set_xlabel(\"True values\")\n    ax.set_ylabel(\"Predicted values\")\n    ax.legend()\n\n    if alpha is not None:\n        ax.text(0.05, 0.95, f\"$\\\\alpha$ = {alpha}\", transform=ax.transAxes, va='top')\n\n    if title:\n        ax.set_title(title)\n\n    plt.tight_layout()\n    if save_dir is not None: \n        fm = FileManager(save_dir)\n        save_path = fm.save_plot(plt.gcf(), save_dir, filename, show=show)\n\n        print (f\"Saved calibration curve to {save_path}\")\n        return save_path\n\n    else: \n        return None\n</code></pre>"},{"location":"api/tuning/","title":"uqregressors.tuning.tuning","text":"<p>Tuning contains the helper function tune_hyperparams which uses the Bayesian hyperparameter optimization framework, Optuna as well as some examples of potential scoring functions. </p> Important features of this hyperparameter optimization method are <ul> <li>Customizable scoring function </li> <li>Customizable hyperparameters to be tuned </li> <li>Customizable number of tuning iterations, search space </li> <li>Support for cross-validation</li> </ul>"},{"location":"api/tuning/#uqregressors.tuning.tuning.interval_score","title":"<code>interval_score(estimator, X, y)</code>","text":"<p>Example of interval_score scoring function for hyperparameter tuning, greater=False</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def interval_score(estimator, X, y): \n    \"\"\"\n    Example of interval_score scoring function for hyperparameter tuning, greater=False\n    \"\"\"\n    alpha = estimator.alpha\n    _, lower, upper = estimator.predict(X)\n    width = upper - lower\n    penalty_lower = (2 / alpha) * (lower - y) * (y &lt; lower)\n    penalty_upper = (2 / alpha) * (y - upper) * (y &gt; upper)\n    return np.mean(width + penalty_lower + penalty_upper)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.interval_width","title":"<code>interval_width(estimator, X, y)</code>","text":"<p>Example of minimizing interval width scoring function for hyperparameter tuning, greater=False</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def interval_width(estimator, X, y): \n    \"\"\"\n    Example of minimizing interval width scoring function for hyperparameter tuning, greater=False\n    \"\"\"\n    _, lower, upper = estimator.predict(X)\n    return np.mean(upper - lower)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.log_likelihood","title":"<code>log_likelihood(estimator, X, y)</code>","text":"<p>Example of maximizing log likelihood scoring function for hyperparameter tuning, greater=True</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def log_likelihood(estimator, X, y): \n    \"\"\"\n    Example of maximizing log likelihood scoring function for hyperparameter tuning, greater=True\n    \"\"\"\n    mean, lower, upper = estimator.predict(X)\n    alpha = estimator.alpha \n    z = norm.ppf(1 - alpha / 2)\n    std = (upper - lower) / (2 * z)\n    std = np.clip(std, 1e-6, None)\n\n    log_likelihoods = -0.5 * np.log(2 * np.pi * std**2) - 0.5 * ((y - mean) / std) ** 2\n    return np.mean(log_likelihoods)\n</code></pre>"},{"location":"api/tuning/#uqregressors.tuning.tuning.tune_hyperparams","title":"<code>tune_hyperparams(regressor, param_space, X, y, score_fn, greater_is_better, n_trials=20, n_splits=3, random_state=42, verbose=True)</code>","text":"<p>Optimizes a scikit-learn-style regressor using Optuna.</p> <p>Supports CV when n_splits &gt; 1, otherwise uses train/val split.</p> <p>Parameters:</p> Name Type Description Default <code>regressor</code> <code>BaseEstimator</code> <p>An instance of a base regressor (must have .fit and .predict).</p> required <code>param_space</code> <code>dict</code> <p>Dict mapping param name \u2192 optuna suggest function (e.g., lambda t: t.suggest_float(...)).</p> required <code>X</code> <code>Union[Tensor, ndarray, DataFrame, Series]</code> <p>Training inputs.</p> required <code>y</code> <code>Union[Tensor, ndarray, DataFrame, Series]</code> <p>Training targets.</p> required <code>score_fn</code> <code>Callable(estimator, X_val, y_val) \u2192 float</code> <p>Scoring function.</p> required <code>greater_is_better</code> <code>bool</code> <p>Whether score_fn should be maximized (True) or minimized (False).</p> required <code>n_trials</code> <code>int</code> <p>Number of Optuna trials.</p> <code>20</code> <code>n_splits</code> <code>int</code> <p>If &gt;1, uses KFold CV; otherwise single train/val split.</p> <code>3</code> <code>random_state</code> <code>int</code> <p>For reproducibility.</p> <code>42</code> <code>verbose</code> <code>bool</code> <p>Print status messages.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[BaseEstimator, float, study]</code> <p>A tuple containing the estimator with the optimized                                           hyperparameters, the minimum score, and the optuna study object</p> Source code in <code>uqregressors\\tuning\\tuning.py</code> <pre><code>def tune_hyperparams(\n    regressor,\n    param_space,\n    X,\n    y,\n    score_fn,\n    greater_is_better,\n    n_trials=20,\n    n_splits=3,\n    random_state=42,\n    verbose=True,\n):\n    \"\"\"\n    Optimizes a scikit-learn-style regressor using Optuna.\n\n    Supports CV when n_splits &gt; 1, otherwise uses train/val split.\n\n    Args:\n        regressor (BaseEstimator): An instance of a base regressor (must have .fit and .predict).\n        param_space (dict): Dict mapping param name \u2192 optuna suggest function (e.g., lambda t: t.suggest_float(...)).\n        X (Union[torch.Tensor, np.ndarray, pd.DataFrame, pd.Series]): Training inputs.\n        y (Union[torch.Tensor, np.ndarray, pd.DataFrame, pd.Series]): Training targets.\n        score_fn (Callable(estimator, X_val, y_val) \u2192 float): Scoring function.\n        greater_is_better (bool): Whether score_fn should be maximized (True) or minimized (False).\n        n_trials (int): Number of Optuna trials.\n        n_splits (int): If &gt;1, uses KFold CV; otherwise single train/val split.\n        random_state (int): For reproducibility.\n        verbose (bool): Print status messages.\n\n    Returns:\n        (Tuple[BaseEstimator, float, optuna.study]): A tuple containing the estimator with the optimized \n                                                     hyperparameters, the minimum score, and the optuna study object\n    \"\"\"\n\n    direction = \"maximize\" if greater_is_better else \"minimize\"\n\n    def objective(trial):\n        # Sample hyperparameters\n        trial_params = {k: suggest_fn(trial) for k, suggest_fn in param_space.items()}\n\n        scores = []\n\n        if n_splits == 1:\n            # Single train/val split\n            X_train, X_val, y_train, y_val = train_test_split(\n                X, y, test_size=0.2, random_state=random_state\n            )\n\n            with tempfile.TemporaryDirectory() as tmpdir: \n                regressor.save(tmpdir)\n                estimator = regressor.__class__.load(tmpdir)\n\n            for param_name, param_value in trial_params.items():\n                setattr(estimator, param_name, param_value)\n\n            estimator.fit(X_train, y_train)\n            score = score_fn(estimator, X_val, y_val)\n            scores.append(score)\n        else:\n            # K-fold CV\n            kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n            for train_idx, val_idx in kf.split(X):\n                X_train, X_val = X[train_idx], X[val_idx]\n                y_train, y_val = y[train_idx], y[val_idx]\n\n                with tempfile.TemporaryDirectory() as tmpdir: \n                    regressor.save(tmpdir)\n                    estimator = regressor.__class__.load(tmpdir)\n\n                for param_name, param_value in trial_params.items():\n                    setattr(estimator, param_name, param_value)\n\n                estimator.fit(X_train, y_train)\n                score = score_fn(estimator, X_val, y_val)\n                scores.append(score)\n\n        mean_score = np.mean(scores)\n\n        if verbose:\n            print(f\"Trial params: {trial_params} -&gt; Score: {mean_score:.4f}\")\n\n        return mean_score\n\n    study = optuna.create_study(direction=direction)\n    study.optimize(objective, n_trials=n_trials)\n\n    # Re-train on full data with best hyperparameters\n    best_params = study.best_params\n\n    with tempfile.TemporaryDirectory() as tmpdir: \n        regressor.save(tmpdir)\n        best_estimator = regressor.__class__.load(tmpdir)\n\n    for k, v in best_params.items():\n        setattr(best_estimator, k, v)\n    best_estimator.fit(X, y)\n\n    if verbose:\n        print(\"Best score:\", study.best_value)\n        print(\"Best hyperparameters:\", best_params)\n\n    return best_estimator, study.best_value, study\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/","title":"uqregressors.bayesian.bbmm_gp","text":"<p>A wrapper for GPyTorch  is created which implements BlackBox Matrix-Matrix Inference Gaussian Process regression (BBMM-GP) as described in  Gardner et al., 2018</p>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP","title":"<code>BBMM_GP</code>","text":"<p>A wrapper around GPyTorch's ExactGP for regression with uncertainty quantification.</p> <p>Supports custom kernels, optimizers, learning schedules, and logging. Outputs mean predictions and confidence intervals using predictive variance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model instance.</p> <code>'BBMM_GP_Regressor'</code> <code>kernel</code> <code>Kernel</code> <p>Covariance kernel.</p> <code>ScaleKernel(RBFKernel())</code> <code>likelihood</code> <code>Likelihood</code> <p>Likelihood function used in GP.</p> <code>GaussianLikelihood()</code> <code>alpha</code> <code>float</code> <p>Significance level for predictive intervals (e.g. 0.1 = 90% CI).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>If True, returns tensors requiring gradients during prediction.</p> <code>False</code> <code>learning_rate</code> <code>float</code> <p>Optimizer learning rate.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>optimizer_cls</code> <code>Callable</code> <p>Optimizer class (e.g., torch.optim.Adam).</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Extra keyword arguments for the optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>Callable or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Extra keyword arguments for the scheduler.</p> <code>None</code> <code>loss_fn</code> <code>Callable or None</code> <p>Custom loss function. Defaults to negative log marginal likelihood.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to train the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>If True, enables wandb logging.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>Name of the wandb project.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>Name of the wandb run.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>List[Logger]</code> <p>Optional list of loggers from hyperparameter tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>_loggers</code> <code>[list]</code> <p>Logger of training loss</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>class BBMM_GP: \n    \"\"\"\n    A wrapper around GPyTorch's ExactGP for regression with uncertainty quantification.\n\n    Supports custom kernels, optimizers, learning schedules, and logging.\n    Outputs mean predictions and confidence intervals using predictive variance.\n\n    Args:\n        name (str): Name of the model instance.\n        kernel (gpytorch.kernels.Kernel): Covariance kernel.\n        likelihood (gpytorch.likelihoods.Likelihood): Likelihood function used in GP.\n        alpha (float): Significance level for predictive intervals (e.g. 0.1 = 90% CI).\n        requires_grad (bool): If True, returns tensors requiring gradients during prediction.\n        learning_rate (float): Optimizer learning rate.\n        epochs (int): Number of training epochs.\n        optimizer_cls (Callable): Optimizer class (e.g., torch.optim.Adam).\n        optimizer_kwargs (dict): Extra keyword arguments for the optimizer.\n        scheduler_cls (Callable or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Extra keyword arguments for the scheduler.\n        loss_fn (Callable or None): Custom loss function. Defaults to negative log marginal likelihood.\n        device (str): Device to train the model on (\"cpu\" or \"cuda\").\n        use_wandb (bool): If True, enables wandb logging.\n        wandb_project (str or None): Name of the wandb project.\n        wandb_run_name (str or None): Name of the wandb run.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (List[Logger]): Optional list of loggers from hyperparameter tuning.\n\n    Attributes: \n        _loggers [list]: Logger of training loss\n    \"\"\"\n    def __init__(self, \n                 name=\"BBMM_GP_Regressor\",\n                 kernel=gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), \n                 likelihood=gpytorch.likelihoods.GaussianLikelihood(), \n                 alpha=0.1,\n                 requires_grad=False,\n                 learning_rate=1e-3,\n                 epochs=200, \n                 optimizer_cls=torch.optim.Adam,\n                 optimizer_kwargs=None,\n                 scheduler_cls=None,\n                 scheduler_kwargs=None,\n                 loss_fn=None, \n                 device=\"cpu\", \n                 use_wandb=False,\n                 wandb_project=None,\n                 wandb_run_name=None, \n                 random_seed=None, \n                 tuning_loggers=[],\n            ):\n        self.name = name\n        self.kernel = kernel \n        self.likelihood = likelihood\n        self.alpha = alpha \n        self.requires_grad = requires_grad\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.optimizer_cls = optimizer_cls \n        self.optimizer_kwargs = optimizer_kwargs or {} \n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n        self.device = device \n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project \n        self.wandb_run_name = wandb_run_name\n        self.model = None\n        self.random_seed = random_seed\n\n        self._loggers = []\n        self.training_logs = None \n        self.tuning_loggers = tuning_loggers \n        self.tuning_logs = None\n\n        self.train_X = None \n        self.train_y = None\n\n    def fit(self, X, y): \n        \"\"\"\n        Fits the GP model to training data.\n\n        Args:\n            X (np.ndarray or torch.Tensor): Training features of shape (n_samples, n_features).\n            y (np.ndarray or torch.Tensor): Training targets of shape (n_samples,).\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n        y_tensor = y_tensor.view(-1)\n\n        self.train_X = X_tensor \n        self.train_y = y_tensor\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        model = ExactGP(self.kernel, X_tensor, y_tensor, self.likelihood)\n        self.model = model.to(self.device)\n\n        self.model.train()\n        self.likelihood.train()\n\n        if self.loss_fn == None: \n            self.mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, model)\n            self.loss_fn = self.mll_loss\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        for epoch in range(self.epochs): \n            optimizer.zero_grad()\n            preds = model(X_tensor)\n            loss = self.loss_fn(preds, y_tensor)\n            loss.backward()\n            optimizer.step() \n\n            if scheduler is not None:\n                scheduler.step()\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": loss})\n\n        self._loggers.append(logger)\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        X_tensor = validate_X_input(X, device=self.device, requires_grad=True)\n        self.model.eval()\n        self.likelihood.eval() \n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var(): \n            preds = self.likelihood(self.model(X_tensor))\n\n        with torch.no_grad(): \n            mean = preds.mean\n            lower_2std, upper_2std = preds.confidence_region() \n            low_std, up_std = (mean - lower_2std) / 2, (upper_2std - mean) / 2 \n\n        z_score = st.norm.ppf(1 - self.alpha / 2)\n        lower = mean - z_score * low_std\n        upper = mean + z_score * up_std\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def mll_loss(self, preds, y): \n        \"\"\"\n        Computes the negative log marginal likelihood (default loss function).\n\n        Args:\n            preds (gpytorch.distributions.MultivariateNormal): GP predictive distribution.\n            y (torch.Tensor): Ground truth targets.\n\n        Returns:\n            (torch.Tensor): Negative log marginal likelihood loss.\n        \"\"\"\n        return -torch.sum(self.mll(preds, y))\n\n\n    def save(self, path):\n        \"\"\"\n        Saves model configuration, weights, and training data to disk.\n\n        Args:\n            path (Union[str, Path]): Path to save directory.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"model\", \"kernel\", \"likelihood\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\", \"train_X\", \"train_y\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module, torch.Tensor))\n        }\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.optimizer_cls.__class__.__name__ if self.scheduler_cls is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.kernel, self.likelihood, self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n        torch.save([self.train_X, self.train_y], path / f\"train.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Loads a saved BBMM_GP model from disk.\n\n        Args:\n            path (Union[str, Path]): Path to saved model directory.\n            device (str): Device to map model to (\"cpu\" or \"cuda\").\n            load_logs (bool): If True, also loads training/tuning logs.\n\n        Returns:\n            (BBMM_GP): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            kernel, likelihood, optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs = pickle.load(f)\n\n        train_X, train_y = torch.load(path / f\"train.pt\")\n        model.model = ExactGP(kernel, train_X, train_y, likelihood)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.fit","title":"<code>fit(X, y)</code>","text":"<p>Fits the GP model to training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray or Tensor</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray or Tensor</code> <p>Training targets of shape (n_samples,).</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fits the GP model to training data.\n\n    Args:\n        X (np.ndarray or torch.Tensor): Training features of shape (n_samples, n_features).\n        y (np.ndarray or torch.Tensor): Training targets of shape (n_samples,).\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n    y_tensor = y_tensor.view(-1)\n\n    self.train_X = X_tensor \n    self.train_y = y_tensor\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n\n    config = {\n        \"learning_rate\": self.learning_rate,\n        \"epochs\": self.epochs,\n    }\n\n    logger = Logger(\n        use_wandb=self.use_wandb,\n        project_name=self.wandb_project,\n        run_name=self.wandb_run_name,\n        config=config,\n    )\n\n    model = ExactGP(self.kernel, X_tensor, y_tensor, self.likelihood)\n    self.model = model.to(self.device)\n\n    self.model.train()\n    self.likelihood.train()\n\n    if self.loss_fn == None: \n        self.mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, model)\n        self.loss_fn = self.mll_loss\n\n    optimizer = self.optimizer_cls(\n        model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n    )\n\n    scheduler = None\n    if self.scheduler_cls is not None:\n        scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n    for epoch in range(self.epochs): \n        optimizer.zero_grad()\n        preds = model(X_tensor)\n        loss = self.loss_fn(preds, y_tensor)\n        loss.backward()\n        optimizer.step() \n\n        if scheduler is not None:\n            scheduler.step()\n        if epoch % (self.epochs / 20) == 0:\n            logger.log({\"epoch\": epoch, \"train_loss\": loss})\n\n    self._loggers.append(logger)\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Loads a saved BBMM_GP model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to saved model directory.</p> required <code>device</code> <code>str</code> <p>Device to map model to (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>If True, also loads training/tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>BBMM_GP</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Loads a saved BBMM_GP model from disk.\n\n    Args:\n        path (Union[str, Path]): Path to saved model directory.\n        device (str): Device to map model to (\"cpu\" or \"cuda\").\n        load_logs (bool): If True, also loads training/tuning logs.\n\n    Returns:\n        (BBMM_GP): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        kernel, likelihood, optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs = pickle.load(f)\n\n    train_X, train_y = torch.load(path / f\"train.pt\")\n    model.model = ExactGP(kernel, train_X, train_y, likelihood)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.mll_loss","title":"<code>mll_loss(preds, y)</code>","text":"<p>Computes the negative log marginal likelihood (default loss function).</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>MultivariateNormal</code> <p>GP predictive distribution.</p> required <code>y</code> <code>Tensor</code> <p>Ground truth targets.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Negative log marginal likelihood loss.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def mll_loss(self, preds, y): \n    \"\"\"\n    Computes the negative log marginal likelihood (default loss function).\n\n    Args:\n        preds (gpytorch.distributions.MultivariateNormal): GP predictive distribution.\n        y (torch.Tensor): Ground truth targets.\n\n    Returns:\n        (torch.Tensor): Negative log marginal likelihood loss.\n    \"\"\"\n    return -torch.sum(self.mll(preds, y))\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    X_tensor = validate_X_input(X, device=self.device, requires_grad=True)\n    self.model.eval()\n    self.likelihood.eval() \n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var(): \n        preds = self.likelihood(self.model(X_tensor))\n\n    with torch.no_grad(): \n        mean = preds.mean\n        lower_2std, upper_2std = preds.confidence_region() \n        low_std, up_std = (mean - lower_2std) / 2, (upper_2std - mean) / 2 \n\n    z_score = st.norm.ppf(1 - self.alpha / 2)\n    lower = mean - z_score * low_std\n    upper = mean + z_score * up_std\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.BBMM_GP.save","title":"<code>save(path)</code>","text":"<p>Saves model configuration, weights, and training data to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to save directory.</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Saves model configuration, weights, and training data to disk.\n\n    Args:\n        path (Union[str, Path]): Path to save directory.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"model\", \"kernel\", \"likelihood\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\", \"train_X\", \"train_y\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module, torch.Tensor))\n    }\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.optimizer_cls.__class__.__name__ if self.scheduler_cls is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.kernel, self.likelihood, self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n    torch.save([self.train_X, self.train_y], path / f\"train.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/bbmm_gp/#uqregressors.bayesian.bbmm_gp.ExactGP","title":"<code>ExactGP</code>","text":"<p>               Bases: <code>ExactGP</code></p> <p>A custom GPyTorch Exact Gaussian Process model using a constant mean and a user-specified kernel.</p> <p>Parameters:</p> Name Type Description Default <code>kernel</code> <code>Kernel</code> <p>Kernel defining the covariance structure of the GP.</p> required <code>train_x</code> <code>Tensor</code> <p>Training inputs of shape (n_samples, n_features).</p> required <code>train_y</code> <code>Tensor</code> <p>Training targets of shape (n_samples,).</p> required <code>likelihood</code> <code>Likelihood</code> <p>Likelihood function (e.g., GaussianLikelihood).</p> required Source code in <code>uqregressors\\bayesian\\bbmm_gp.py</code> <pre><code>class ExactGP(gpytorch.models.ExactGP): \n    \"\"\"\n    A custom GPyTorch Exact Gaussian Process model using a constant mean and a user-specified kernel.\n\n    Args:\n        kernel (gpytorch.kernels.Kernel): Kernel defining the covariance structure of the GP.\n        train_x (torch.Tensor): Training inputs of shape (n_samples, n_features).\n        train_y (torch.Tensor): Training targets of shape (n_samples,).\n        likelihood (gpytorch.likelihoods.Likelihood): Likelihood function (e.g., GaussianLikelihood).\n    \"\"\"\n    def __init__(self, kernel, train_x, train_y, likelihood):\n        super(ExactGP, self).__init__(train_x, train_y, likelihood)\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = kernel\n\n    def forward(self, x): \n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n</code></pre>"},{"location":"api/Bayesian/deep_ens/","title":"uqregressors.bayesian.deep_ens","text":"<p>Deep Ensembles are implemented as in Lakshimnarayanan et al. 2017. </p>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens--deep-ensembles","title":"Deep Ensembles","text":"<p>This module implements Deep Ensemble Regressors for regression of a one dimensional output. </p> Key features are <ul> <li>Customizable neural network architecture </li> <li>Prediction Intervals based on Gaussian assumption </li> <li>Parallel training of ensemble members with Joblib</li> <li>Customizable optimizer and loss function</li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor","title":"<code>DeepEnsembleRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Deep Ensemble Regressor with uncertainty estimation using neural networks.</p> <p>Trains an ensemble of MLP models to predict both mean and variance for regression tasks, and provides predictive uncertainty intervals.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the regressor for config files.</p> <code>'Deep_Ensemble_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of ensemble members.</p> <code>5</code> <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes for each MLP.</p> <code>[64, 64]</code> <code>alpha</code> <code>float</code> <p>Significance level for prediction intervals (e.g., 0.1 for 90% interval).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>If True, returned predictions require gradients.</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>Name of activation function to use (e.g., 'ReLU').</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for optimizer.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>Optimizer</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Additional kwargs for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>_LRScheduler or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Additional kwargs for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable</code> <p>Loss function accepting (preds, targets).</p> <code>None</code> <code>device</code> <code>str or device</code> <p>Device to run training on ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to use Weights &amp; Biases logging.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>WandB project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>WandB run name prefix.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs to train ensemble members.</p> <code>1</code> <code>random_seed</code> <code>int or None</code> <p>Seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to scale input and output data.</p> <code>True</code> <code>input_scaler</code> <code>object or None</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>object or None</code> <p>Scaler for target values.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>List of tuning loggers.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>models</code> <code>list</code> <p>List of trained PyTorch MLP models.</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of input features.</p> <code>_loggers</code> <code>list</code> <p>Training loggers for each model.</p> <code>training_logs</code> <p>Logs from training.</p> <code>tuning_logs</code> <p>Logs from hyperparameter tuning.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>class DeepEnsembleRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Deep Ensemble Regressor with uncertainty estimation using neural networks.\n\n    Trains an ensemble of MLP models to predict both mean and variance for regression tasks,\n    and provides predictive uncertainty intervals.\n\n    Args:\n        name (str): Name of the regressor for config files.\n        n_estimators (int): Number of ensemble members.\n        hidden_sizes (list of int): List of hidden layer sizes for each MLP.\n        alpha (float): Significance level for prediction intervals (e.g., 0.1 for 90% interval).\n        requires_grad (bool): If True, returned predictions require gradients.\n        activation_str (str): Name of activation function to use (e.g., 'ReLU').\n        learning_rate (float): Learning rate for optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (torch.optim.Optimizer): Optimizer class.\n        optimizer_kwargs (dict): Additional kwargs for optimizer.\n        scheduler_cls (torch.optim.lr_scheduler._LRScheduler or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Additional kwargs for scheduler.\n        loss_fn (callable): Loss function accepting (preds, targets).\n        device (str or torch.device): Device to run training on ('cpu' or 'cuda').\n        use_wandb (bool): Whether to use Weights &amp; Biases logging.\n        wandb_project (str or None): WandB project name.\n        wandb_run_name (str or None): WandB run name prefix.\n        n_jobs (int): Number of parallel jobs to train ensemble members.\n        random_seed (int or None): Seed for reproducibility.\n        scale_data (bool): Whether to scale input and output data.\n        input_scaler (object or None): Scaler for input features.\n        output_scaler (object or None): Scaler for target values.\n        tuning_loggers (list): List of tuning loggers.\n\n    Attributes:\n        models (list): List of trained PyTorch MLP models.\n        input_dim (int): Dimensionality of input features.\n        _loggers (list): Training loggers for each model.\n        training_logs: Logs from training.\n        tuning_logs: Logs from hyperparameter tuning.\n    \"\"\"\n    def __init__(\n        self,\n        name = \"Deep_Ensemble_Regressor\",\n        n_estimators=5,\n        hidden_sizes=[64, 64],\n        alpha=0.1,\n        requires_grad=False,\n        activation_str=\"ReLU\",\n        learning_rate=1e-3,\n        epochs=200,\n        batch_size=32,\n        optimizer_cls=torch.optim.Adam,\n        optimizer_kwargs=None,\n        scheduler_cls=None,\n        scheduler_kwargs=None,\n        loss_fn=None,\n        device=\"cpu\",\n        use_wandb=False,\n        wandb_project=None,\n        wandb_run_name=None,\n        n_jobs=1,\n        random_seed=None,\n        scale_data=True, \n        input_scaler=None,\n        output_scaler=None, \n        tuning_loggers = [],\n    ):\n        self.name=name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.activation_str = activation_str\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.nll_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n        self.models = []\n        self.input_dim = None\n\n        self.scale_data = scale_data\n\n        if scale_data: \n            self.input_scaler = input_scaler or TorchStandardScaler()\n            self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n\n    def nll_loss(self, preds, y): \n        \"\"\"\n        Negative log-likelihood loss assuming Gaussian outputs.\n\n        Args:\n            preds (torch.Tensor): Predicted means and variances, shape (batch_size, 2).\n            y (torch.Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (torch.Tensor): Scalar loss value.\n        \"\"\"\n        means = preds[:, 0]\n        variances = preds[:, 1]\n        precision = 1 / variances\n        squared_error = (y.view(-1) - means) ** 2\n        nll = 0.5 * (torch.log(variances) + precision * squared_error)\n        return nll.mean()\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, idx): \n        \"\"\"\n        Train a single ensemble member.\n\n        Args:\n            X_tensor (torch.Tensor): Input tensor.\n            y_tensor (torch.Tensor): Target tensor.\n            input_dim (int): Number of input features.\n            idx (int): Index of the model (for seeding and logging).\n\n        Returns:\n            (Tuple[MLP, Logger]): (trained model, logger instance)\n        \"\"\"\n        X_tensor = X_tensor.to(self.device)\n        y_tensor = y_tensor.to(self.device)\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + idx)\n            np.random.seed(self.random_seed + idx)\n\n        activation = get_activation(self.activation_str)\n        model = MLP(input_dim, self.hidden_sizes, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{idx}\"\n        )\n\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss.item()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n        logger.finish()\n        return model, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (DeepEnsembleRegressor): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, i)\n            for i in range(self.n_estimators)\n        )\n\n        self.models, self._loggers = zip(*results)\n\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = [] \n\n        for model in self.models: \n            model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n        preds = torch.stack(preds)\n\n        means = preds[:, :, 0]\n        variances = preds[:, :, 1]\n\n        mean = means.mean(dim=0)\n        variance = torch.mean(variances + means ** 2, dim=0) - mean ** 2\n        std = variance.sqrt()\n\n        std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n\n        lower = mean - std * std_mult \n        upper = mean + std * std_mult \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze() \n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained ensemble to disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to save the model and metadata.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved ensemble regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (DeepEnsembleRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        model = cls(**config)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = MLP(model.input_dim, config[\"hidden_sizes\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>DeepEnsembleRegressor</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (DeepEnsembleRegressor): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, i)\n        for i in range(self.n_estimators)\n    )\n\n    self.models, self._loggers = zip(*results)\n\n    return self\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved ensemble regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>DeepEnsembleRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved ensemble regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (DeepEnsembleRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    model = cls(**config)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = MLP(model.input_dim, config[\"hidden_sizes\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.nll_loss","title":"<code>nll_loss(preds, y)</code>","text":"<p>Negative log-likelihood loss assuming Gaussian outputs.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted means and variances, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss value.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def nll_loss(self, preds, y): \n    \"\"\"\n    Negative log-likelihood loss assuming Gaussian outputs.\n\n    Args:\n        preds (torch.Tensor): Predicted means and variances, shape (batch_size, 2).\n        y (torch.Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (torch.Tensor): Scalar loss value.\n    \"\"\"\n    means = preds[:, 0]\n    variances = preds[:, 1]\n    precision = 1 / variances\n    squared_error = (y.view(-1) - means) ** 2\n    nll = 0.5 * (torch.log(variances) + precision * squared_error)\n    return nll.mean()\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = [] \n\n    for model in self.models: \n        model.eval()\n        pred = model(X_tensor)\n        preds.append(pred)\n\n    preds = torch.stack(preds)\n\n    means = preds[:, :, 0]\n    variances = preds[:, :, 1]\n\n    mean = means.mean(dim=0)\n    variance = torch.mean(variances + means ** 2, dim=0) - mean ** 2\n    std = variance.sqrt()\n\n    std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n\n    lower = mean - std * std_mult \n    upper = mean + std * std_mult \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze() \n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.DeepEnsembleRegressor.save","title":"<code>save(path)</code>","text":"<p>Save the trained ensemble to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to save the model and metadata.</p> required Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained ensemble to disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to save the model and metadata.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/deep_ens/#uqregressors.bayesian.deep_ens.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple multi-layer perceptron which outputs a mean and a positive variance per input sample.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes.</p> required <code>activation</code> <code>Module</code> <p>Activation function class (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\bayesian\\deep_ens.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple multi-layer perceptron which outputs a mean and a positive variance per input sample.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): List of hidden layer sizes.\n        activation (torch.nn.Module): Activation function class (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, activation):\n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            input_dim = h\n        output_layer = nn.Linear(hidden_sizes[-1], 2)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        outputs = self.model(x)\n        means = outputs[:, 0]\n        unscaled_variances = outputs[:, 1]\n        scaled_variance = F.softplus(unscaled_variances) + 1e-6\n        scaled_outputs = torch.cat((means.unsqueeze(dim=1), scaled_variance.unsqueeze(dim=1)), dim=1)\n\n        return scaled_outputs\n</code></pre>"},{"location":"api/Bayesian/dropout/","title":"uqregressors.bayesian.dropout","text":"<p>Monte Carlo Dropout is implemented as in Gal and Ghahramani 2016. </p>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout--monte-carlo-dropout","title":"Monte Carlo Dropout","text":"<p>This module implements a Monte Carlo (MC) Dropout Regressor for regression on a one dimensional output with uncertainty quantification. It estimates predictive uncertainty by performing multiple stochastic forward passes through a dropout-enabled neural network.</p> Key features are <ul> <li>Customizable neural network architecture </li> <li>Aleatoric uncertainty included with hyperparameter tau</li> <li>Prediction Intervals based on Gaussian assumption </li> <li>Customizable optimizer and loss function</li> <li>Optional Input/Output Normalization</li> </ul> <p>Warning</p> <p>Using hyperparameter optimization to optimize the aleatoric uncertainty hyperparameter tau is often necessary to obtain correct predictive intervals!</p>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor","title":"<code>MCDropoutRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>MC Dropout Regressor with uncertainty estimation using neural networks. </p> <p>This class trains a dropout MLP and takes stochastic forward passes to provide predictive uncertainty intervals. It makes a Gaussian assumption on the output distribution, and often requires tuning of the hyperparameter tau</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model instance.</p> <code>'MC_Dropout_Regressor'</code> <code>hidden_sizes</code> <code>List[int]</code> <p>Hidden layer sizes for the MLP.</p> <code>[64, 64]</code> <code>dropout</code> <code>float</code> <p>Dropout rate to apply after each hidden layer.</p> <code>0.1</code> <code>tau</code> <code>float</code> <p>Precision parameter (used in predictive variance).</p> <code>1e-06</code> <code>use_paper_weight_decay</code> <code>bool</code> <p>Whether to use paper's theoretical weight decay.</p> <code>True</code> <code>alpha</code> <code>float</code> <p>Significance level (1 - confidence level) for prediction intervals.</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether to track gradients in prediction output.</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>Activation function name (e.g., \"ReLU\", \"Tanh\").</p> <code>'ReLU'</code> <code>n_samples</code> <code>int</code> <p>Number of stochastic forward passes for prediction.</p> <code>100</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>Optimizer</code> <p>PyTorch optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Optional kwargs to pass to optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>Optional[Callable]</code> <p>Optional learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Optional kwargs for the scheduler.</p> <code>None</code> <code>loss_fn</code> <code>Callable</code> <p>Loss function for training (default: MSE).</p> <code>mse_loss</code> <code>device</code> <code>str</code> <p>Device to run training/prediction on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>If True, logs training to Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str</code> <p>W&amp;B project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str</code> <p>W&amp;B run name.</p> <code>None</code> <code>random_seed</code> <code>Optional[int]</code> <p>Seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to standardize inputs and outputs.</p> <code>True</code> <code>input_scaler</code> <code>Optional[TorchStandardScaler]</code> <p>Custom input scaler.</p> <code>None</code> <code>output_scaler</code> <code>Optional[TorchStandardScaler]</code> <p>Custom output scaler.</p> <code>None</code> <code>tuning_loggers</code> <code>List[Logger]</code> <p>External loggers returned from hyperparameter tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>MLP</code> <p>Trained PyTorch MLP model.</p> <code>input_dim</code> <code>int</code> <p>Dimensionality of input features.</p> <code>_loggers</code> <code>Logger</code> <p>Training logger.</p> <code>training_logs</code> <p>Logs from training.</p> <code>tuning_logs</code> <p>Logs from hyperparameter tuning.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>class MCDropoutRegressor(BaseEstimator, RegressorMixin):\n    \"\"\" \n    MC Dropout Regressor with uncertainty estimation using neural networks. \n\n    This class trains a dropout MLP and takes stochastic forward passes to provide predictive uncertainty intervals.\n    It makes a Gaussian assumption on the output distribution, and often requires tuning of the hyperparameter tau\n\n    Args:\n        name (str): Name of the model instance.\n        hidden_sizes (List[int]): Hidden layer sizes for the MLP.\n        dropout (float): Dropout rate to apply after each hidden layer.\n        tau (float): Precision parameter (used in predictive variance).\n        use_paper_weight_decay (bool): Whether to use paper's theoretical weight decay.\n        alpha (float): Significance level (1 - confidence level) for prediction intervals.\n        requires_grad (bool): Whether to track gradients in prediction output.\n        activation_str (str): Activation function name (e.g., \"ReLU\", \"Tanh\").\n        n_samples (int): Number of stochastic forward passes for prediction.\n        learning_rate (float): Learning rate for the optimizer.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (Optimizer): PyTorch optimizer class.\n        optimizer_kwargs (dict): Optional kwargs to pass to optimizer.\n        scheduler_cls (Optional[Callable]): Optional learning rate scheduler class.\n        scheduler_kwargs (dict): Optional kwargs for the scheduler.\n        loss_fn (Callable): Loss function for training (default: MSE).\n        device (str): Device to run training/prediction on (\"cpu\" or \"cuda\").\n        use_wandb (bool): If True, logs training to Weights &amp; Biases.\n        wandb_project (str): W&amp;B project name.\n        wandb_run_name (str): W&amp;B run name.\n        random_seed (Optional[int]): Seed for reproducibility.\n        scale_data (bool): Whether to standardize inputs and outputs.\n        input_scaler (Optional[TorchStandardScaler]): Custom input scaler.\n        output_scaler (Optional[TorchStandardScaler]): Custom output scaler.\n        tuning_loggers (List[Logger]): External loggers returned from hyperparameter tuning.\n\n    Attributes:\n        model (MLP): Trained PyTorch MLP model.\n        input_dim (int): Dimensionality of input features.\n        _loggers (Logger): Training logger.\n        training_logs: Logs from training.\n        tuning_logs: Logs from hyperparameter tuning.\n    \"\"\"\n    def __init__(\n        self,\n        name=\"MC_Dropout_Regressor\",\n        hidden_sizes=[64, 64],\n        dropout=0.1,\n        tau=1.0e-6,\n        use_paper_weight_decay=True,\n        alpha=0.1,\n        requires_grad=False, \n        activation_str=\"ReLU\",\n        n_samples=100,\n        learning_rate=1e-3,\n        epochs=200,\n        batch_size=32,\n        optimizer_cls=torch.optim.Adam,\n        optimizer_kwargs=None,\n        scheduler_cls=None,\n        scheduler_kwargs=None,\n        loss_fn=torch.nn.functional.mse_loss,\n        device=\"cpu\",\n        use_wandb=False,\n        wandb_project=None,\n        wandb_run_name=None,\n        random_seed=None,\n        scale_data=True, \n        input_scaler=None,\n        output_scaler=None, \n        tuning_loggers = []\n    ):\n        self.name=name\n        self.hidden_sizes = hidden_sizes\n        self.dropout = dropout\n        self.tau = tau\n        self.use_paper_weight_decay = use_paper_weight_decay\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.activation_str = activation_str\n        self.n_samples = n_samples\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n\n        self.device = device\n        self.model = None\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n        self.random_seed = random_seed\n        self.input_dim = None\n\n        self.scale_data = scale_data\n        self.input_scaler = input_scaler or TorchStandardScaler()\n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = [] \n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers \n        self.tuning_logs = None\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the MC Dropout model on training data.\n\n        Args:\n            X (array-like): Training features of shape (n_samples, n_features).\n            y (array-like): Target values of shape (n_samples,).\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        model, logger = self._fit_single_model(X_tensor, y_tensor)\n        self._loggers.append(logger)\n\n    def _fit_single_model(self, X_tensor, y_tensor):\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n            \"batch_size\": self.batch_size,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        activation = get_activation(self.activation_str)\n\n        model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n        self.model = model.to(self.device)\n\n        optimizer = self.optimizer_cls(\n            self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor, y_tensor)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self.model.train()\n        for epoch in range(self.epochs):\n            epoch_loss = 0.0\n            for xb, yb in dataloader: \n                optimizer.zero_grad()\n                preds = self.model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss\n\n            if scheduler is not None:\n                scheduler.step()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n        logger.finish()\n\n        return self, logger\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        self.model.train()\n        preds = []\n        with torch.no_grad():\n            for _ in range(self.n_samples):\n                preds.append(self.model(X_tensor))\n        preds = torch.stack(preds)\n        mean = preds.mean(dim=0)\n        variance = torch.var(preds, dim=0) + 1 / self.tau \n        std = variance.sqrt()\n        std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n        lower = mean - std * std_mult\n        upper = mean + std * std_mult\n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean).squeeze()\n            lower = self.output_scaler.inverse_transform(lower).squeeze()\n            upper = self.output_scaler.inverse_transform(upper).squeeze()\n\n        else: \n            mean = mean.squeeze() \n            lower = lower.squeeze() \n            upper = upper.squeeze() \n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save model weights, config, and scalers to disk.\n\n        Args:\n            path (str or Path): Directory to save model components.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \n                         \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved MC dropout regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (MCDropoutRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n\n        model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n\n    def _predictive_log_likelihood(self, X, y_true, tau):\n        self.model.train()\n\n        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n        y_true = y_true.reshape(-1)\n        preds = []\n\n        with torch.no_grad():\n            for _ in range(self.n_samples):\n                pred = self.model(X_tensor).cpu().numpy().squeeze()\n                preds.append(pred)\n        preds = np.stack(preds, axis=0)  # Shape: (T, N)\n\n        mean_preds = preds.mean(axis=0)\n        var_preds = preds.var(axis=0) + (1 / tau)\n\n        log_likelihoods = (\n            -0.5 * np.log(2 * np.pi * var_preds)\n            - 0.5 * ((y_true - mean_preds) ** 2) / var_preds\n        )\n\n        return np.mean(log_likelihoods)\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the MC Dropout model on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>array - like</code> <p>Target values of shape (n_samples,).</p> required Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the MC Dropout model on training data.\n\n    Args:\n        X (array-like): Training features of shape (n_samples, n_features).\n        y (array-like): Target values of shape (n_samples,).\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    model, logger = self._fit_single_model(X_tensor, y_tensor)\n    self._loggers.append(logger)\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved MC dropout regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>MCDropoutRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved MC dropout regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (MCDropoutRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n\n    model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    self.model.train()\n    preds = []\n    with torch.no_grad():\n        for _ in range(self.n_samples):\n            preds.append(self.model(X_tensor))\n    preds = torch.stack(preds)\n    mean = preds.mean(dim=0)\n    variance = torch.var(preds, dim=0) + 1 / self.tau \n    std = variance.sqrt()\n    std_mult = torch.tensor(st.norm.ppf(1 - self.alpha / 2), device=mean.device)\n    lower = mean - std * std_mult\n    upper = mean + std * std_mult\n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean).squeeze()\n        lower = self.output_scaler.inverse_transform(lower).squeeze()\n        upper = self.output_scaler.inverse_transform(upper).squeeze()\n\n    else: \n        mean = mean.squeeze() \n        lower = lower.squeeze() \n        upper = upper.squeeze() \n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MCDropoutRegressor.save","title":"<code>save(path)</code>","text":"<p>Save model weights, config, and scalers to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model components.</p> required Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save model weights, config, and scalers to disk.\n\n    Args:\n        path (str or Path): Directory to save model components.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \n                     \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Bayesian/dropout/#uqregressors.bayesian.dropout.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\bayesian\\dropout.py</code> <pre><code>class MLP(nn.Module):\n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation):\n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            layers.append(nn.Dropout(dropout))\n            input_dim = h\n        layers.append(nn.Linear(hidden_sizes[-1], 1))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"api/Bayesian/gp/","title":"uqregressors.bayesian.gaussian_process","text":"<p>This is a compatibility wrapper of the scikit-learn Gaussian Process Regressor  such that predictions return intervals rather than means and/or standard deviations. </p>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor","title":"<code>GPRegressor</code>","text":"<p>A wrapper for scikit-learn's GaussianProcessRegressor with prediction intervals.</p> <p>This class provides a simplified interface to fit a Gaussian Process (GP) regressor, make predictions with uncertainty intervals, and save/load the model configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'GP_Regressor'</code> <code>kernel</code> <code>Kernel</code> <p>Kernel to use for the GP model.</p> <code>RBF()</code> <code>alpha</code> <code>float</code> <p>Significance level for the prediction interval.</p> <code>0.1</code> <code>gp_kwargs</code> <code>dict</code> <p>Additional keyword arguments for GaussianProcessRegressor.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the model.</p> <code>kernel</code> <code>Kernel</code> <p>Kernel to use in the GP model.</p> <code>alpha</code> <code>float</code> <p>Significance level for confidence intervals (e.g., 0.1 for 90% CI).</p> <code>gp_kwargs</code> <code>dict</code> <p>Additional keyword arguments for the GaussianProcessRegressor.</p> <code>model</code> <code>GaussianProcessRegressor</code> <p>Fitted scikit-learn GP model.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>class GPRegressor: \n    \"\"\"\n    A wrapper for scikit-learn's GaussianProcessRegressor with prediction intervals.\n\n    This class provides a simplified interface to fit a Gaussian Process (GP) regressor,\n    make predictions with uncertainty intervals, and save/load the model configuration.\n\n    Args:\n            name (str): Name of the model.\n            kernel (sklearn.gaussian_process.kernels.Kernel): Kernel to use for the GP model.\n            alpha (float): Significance level for the prediction interval.\n            gp_kwargs (dict, optional): Additional keyword arguments for GaussianProcessRegressor.\n\n    Attributes:\n        name (str): Name of the model.\n        kernel (sklearn.gaussian_process.kernels.Kernel): Kernel to use in the GP model.\n        alpha (float): Significance level for confidence intervals (e.g., 0.1 for 90% CI).\n        gp_kwargs (dict): Additional keyword arguments for the GaussianProcessRegressor.\n        model (GaussianProcessRegressor): Fitted scikit-learn GP model.\n    \"\"\"\n    def __init__(self, name=\"GP_Regressor\", kernel = RBF(), \n                 alpha=0.1, \n                 gp_kwargs=None):\n\n        self.name = name\n        self.kernel = kernel \n        self.alpha = alpha \n        self.gp_kwargs = gp_kwargs or {}\n        self.model = None\n\n    def fit(self, X, y): \n        \"\"\"\n        Fits the GP model to the input data.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n            y (np.ndarray): Target values of shape (n_samples,).\n        \"\"\"\n        model = GaussianProcessRegressor(kernel=self.kernel, **self.gp_kwargs)\n        model.fit(X, y)\n        self.model = model\n\n    def predict(self, X):\n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        preds, std = self.model.predict(X, return_std=True)\n        z_score = st.norm.ppf(1 - self.alpha / 2)\n        mean = preds\n        lower = mean - z_score * std\n        upper = mean + z_score * std\n        return mean, lower, upper\n\n    def save(self, path): \n        \"\"\"\n        Saves the model and its configuration to disk.\n\n        Args:\n            path (Union[str, Path]): Directory where model and config will be saved.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True) \n\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"kernel\", \"model\"]\n            and not callable(v)\n            and not isinstance(v, ())\n        }\n        config[\"kernel\"] = self.kernel.__class__.__name__\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"model.pkl\", 'wb') as file: \n            pickle.dump(self, file)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False): \n        \"\"\"\n        Loads a previously saved GPRegressor from disk.\n\n        Args:\n            path (Union[str, Path]): Path to the directory containing the saved model.\n            device (str, optional): Unused, included for compatibility. Defaults to \"cpu\".\n            load_logs (bool, optional): Unused, included for compatibility. Defaults to False.\n\n        Returns:\n            (GPRegressor): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        with open(path / \"model.pkl\", 'rb') as file: \n            model = pickle.load(file)\n\n        return model\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fits the GP model to the input data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray</code> <p>Target values of shape (n_samples,).</p> required Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fits the GP model to the input data.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n        y (np.ndarray): Target values of shape (n_samples,).\n    \"\"\"\n    model = GaussianProcessRegressor(kernel=self.kernel, **self.gp_kwargs)\n    model.fit(X, y)\n    self.model = model\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Loads a previously saved GPRegressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the directory containing the saved model.</p> required <code>device</code> <code>str</code> <p>Unused, included for compatibility. Defaults to \"cpu\".</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Unused, included for compatibility. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GPRegressor</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False): \n    \"\"\"\n    Loads a previously saved GPRegressor from disk.\n\n    Args:\n        path (Union[str, Path]): Path to the directory containing the saved model.\n        device (str, optional): Unused, included for compatibility. Defaults to \"cpu\".\n        load_logs (bool, optional): Unused, included for compatibility. Defaults to False.\n\n    Returns:\n        (GPRegressor): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    with open(path / \"model.pkl\", 'rb') as file: \n        model = pickle.load(file)\n\n    return model\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    preds, std = self.model.predict(X, return_std=True)\n    z_score = st.norm.ppf(1 - self.alpha / 2)\n    mean = preds\n    lower = mean - z_score * std\n    upper = mean + z_score * std\n    return mean, lower, upper\n</code></pre>"},{"location":"api/Bayesian/gp/#uqregressors.bayesian.gaussian_process.GPRegressor.save","title":"<code>save(path)</code>","text":"<p>Saves the model and its configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Directory where model and config will be saved.</p> required Source code in <code>uqregressors\\bayesian\\gaussian_process.py</code> <pre><code>def save(self, path): \n    \"\"\"\n    Saves the model and its configuration to disk.\n\n    Args:\n        path (Union[str, Path]): Directory where model and config will be saved.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True) \n\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"kernel\", \"model\"]\n        and not callable(v)\n        and not isinstance(v, ())\n    }\n    config[\"kernel\"] = self.kernel.__class__.__name__\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"model.pkl\", 'wb') as file: \n        pickle.dump(self, file)\n</code></pre>"},{"location":"api/Conformal/conformal_ens/","title":"uqregressors.conformal.conformal_ens","text":"<p>This method employs normalized conformal prediction as described in Tibshirani, 2023.  The difficulty measure, , used for normalization is taken to be the standard deviation of the predictions of all models in an ensemble, while the ensemble mean  is returned as the mean prediction. </p>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens--normalized-conformal-ensemble","title":"Normalized Conformal Ensemble","text":"<p>This module implements normalized conformal ensemble prediction in a split conformal context for regression on a one dimensional output </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Customizable dropout to increase ensemble diversity</li> <li>Prediction intervals without distributional assumptions  </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor","title":"<code>ConformalEnsRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Conformal Ensemble Regressor for uncertainty estimation in regression tasks. </p> <p>This class trains an ensemble of MLP models, and applies normalized conformal prediction on a split calibration set to calibrate prediction intervals. </p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model. </p> <code>'Conformal_Ens_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of models to train. </p> <code>5</code> <code>hidden_sizes</code> <code>list</code> <p>sizes of the hidden layers for each quantile regressor. </p> <code>[64, 64]</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level). </p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient, determines output type.</p> <code>False</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers. </p> <code>None</code> <code>pred_with_dropout</code> <code>bool</code> <p>Whether dropout should be applied at test time, dropout must be non-Null</p> <code>False</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function. </p> <code>'ReLU'</code> <code>cal_size</code> <code>float</code> <p>Proportion of training samples to use for calibration, between 0 and 1.  </p> <code>0.2</code> <code>gamma</code> <code>float</code> <p>Stability constant added to difficulty score . </p> <code>0</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>mse_loss</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>n_jobs</code> <code>float</code> <p>Number of parallel jobs for training.</p> <code>1</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>models</code> <code>list[QuantNN]</code> <p>A list of the models in the ensemble.</p> <code>residuals</code> <code>Tensor</code> <p>The combined residuals on the calibration set. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>class ConformalEnsRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Conformal Ensemble Regressor for uncertainty estimation in regression tasks. \n\n    This class trains an ensemble of MLP models, and applies normalized conformal prediction on a split\n    calibration set to calibrate prediction intervals. \n\n    Args: \n        name (str): Name of the model. \n        n_estimators (int): Number of models to train. \n        hidden_sizes (list): sizes of the hidden layers for each quantile regressor. \n        alpha (float): Miscoverage rate (1 - confidence level). \n        requires_grad (bool): Whether inputs should require gradient, determines output type.\n        dropout (float or None): Dropout rate for the neural network layers. \n        pred_with_dropout (bool): Whether dropout should be applied at test time, dropout must be non-Null\n        activation_str (str): String identifier of the activation function. \n        cal_size (float): Proportion of training samples to use for calibration, between 0 and 1.  \n        gamma (float): Stability constant added to difficulty score . \n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        n_jobs (float): Number of parallel jobs for training.\n        random_seed (int or None): Random seed for reproducibility.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        models (list[QuantNN]): A list of the models in the ensemble.\n        residuals (Tensor): The combined residuals on the calibration set. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n    \"\"\"\n    def __init__(self, \n                 name=\"Conformal_Ens_Regressor\",\n                 n_estimators=5, \n                 hidden_sizes=[64, 64], \n                 alpha=0.1, \n                 requires_grad=False,\n                 dropout=None,\n                 pred_with_dropout=False,\n                 activation_str=\"ReLU\",\n                 cal_size = 0.2, \n                 gamma = 0,\n                 learning_rate=1e-3,\n                 epochs=200,\n                 batch_size=32,\n                 optimizer_cls=torch.optim.Adam,\n                 optimizer_kwargs=None,\n                 scheduler_cls=None,\n                 scheduler_kwargs=None,\n                 loss_fn=nn.functional.mse_loss,\n                 device=\"cpu\", \n                 use_wandb=False, \n                 wandb_project=None, \n                 wandb_run_name=None, \n                 n_jobs=1, \n                 random_seed=None, \n                 scale_data=True, \n                 input_scaler=None,\n                 output_scaler=None,\n                 tuning_loggers = []\n    ): \n        self.name = name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.dropout = dropout\n        self.pred_with_dropout = pred_with_dropout\n        self.activation_str = activation_str\n        self.cal_size = cal_size\n        self.gamma = gamma\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self.input_dim = None\n        self.conformity_scores = None\n        self.conformity_score = None\n        self.models = []\n        self.residuals = []\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, train_idx, cal_idx, model_idx): \n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + model_idx)\n            np.random.seed(self.random_seed + model_idx)\n\n        activation = get_activation(self.activation_str)\n        model = MLP(input_dim, self.hidden_sizes, self.dropout, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_tensor[train_idx], y_tensor[train_idx])\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(model_idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{model_idx}\"\n        )\n\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss.item()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n        if self.pred_with_dropout: \n            model.train()\n        else: \n            model.eval()\n\n        test_X = X_tensor[cal_idx]\n        cal_preds = model(test_X)\n\n        logger.finish()\n        return model, cal_preds, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (ConformalEnsRegressor): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        train_idx, cal_idx = self._train_test_split(X_tensor, 0.2, self.random_seed)\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n            for i in range(self.n_estimators)\n        )\n\n        self.models = [result[0] for result in results]\n        cal_preds = torch.stack([result[1] for result in results]).squeeze()\n        self._loggers = [result[2] for result in results]\n\n        mean_cal_preds = torch.mean(cal_preds, dim=0).squeeze()\n        var_cal_preds = torch.var(cal_preds, dim=0).squeeze()\n        std_cal_preds = var_cal_preds.sqrt()\n        self.residuals = torch.abs(mean_cal_preds - y_tensor[cal_idx].squeeze())\n\n        self.conformity_scores = self.residuals / (std_cal_preds + self.gamma)\n\n        return self \n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"        \n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n+1)) \n        q = min(q, n-1) \n\n        res_quantile = n-q\n        self.conformity_score = torch.topk(self.conformity_scores, res_quantile).values[-1]\n\n        preds = []\n\n        with torch.no_grad(): \n            for model in self.models: \n                if self.pred_with_dropout: \n                    model.train()\n                else: \n                    model.eval()\n                pred = model(X_tensor)\n                preds.append(pred)\n\n        preds = torch.stack(preds)[:, :, 0]\n        mean = torch.mean(preds, dim=0)\n        variances = torch.var(preds, dim=0)\n        stds = variances.sqrt()\n        conformal_widths = self.conformity_score * (stds + self.gamma) \n        lower = mean - conformal_widths \n        upper = mean + conformal_widths \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained model and associated configuration to disk.\n\n        Args:\n            path (str or Path): Directory to save model files.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"residuals\", \"conformity_score\", \"conformity_scores\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        # Save residuals and conformity score\n        torch.save({\n            \"residuals\": self.residuals.cpu(),\n            \"conformity_score\": self.conformity_score, \n            \"conformity_scores\": self.conformity_scores\n        }, path / \"extras.pt\")\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved KFoldCQR model from disk.\n\n        Args:\n            path (str or Path): Directory containing saved model files.\n            device (str): Device to load the model on (\"cpu\" or \"cuda\").\n            load_logs (bool): Whether to also load training logs.\n\n        Returns:\n            (ConformalEnsRegressor): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = MLP(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        # Load extras\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.residuals = extras.get(\"residuals\", None)\n            model.conformity_score = extras.get(\"conformity_score\", None)\n            model.conformity_scores = extras.get(\"conformity_scores\", None)\n        else:\n            model.residuals = None\n            model.conformity_score = None\n            model.conformity_scores = None\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n\n    def _train_test_split(self, X, cal_size, seed=None):\n        \"\"\"\n        For internal use in calibration splitting only, \n        see uqregressors/utils/torch_sklearn_utils for a global version\n        \"\"\"\n        if seed is not None: \n            torch.manual_seed(seed)\n\n        n = len(X)\n        n_cal = int(np.ceil(cal_size * n))\n        all_idx = np.arange(n)\n        cal_idx = np.random.randint(n, size=n_cal)\n        mask = np.ones(n, dtype=bool)\n        mask[cal_idx] = False \n        train_idx = all_idx[mask] \n        return train_idx, cal_idx\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>ConformalEnsRegressor</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (ConformalEnsRegressor): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    train_idx, cal_idx = self._train_test_split(X_tensor, 0.2, self.random_seed)\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n        for i in range(self.n_estimators)\n    )\n\n    self.models = [result[0] for result in results]\n    cal_preds = torch.stack([result[1] for result in results]).squeeze()\n    self._loggers = [result[2] for result in results]\n\n    mean_cal_preds = torch.mean(cal_preds, dim=0).squeeze()\n    var_cal_preds = torch.var(cal_preds, dim=0).squeeze()\n    std_cal_preds = var_cal_preds.sqrt()\n    self.residuals = torch.abs(mean_cal_preds - y_tensor[cal_idx].squeeze())\n\n    self.conformity_scores = self.residuals / (std_cal_preds + self.gamma)\n\n    return self \n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved KFoldCQR model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory containing saved model files.</p> required <code>device</code> <code>str</code> <p>Device to load the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to also load training logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>ConformalEnsRegressor</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved KFoldCQR model from disk.\n\n    Args:\n        path (str or Path): Directory containing saved model files.\n        device (str): Device to load the model on (\"cpu\" or \"cuda\").\n        load_logs (bool): Whether to also load training logs.\n\n    Returns:\n        (ConformalEnsRegressor): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = MLP(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    # Load extras\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.residuals = extras.get(\"residuals\", None)\n        model.conformity_score = extras.get(\"conformity_score\", None)\n        model.conformity_scores = extras.get(\"conformity_scores\", None)\n    else:\n        model.residuals = None\n        model.conformity_score = None\n        model.conformity_scores = None\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"        \n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n+1)) \n    q = min(q, n-1) \n\n    res_quantile = n-q\n    self.conformity_score = torch.topk(self.conformity_scores, res_quantile).values[-1]\n\n    preds = []\n\n    with torch.no_grad(): \n        for model in self.models: \n            if self.pred_with_dropout: \n                model.train()\n            else: \n                model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n    preds = torch.stack(preds)[:, :, 0]\n    mean = torch.mean(preds, dim=0)\n    variances = torch.var(preds, dim=0)\n    stds = variances.sqrt()\n    conformal_widths = self.conformity_score * (stds + self.gamma) \n    lower = mean - conformal_widths \n    upper = mean + conformal_widths \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.ConformalEnsRegressor.save","title":"<code>save(path)</code>","text":"<p>Save the trained model and associated configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model files.</p> required Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained model and associated configuration to disk.\n\n    Args:\n        path (str or Path): Directory to save model files.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"residuals\", \"conformity_score\", \"conformity_scores\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    # Save residuals and conformity score\n    torch.save({\n        \"residuals\": self.residuals.cpu(),\n        \"conformity_score\": self.conformity_score, \n        \"conformity_scores\": self.conformity_scores\n    }, path / \"extras.pt\")\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/conformal_ens/#uqregressors.conformal.conformal_ens.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\conformal_ens.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__()\n        layers = []\n        for h in hidden_sizes: \n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim=h \n        output_layer = nn.Linear(hidden_sizes[-1], 1)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x): \n        return self.model(x)\n</code></pre>"},{"location":"api/Conformal/cqr/","title":"uqregressors.conformal.cqr","text":"<p>This class implements split conformal quantile regression as described by Romano et al. 2018</p> <p>Tip</p> <p>The quantiles of the underlying quantile regressor can be tuned with the parameters tau_lo and tau_hi as in the paper. This can often result in more efficient intervals. </p>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr--conformalized-quantile-regression-cqr","title":"Conformalized Quantile Regression (CQR)","text":"<p>This module implements CQR in a split conformal context for regression on a one dimensional output </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Tunable quantiles of the underyling quantile regressor</li> <li>Prediction intervals without distributional assumptions  </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor","title":"<code>ConformalQuantileRegressor</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>Conformalized Quantile Regressor for uncertainty estimation in regression tasks.</p> <p>This class trains one quantile neural network and conformalizes it with split conformal prediction</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'Conformal_Quantile_Regressor'</code> <code>hidden_sizes</code> <code>list</code> <p>Sizes of the hidden layers for each quantile regressor.</p> <code>[64, 64]</code> <code>cal_size</code> <code>float</code> <p>Proportion of training samples to use for calibration, between 0 and 1. </p> <code>0.2</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient.</p> <code>False</code> <code>tau_lo</code> <code>float</code> <p>Lower quantile, defaults to alpha/2.</p> <code>None</code> <code>tau_hi</code> <code>float</code> <p>Upper quantile, defaults to 1 - alpha/2.</p> <code>None</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function.</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>quantiles</code> <code>Tensor</code> <p>The lower and upper quantiles for prediction.</p> <code>residuals</code> <code>Tensor</code> <p>The residuals on the calibration set. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>class ConformalQuantileRegressor(BaseEstimator, RegressorMixin): \n    \"\"\"\n    Conformalized Quantile Regressor for uncertainty estimation in regression tasks.\n\n    This class trains one quantile neural network and conformalizes it with split conformal prediction\n\n    Args:\n        name (str): Name of the model.\n        hidden_sizes (list): Sizes of the hidden layers for each quantile regressor.\n        cal_size (float): Proportion of training samples to use for calibration, between 0 and 1. \n        dropout (float or None): Dropout rate for the neural network layers.\n        alpha (float): Miscoverage rate (1 - confidence level).\n        requires_grad (bool): Whether inputs should require gradient.\n        tau_lo (float): Lower quantile, defaults to alpha/2.\n        tau_hi (float): Upper quantile, defaults to 1 - alpha/2.\n        activation_str (str): String identifier of the activation function.\n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        quantiles (Tensor): The lower and upper quantiles for prediction.\n        residuals (Tensor): The residuals on the calibration set. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n    \"\"\"\n    def __init__(\n            self, \n            name=\"Conformal_Quantile_Regressor\",\n            hidden_sizes = [64, 64],\n            cal_size = 0.2, \n            dropout = None, \n            alpha = 0.1, \n            requires_grad = False, \n            tau_lo = None, \n            tau_hi = None,\n            activation_str=\"ReLU\",\n            learning_rate=1e-3,\n            epochs=200, \n            batch_size=32,\n            optimizer_cls = torch.optim.Adam, \n            optimizer_kwargs=None, \n            scheduler_cls=None, \n            scheduler_kwargs=None, \n            loss_fn=None, \n            device=\"cpu\", \n            use_wandb=False, \n            wandb_project=None,\n            wandb_run_name=None,\n            scale_data=True, \n            input_scaler=None,\n            output_scaler=None, \n            random_seed=None,\n            tuning_loggers = []\n    ):\n        self.name = name\n        self.hidden_sizes = hidden_sizes \n        self.cal_size = cal_size \n        self.dropout = dropout \n        self.alpha = alpha \n        self.requires_grad = requires_grad\n        self.tau_lo = tau_lo or alpha / 2 \n        self.tau_hi = tau_hi or 1 - alpha / 2\n        self.activation_str = activation_str \n        self.learning_rate = learning_rate \n        self.epochs = epochs \n        self.batch_size = batch_size \n        self.optimizer_cls = optimizer_cls \n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.quantile_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.random_seed = random_seed\n\n        self.quantiles = torch.tensor([self.tau_lo, self.tau_hi], device=self.device)\n\n        self.residuals = [] \n        self.conformal_width = None \n        self.input_dim = None\n\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n\n    def quantile_loss(self, preds, y): \n        \"\"\"\n        Quantile loss used for training the quantile regressor.\n\n        Args:\n            preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n            y (Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (Tensor): Scalar loss.\n        \"\"\"\n        error = y.view(-1, 1) - preds \n        return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the conformal quantile regressor model on training data. \n\n        Args:\n            X (array-like): Training features of shape (n_samples, n_features).\n            y (array-like): Target values of shape (n_samples,).\n        \"\"\"\n        X, y = validate_and_prepare_inputs(X, y, device=self.device)\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X = self.input_scaler.fit_transform(X)\n            y = self.output_scaler.fit_transform(y.reshape(-1, 1))\n\n        X_train, X_cal, y_train, y_cal = train_test_split(X, y, test_size=self.cal_size, random_state=self.random_seed, device=self.device, shuffle=True)\n\n        input_dim = X.shape[1]\n        self.input_dim = input_dim \n\n        config = {\n            \"learning_rate\": self.learning_rate,\n            \"epochs\": self.epochs,\n            \"batch_size\": self.batch_size,\n        }\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name,\n            config=config,\n        )\n\n        activation = get_activation(self.activation_str)\n\n        self.model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n        self.model.to(self.device)\n\n        optimizer = self.optimizer_cls(\n            self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n\n        scheduler = None\n        if self.scheduler_cls is not None:\n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        self.model.train()\n        for epoch in range(self.epochs):\n            epoch_loss = 0.0\n            for xb, yb in dataloader: \n                optimizer.zero_grad()\n                preds = self.model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward()\n                optimizer.step()\n                epoch_loss += loss\n\n            if scheduler is not None:\n                scheduler.step()\n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n        oof_preds = self.model(X_cal)\n        loss_matrix = (oof_preds - y_cal) * torch.tensor([1, -1], device=self.device)\n        self.residuals = torch.max(loss_matrix, dim=1).values\n\n        logger.finish()\n        self._loggers.append(logger)\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        self.model.eval()\n\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n + 1))\n        q = min(q, n-1)\n        res_quantile = n-q\n\n        self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed)\n            np.random.seed(self.random_seed)\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = self.model(X_tensor)\n        lower_cq = preds[:, 0].unsqueeze(dim=1)\n        upper_cq = preds[:, 1].unsqueeze(dim=1)\n        lower = lower_cq - self.conformal_width \n        upper = upper_cq + self.conformal_width \n        mean = (lower + upper) / 2 \n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean).squeeze()\n            lower = self.output_scaler.inverse_transform(lower).squeeze()\n            upper = self.output_scaler.inverse_transform(upper).squeeze()\n        else: \n            mean = mean.squeeze() \n            lower = lower.squeeze() \n            upper = upper.squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path): \n        \"\"\"\n        Save model weights, config, and scalers to disk.\n\n        Args:\n            path (str or Path): Directory to save model components.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"model\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \"output_scaler\", \"quantiles\", \n                         \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                         self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        # Save model weights\n        torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n        torch.save({\n            \"conformal_width\": self.conformal_width, \n            \"residuals\": self.residuals,\n            \"quantiles\": self.quantiles\n        }, path / \"extras.pt\")\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False): \n        \"\"\"\n        Load a saved MC dropout regressor from disk.\n\n        Args:\n            path (str or pathlib.Path): Directory path to load the model from.\n            device (str or torch.device): Device to load the model onto.\n            load_logs (bool): Whether to load training and tuning logs.\n\n        Returns:\n            (ConformalQuantileRegressor): Loaded model instance.\n        \"\"\"\n        path = Path(path)\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        model = cls(**config)\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n\n        model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n        model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.residuals = extras.get(\"residuals\", None)\n            model.conformal_width = extras.get(\"conformal_width\", None)\n            model.quantiles = extras.get(\"quantiles\", None)\n        else:\n            model.residuals = None\n            model.conformal_width = None\n            model.quantiles = None\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler \n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the conformal quantile regressor model on training data. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Training features of shape (n_samples, n_features).</p> required <code>y</code> <code>array - like</code> <p>Target values of shape (n_samples,).</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the conformal quantile regressor model on training data. \n\n    Args:\n        X (array-like): Training features of shape (n_samples, n_features).\n        y (array-like): Target values of shape (n_samples,).\n    \"\"\"\n    X, y = validate_and_prepare_inputs(X, y, device=self.device)\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X = self.input_scaler.fit_transform(X)\n        y = self.output_scaler.fit_transform(y.reshape(-1, 1))\n\n    X_train, X_cal, y_train, y_cal = train_test_split(X, y, test_size=self.cal_size, random_state=self.random_seed, device=self.device, shuffle=True)\n\n    input_dim = X.shape[1]\n    self.input_dim = input_dim \n\n    config = {\n        \"learning_rate\": self.learning_rate,\n        \"epochs\": self.epochs,\n        \"batch_size\": self.batch_size,\n    }\n\n    logger = Logger(\n        use_wandb=self.use_wandb,\n        project_name=self.wandb_project,\n        run_name=self.wandb_run_name,\n        config=config,\n    )\n\n    activation = get_activation(self.activation_str)\n\n    self.model = MLP(self.input_dim, self.hidden_sizes, self.dropout, activation)\n    self.model.to(self.device)\n\n    optimizer = self.optimizer_cls(\n        self.model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n    )\n\n    scheduler = None\n    if self.scheduler_cls is not None:\n        scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n    dataset = TensorDataset(X_train, y_train)\n    dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n    self.model.train()\n    for epoch in range(self.epochs):\n        epoch_loss = 0.0\n        for xb, yb in dataloader: \n            optimizer.zero_grad()\n            preds = self.model(xb)\n            loss = self.loss_fn(preds, yb)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss\n\n        if scheduler is not None:\n            scheduler.step()\n\n        if epoch % (self.epochs / 20) == 0:\n            logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n    oof_preds = self.model(X_cal)\n    loss_matrix = (oof_preds - y_cal) * torch.tensor([1, -1], device=self.device)\n    self.residuals = torch.max(loss_matrix, dim=1).values\n\n    logger.finish()\n    self._loggers.append(logger)\n    return self\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved MC dropout regressor from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory path to load the model from.</p> required <code>device</code> <code>str or device</code> <p>Device to load the model onto.</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to load training and tuning logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>ConformalQuantileRegressor</code> <p>Loaded model instance.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False): \n    \"\"\"\n    Load a saved MC dropout regressor from disk.\n\n    Args:\n        path (str or pathlib.Path): Directory path to load the model from.\n        device (str or torch.device): Device to load the model onto.\n        load_logs (bool): Whether to load training and tuning logs.\n\n    Returns:\n        (ConformalQuantileRegressor): Loaded model instance.\n    \"\"\"\n    path = Path(path)\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    model = cls(**config)\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n\n    model.model = MLP(model.input_dim, config[\"hidden_sizes\"], model.dropout, activation).to(device)\n    model.model.load_state_dict(torch.load(path / f\"model.pt\", map_location=device))\n\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.residuals = extras.get(\"residuals\", None)\n        model.conformal_width = extras.get(\"conformal_width\", None)\n        model.quantiles = extras.get(\"quantiles\", None)\n    else:\n        model.residuals = None\n        model.conformal_width = None\n        model.quantiles = None\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler \n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    self.model.eval()\n\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n + 1))\n    q = min(q, n-1)\n    res_quantile = n-q\n\n    self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n    if self.random_seed is not None: \n        torch.manual_seed(self.random_seed)\n        np.random.seed(self.random_seed)\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = self.model(X_tensor)\n    lower_cq = preds[:, 0].unsqueeze(dim=1)\n    upper_cq = preds[:, 1].unsqueeze(dim=1)\n    lower = lower_cq - self.conformal_width \n    upper = upper_cq + self.conformal_width \n    mean = (lower + upper) / 2 \n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean).squeeze()\n        lower = self.output_scaler.inverse_transform(lower).squeeze()\n        upper = self.output_scaler.inverse_transform(upper).squeeze()\n    else: \n        mean = mean.squeeze() \n        lower = lower.squeeze() \n        upper = upper.squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.quantile_loss","title":"<code>quantile_loss(preds, y)</code>","text":"<p>Quantile loss used for training the quantile regressor.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted quantiles, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss.</p> Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def quantile_loss(self, preds, y): \n    \"\"\"\n    Quantile loss used for training the quantile regressor.\n\n    Args:\n        preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n        y (Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (Tensor): Scalar loss.\n    \"\"\"\n    error = y.view(-1, 1) - preds \n    return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.ConformalQuantileRegressor.save","title":"<code>save(path)</code>","text":"<p>Save model weights, config, and scalers to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model components.</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>def save(self, path): \n    \"\"\"\n    Save model weights, config, and scalers to disk.\n\n    Args:\n        path (str or Path): Directory to save model components.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"model\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \"input_scaler\", \"output_scaler\", \"quantiles\", \n                     \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                     self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    # Save model weights\n    torch.save(self.model.state_dict(), path / f\"model.pt\")\n\n    torch.save({\n        \"conformal_width\": self.conformal_width, \n        \"residuals\": self.residuals,\n        \"quantiles\": self.quantiles\n    }, path / \"extras.pt\")\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/cqr/#uqregressors.conformal.cqr.MLP","title":"<code>MLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple feedforward neural network with dropout for regression.</p> <p>This MLP supports customizable hidden layer sizes, activation functions, and dropout. It outputs a single scalar per input \u2014 the predictive mean.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>hidden_sizes</code> <code>list of int</code> <p>Sizes of the hidden layers.</p> required <code>dropout</code> <code>float or None</code> <p>Dropout rate (applied after each activation).</p> required <code>activation</code> <code>callable</code> <p>Activation function (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\cqr.py</code> <pre><code>class MLP(nn.Module): \n    \"\"\"\n    A simple feedforward neural network with dropout for regression.\n\n    This MLP supports customizable hidden layer sizes, activation functions,\n    and dropout. It outputs a single scalar per input \u2014 the predictive mean.\n\n    Args:\n        input_dim (int): Number of input features.\n        hidden_sizes (list of int): Sizes of the hidden layers.\n        dropout (float or None): Dropout rate (applied after each activation).\n        activation (callable): Activation function (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__() \n        layers = [] \n        for h in hidden_sizes: \n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim = h \n        layers.append(nn.Linear(hidden_sizes[-1], 2))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x): \n        return self.model(x)\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/","title":"uqregressors.conformal.k_fold_cqr","text":"<p>This class implements conformal quantile regression in a K-fold manner to obtain uncertainty estimates which are often conservative, but use the entire dataset available.  This can result in large improvements over split conformal quantile regression, particularly in cases where the dataset is sparse. </p> <p>Tip</p> <p>The quantiles of the underlying quantile regressor can be tuned with the parameters tau_lo and tau_hi as in the paper. This can often result in more efficient intervals. </p> <p>Note</p> <p>K-fold Conformal Quantile Regression can be overly conservative in prediction intervals, particularly in sparse data settings or when the underlying estimator has high variance.</p>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr--k-fold-cqr","title":"K-Fold-CQR","text":"<p>This module implements conformal quantile regression in a K-fold manner for regression of a one dimensional output. </p> Key features are <ul> <li>Customizable neural network architecture</li> <li>Tunable quantiles of the underyling regressors</li> <li>Prediction intervals without distributional assumptions </li> <li>Parallel training of ensemble models with Joblib </li> <li>Customizable optimizer and loss function </li> <li>Optional Input/Output Normalization</li> </ul>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR","title":"<code>KFoldCQR</code>","text":"<p>               Bases: <code>BaseEstimator</code>, <code>RegressorMixin</code></p> <p>K-Fold Conformalized Quantile Regressor for uncertainty estimation in regression tasks.</p> <p>This class trains an ensemble of quantile neural networks using K-Fold cross-validation, and applies conformal prediction to calibrate prediction intervals.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model.</p> <code>'K_Fold_CQR_Regressor'</code> <code>n_estimators</code> <code>int</code> <p>Number of K-Fold models to train.</p> <code>5</code> <code>hidden_sizes</code> <code>list</code> <p>Sizes of the hidden layers for each quantile regressor.</p> <code>[64, 64]</code> <code>dropout</code> <code>float or None</code> <p>Dropout rate for the neural network layers.</p> <code>None</code> <code>alpha</code> <code>float</code> <p>Miscoverage rate (1 - confidence level).</p> <code>0.1</code> <code>requires_grad</code> <code>bool</code> <p>Whether inputs should require gradient.</p> <code>False</code> <code>tau_lo</code> <code>float</code> <p>Lower quantile, defaults to alpha/2.</p> <code>None</code> <code>tau_hi</code> <code>float</code> <p>Upper quantile, defaults to 1 - alpha/2.</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>Number of parallel jobs for training.</p> <code>1</code> <code>activation_str</code> <code>str</code> <p>String identifier of the activation function.</p> <code>'ReLU'</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for training.</p> <code>0.001</code> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>200</code> <code>batch_size</code> <code>int</code> <p>Batch size for training.</p> <code>32</code> <code>optimizer_cls</code> <code>type</code> <p>Optimizer class.</p> <code>Adam</code> <code>optimizer_kwargs</code> <code>dict</code> <p>Keyword arguments for optimizer.</p> <code>None</code> <code>scheduler_cls</code> <code>type or None</code> <p>Learning rate scheduler class.</p> <code>None</code> <code>scheduler_kwargs</code> <code>dict</code> <p>Keyword arguments for scheduler.</p> <code>None</code> <code>loss_fn</code> <code>callable or None</code> <p>Loss function, defaults to quantile loss.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for training and inference.</p> <code>'cpu'</code> <code>use_wandb</code> <code>bool</code> <p>Whether to log training with Weights &amp; Biases.</p> <code>False</code> <code>wandb_project</code> <code>str or None</code> <p>wandb project name.</p> <code>None</code> <code>wandb_run_name</code> <code>str or None</code> <p>wandb run name.</p> <code>None</code> <code>scale_data</code> <code>bool</code> <p>Whether to normalize input/output data.</p> <code>True</code> <code>input_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for input features.</p> <code>None</code> <code>output_scaler</code> <code>TorchStandardScaler</code> <p>Scaler for target outputs.</p> <code>None</code> <code>random_seed</code> <code>int or None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>tuning_loggers</code> <code>list</code> <p>Optional list of loggers for tuning.</p> <code>[]</code> <p>Attributes:</p> Name Type Description <code>quantiles</code> <code>Tensor</code> <p>The lower and upper quantiles for prediction.</p> <code>models</code> <code>list[QuantNN]</code> <p>A list of the models in the ensemble.</p> <code>residuals</code> <code>Tensor</code> <p>The combined residuals on the calibration sets. </p> <code>conformal_width</code> <code>Tensor</code> <p>The width needed to conformalize the quantile regressor, q. </p> <code>_loggers</code> <code>list[Logger]</code> <p>Training loggers for each ensemble member.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>class KFoldCQR(BaseEstimator, RegressorMixin): \n    \"\"\"\n    K-Fold Conformalized Quantile Regressor for uncertainty estimation in regression tasks.\n\n    This class trains an ensemble of quantile neural networks using K-Fold cross-validation,\n    and applies conformal prediction to calibrate prediction intervals.\n\n    Args:\n        name (str): Name of the model.\n        n_estimators (int): Number of K-Fold models to train.\n        hidden_sizes (list): Sizes of the hidden layers for each quantile regressor.\n        dropout (float or None): Dropout rate for the neural network layers.\n        alpha (float): Miscoverage rate (1 - confidence level).\n        requires_grad (bool): Whether inputs should require gradient.\n        tau_lo (float): Lower quantile, defaults to alpha/2.\n        tau_hi (float): Upper quantile, defaults to 1 - alpha/2.\n        n_jobs (int): Number of parallel jobs for training.\n        activation_str (str): String identifier of the activation function.\n        learning_rate (float): Learning rate for training.\n        epochs (int): Number of training epochs.\n        batch_size (int): Batch size for training.\n        optimizer_cls (type): Optimizer class.\n        optimizer_kwargs (dict): Keyword arguments for optimizer.\n        scheduler_cls (type or None): Learning rate scheduler class.\n        scheduler_kwargs (dict): Keyword arguments for scheduler.\n        loss_fn (callable or None): Loss function, defaults to quantile loss.\n        device (str): Device to use for training and inference.\n        use_wandb (bool): Whether to log training with Weights &amp; Biases.\n        wandb_project (str or None): wandb project name.\n        wandb_run_name (str or None): wandb run name.\n        scale_data (bool): Whether to normalize input/output data.\n        input_scaler (TorchStandardScaler): Scaler for input features.\n        output_scaler (TorchStandardScaler): Scaler for target outputs.\n        random_seed (int or None): Random seed for reproducibility.\n        tuning_loggers (list): Optional list of loggers for tuning.\n\n    Attributes: \n        quantiles (Tensor): The lower and upper quantiles for prediction.\n        models (list[QuantNN]): A list of the models in the ensemble.\n        residuals (Tensor): The combined residuals on the calibration sets. \n        conformal_width (Tensor): The width needed to conformalize the quantile regressor, q. \n        _loggers (list[Logger]): Training loggers for each ensemble member. \n    \"\"\"\n    def __init__(\n            self, \n            name=\"K_Fold_CQR_Regressor\",\n            n_estimators=5,\n            hidden_sizes=[64, 64], \n            dropout = None,\n            alpha=0.1, \n            requires_grad=False,\n            tau_lo = None, \n            tau_hi = None, \n            n_jobs=1, \n            activation_str=\"ReLU\",\n            learning_rate=1e-3,\n            epochs=200,\n            batch_size=32,\n            optimizer_cls=torch.optim.Adam,\n            optimizer_kwargs=None,\n            scheduler_cls=None,\n            scheduler_kwargs=None,\n            loss_fn=None,\n            device=\"cpu\",\n            use_wandb=False,\n            wandb_project=None,\n            wandb_run_name=None,\n            scale_data = True, \n            input_scaler = None, \n            output_scaler = None,\n            random_seed=None, \n            tuning_loggers = []\n    ):\n        self.name = name\n        self.n_estimators = n_estimators\n        self.hidden_sizes = hidden_sizes\n        self.dropout = dropout\n        self.alpha = alpha\n        self.requires_grad = requires_grad\n        self.tau_lo = tau_lo or alpha / 2 \n        self.tau_hi = tau_hi or 1 - alpha / 2\n        self.activation_str = activation_str\n        self.learning_rate = learning_rate\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.optimizer_cls = optimizer_cls\n        self.optimizer_kwargs = optimizer_kwargs or {}\n        self.scheduler_cls = scheduler_cls\n        self.scheduler_kwargs = scheduler_kwargs or {}\n        self.loss_fn = loss_fn or self.quantile_loss\n        self.device = device\n\n        self.use_wandb = use_wandb\n        self.wandb_project = wandb_project\n        self.wandb_run_name = wandb_run_name\n\n        self.n_jobs = n_jobs\n        self.random_seed = random_seed\n        self.quantiles = torch.tensor([self.tau_lo, self.tau_hi], device=self.device)\n        self.models = []\n        self.residuals = []\n        self.conformal_width = None\n        self.input_dim = None\n        if self.n_estimators == 1: \n            raise ValueError(\"n_estimators set to 1. To use a single Quantile Regressor, use a non-ensembled Quantile Regressor class\")\n        self.scale_data = scale_data \n        self.input_scaler = input_scaler or TorchStandardScaler() \n        self.output_scaler = output_scaler or TorchStandardScaler()\n\n        self._loggers = []\n        self.training_logs = None\n        self.tuning_loggers = tuning_loggers\n        self.tuning_logs = None\n\n\n    def quantile_loss(self, preds, y): \n        \"\"\"\n        Quantile loss used for training the quantile regressors.\n\n        Args:\n            preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n            y (Tensor): True target values, shape (batch_size,).\n\n        Returns:\n            (Tensor): Scalar loss.\n        \"\"\"\n        error = y.view(-1, 1) - preds\n        return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n\n    def _train_single_model(self, X_tensor, y_tensor, input_dim, train_idx, cal_idx, model_idx): \n        if self.random_seed is not None: \n            torch.manual_seed(self.random_seed + model_idx)\n            np.random.seed(self.random_seed + model_idx)\n\n        activation = get_activation(self.activation_str)\n        model = QuantNN(input_dim, self.hidden_sizes, self.dropout, activation).to(self.device)\n\n        optimizer = self.optimizer_cls(\n            model.parameters(), lr=self.learning_rate, **self.optimizer_kwargs\n        )\n        scheduler = None \n        if self.scheduler_cls: \n            scheduler = self.scheduler_cls(optimizer, **self.scheduler_kwargs)\n\n        X_train = X_tensor.detach()[train_idx]\n        y_train = y_tensor.detach()[train_idx]\n        dataset = TensorDataset(X_train, y_train)\n        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n\n        logger = Logger(\n            use_wandb=self.use_wandb,\n            project_name=self.wandb_project,\n            run_name=self.wandb_run_name + str(model_idx) if self.wandb_run_name is not None else None,\n            config={\"n_estimators\": self.n_estimators, \"learning_rate\": self.learning_rate, \"epochs\": self.epochs},\n            name=f\"Estimator-{model_idx}\"\n        )\n\n        model.train()\n        for epoch in range(self.epochs): \n            model.train()\n            epoch_loss = 0.0 \n            for xb, yb in dataloader: \n                optimizer.zero_grad() \n                preds = model(xb)\n                loss = self.loss_fn(preds, yb)\n                loss.backward() \n                optimizer.step() \n                epoch_loss += loss \n\n            if epoch % (self.epochs / 20) == 0:\n                logger.log({\"epoch\": epoch, \"train_loss\": epoch_loss})\n\n            if scheduler: \n                scheduler.step()\n\n\n        test_X = X_tensor[cal_idx]\n        test_y = y_tensor[cal_idx]\n        oof_preds = model(test_X)\n        loss_matrix =(oof_preds - test_y) * torch.tensor([1.0, -1.0], device=self.device)\n        residuals = torch.max(loss_matrix, dim=1).values\n        logger.finish()\n        return model, residuals, logger\n\n    def fit(self, X, y): \n        \"\"\"\n        Fit the ensemble on training data.\n\n        Args:\n            X (array-like or torch.Tensor): Training inputs.\n            y (array-like or torch.Tensor): Training targets.\n\n        Returns:\n            (KFoldCQR): Fitted estimator.\n        \"\"\"\n        X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n        input_dim = X_tensor.shape[1]\n        self.input_dim = input_dim\n\n\n        if self.scale_data:\n            X_tensor = self.input_scaler.fit_transform(X_tensor)\n            y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n        kf = TorchKFold(n_splits=self.n_estimators, shuffle=True)\n\n        results = Parallel(n_jobs=self.n_jobs)(\n            delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n            for i, (train_idx, cal_idx) in enumerate(kf.split(X_tensor))\n        )\n\n        self.models = [result[0] for result in results]\n        self.residuals = torch.cat([result[1] for result in results], dim=0).ravel()\n        self._loggers = [result[2] for result in results]\n\n        return self\n\n    def predict(self, X): \n        \"\"\"\n        Predicts the target values with uncertainty estimates.\n\n        Args:\n            X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n        Returns:\n            (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n                mean predictions,\n                lower bound of the prediction interval,\n                upper bound of the prediction interval.\n\n        !!! note\n            If `requires_grad` is False, all returned arrays are NumPy arrays.\n            Otherwise, they are PyTorch tensors with gradients.\n        \"\"\"\n        X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n        n = len(self.residuals)\n        q = int((1 - self.alpha) * (n + 1))\n        q = min(q, n-1)\n\n        res_quantile = n-q\n\n        self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n        if self.scale_data: \n            X_tensor = self.input_scaler.transform(X_tensor)\n\n        preds = [] \n\n        with torch.no_grad(): \n            for model in self.models: \n                model.eval()\n                pred = model(X_tensor)\n                preds.append(pred)\n\n        preds = torch.stack(preds)\n\n        means = torch.mean(preds, dim=2) \n        mean = torch.mean(means, dim=0)\n\n        lower_cq = torch.mean(preds[:, :, 0], dim=0)\n        upper_cq = torch.mean(preds[:, :, 1], dim=0)\n\n        lower = lower_cq - self.conformal_width\n        upper = upper_cq + self.conformal_width\n\n        if self.scale_data: \n            mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n            lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n            upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n        if not self.requires_grad: \n            return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n        else: \n            return mean, lower, upper\n\n    def save(self, path):\n        \"\"\"\n        Save the trained model and associated configuration to disk.\n\n        Args:\n            path (str or Path): Directory to save model files.\n        \"\"\"\n        path = Path(path)\n        path.mkdir(parents=True, exist_ok=True)\n\n        # Save config (exclude non-serializable or large objects)\n        config = {\n            k: v for k, v in self.__dict__.items()\n            if k not in [\"models\", \"quantiles\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                         \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n            and not callable(v)\n            and not isinstance(v, (torch.nn.Module,))\n        }\n\n        config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n        config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n        config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n        config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n        with open(path / \"config.json\", \"w\") as f:\n            json.dump(config, f, indent=4)\n\n        # Save model weights\n        for i, model in enumerate(self.models):\n            torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n        # Save residuals and conformity score\n        torch.save({\n            \"conformal_width\": self.conformal_width, \n            \"residuals\": self.residuals,\n            \"quantiles\": self.quantiles,\n        }, path / \"extras.pt\")\n\n        with open(path / \"extras.pkl\", 'wb') as f: \n            pickle.dump([self.optimizer_cls, \n                        self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n        for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n            logger.save_to_file(path, idx=i, name=\"estimator\")\n\n        for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n            logger.save_to_file(path, name=\"tuning\", idx=i)\n\n    @classmethod\n    def load(cls, path, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Load a saved KFoldCQR model from disk.\n\n        Args:\n            path (str or Path): Directory containing saved model files.\n            device (str): Device to load the model on (\"cpu\" or \"cuda\").\n            load_logs (bool): Whether to also load training logs.\n\n        Returns:\n            (KFoldCQR): The loaded model instance.\n        \"\"\"\n        path = Path(path)\n\n        # Load config\n        with open(path / \"config.json\", \"r\") as f:\n            config = json.load(f)\n        config[\"device\"] = device\n\n        config.pop(\"optimizer\", None)\n        config.pop(\"scheduler\", None)\n        config.pop(\"input_scaler\", None)\n        config.pop(\"output_scaler\", None)\n\n        input_dim = config.pop(\"input_dim\", None)\n        model = cls(**config)\n\n        # Recreate models\n        model.input_dim = input_dim\n        activation = get_activation(config[\"activation_str\"])\n        model.models = []\n        for i in range(config[\"n_estimators\"]):\n            m = QuantNN(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n            m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n            model.models.append(m)\n\n        # Load extras\n        extras_path = path / \"extras.pt\"\n        if extras_path.exists():\n            extras = torch.load(extras_path, map_location=device, weights_only=False)\n            model.conformal_width = extras.get(\"conformal_width\", None)\n            model.residuals = extras.get(\"residuals\", None)\n            model.quantiles = extras.get(\"quantiles\", None)\n        else:\n            model.conformal_width = None\n            model.residuals = None\n            model.quantiles = None\n\n        with open(path / \"extras.pkl\", 'rb') as f: \n            optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n        model.optimizer_cls = optimizer_cls \n        model.optimizer_kwargs = optimizer_kwargs \n        model.scheduler_cls = scheduler_cls \n        model.scheduler_kwargs = scheduler_kwargs\n        model.input_scaler = input_scaler\n        model.output_scaler = output_scaler\n\n        if load_logs: \n            logs_path = path / \"logs\"\n            training_logs = [] \n            tuning_logs = []\n            if logs_path.exists() and logs_path.is_dir(): \n                estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n                for log_file in estimator_log_files:\n                    with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                        training_logs.append(f.read())\n\n                tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n                for log_file in tuning_log_files: \n                    with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                        tuning_logs.append(f.read())\n\n            model.training_logs = training_logs\n            model.tuning_logs = tuning_logs\n\n        return model\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the ensemble on training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Training inputs.</p> required <code>y</code> <code>array - like or Tensor</code> <p>Training targets.</p> required <p>Returns:</p> Type Description <code>KFoldCQR</code> <p>Fitted estimator.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def fit(self, X, y): \n    \"\"\"\n    Fit the ensemble on training data.\n\n    Args:\n        X (array-like or torch.Tensor): Training inputs.\n        y (array-like or torch.Tensor): Training targets.\n\n    Returns:\n        (KFoldCQR): Fitted estimator.\n    \"\"\"\n    X_tensor, y_tensor = validate_and_prepare_inputs(X, y, device=self.device, requires_grad=self.requires_grad)\n    input_dim = X_tensor.shape[1]\n    self.input_dim = input_dim\n\n\n    if self.scale_data:\n        X_tensor = self.input_scaler.fit_transform(X_tensor)\n        y_tensor = self.output_scaler.fit_transform(y_tensor)\n\n    kf = TorchKFold(n_splits=self.n_estimators, shuffle=True)\n\n    results = Parallel(n_jobs=self.n_jobs)(\n        delayed(self._train_single_model)(X_tensor, y_tensor, input_dim, train_idx, cal_idx, i)\n        for i, (train_idx, cal_idx) in enumerate(kf.split(X_tensor))\n    )\n\n    self.models = [result[0] for result in results]\n    self.residuals = torch.cat([result[1] for result in results], dim=0).ravel()\n    self._loggers = [result[2] for result in results]\n\n    return self\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.load","title":"<code>load(path, device='cpu', load_logs=False)</code>  <code>classmethod</code>","text":"<p>Load a saved KFoldCQR model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory containing saved model files.</p> required <code>device</code> <code>str</code> <p>Device to load the model on (\"cpu\" or \"cuda\").</p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether to also load training logs.</p> <code>False</code> <p>Returns:</p> Type Description <code>KFoldCQR</code> <p>The loaded model instance.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>@classmethod\ndef load(cls, path, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Load a saved KFoldCQR model from disk.\n\n    Args:\n        path (str or Path): Directory containing saved model files.\n        device (str): Device to load the model on (\"cpu\" or \"cuda\").\n        load_logs (bool): Whether to also load training logs.\n\n    Returns:\n        (KFoldCQR): The loaded model instance.\n    \"\"\"\n    path = Path(path)\n\n    # Load config\n    with open(path / \"config.json\", \"r\") as f:\n        config = json.load(f)\n    config[\"device\"] = device\n\n    config.pop(\"optimizer\", None)\n    config.pop(\"scheduler\", None)\n    config.pop(\"input_scaler\", None)\n    config.pop(\"output_scaler\", None)\n\n    input_dim = config.pop(\"input_dim\", None)\n    model = cls(**config)\n\n    # Recreate models\n    model.input_dim = input_dim\n    activation = get_activation(config[\"activation_str\"])\n    model.models = []\n    for i in range(config[\"n_estimators\"]):\n        m = QuantNN(model.input_dim, config[\"hidden_sizes\"], config[\"dropout\"], activation).to(device)\n        m.load_state_dict(torch.load(path / f\"model_{i}.pt\", map_location=device))\n        model.models.append(m)\n\n    # Load extras\n    extras_path = path / \"extras.pt\"\n    if extras_path.exists():\n        extras = torch.load(extras_path, map_location=device, weights_only=False)\n        model.conformal_width = extras.get(\"conformal_width\", None)\n        model.residuals = extras.get(\"residuals\", None)\n        model.quantiles = extras.get(\"quantiles\", None)\n    else:\n        model.conformal_width = None\n        model.residuals = None\n        model.quantiles = None\n\n    with open(path / \"extras.pkl\", 'rb') as f: \n        optimizer_cls, optimizer_kwargs, scheduler_cls, scheduler_kwargs, input_scaler, output_scaler = pickle.load(f)\n\n    model.optimizer_cls = optimizer_cls \n    model.optimizer_kwargs = optimizer_kwargs \n    model.scheduler_cls = scheduler_cls \n    model.scheduler_kwargs = scheduler_kwargs\n    model.input_scaler = input_scaler\n    model.output_scaler = output_scaler\n\n    if load_logs: \n        logs_path = path / \"logs\"\n        training_logs = [] \n        tuning_logs = []\n        if logs_path.exists() and logs_path.is_dir(): \n            estimator_log_files = sorted(logs_path.glob(\"estimator_*.log\"))\n            for log_file in estimator_log_files:\n                with open(log_file, \"r\", encoding=\"utf-8\") as f:\n                    training_logs.append(f.read())\n\n            tuning_log_files = sorted(logs_path.glob(\"tuning_*.log\"))\n            for log_file in tuning_log_files: \n                with open(log_file, \"r\", encoding=\"utf-8\") as f: \n                    tuning_logs.append(f.read())\n\n        model.training_logs = training_logs\n        model.tuning_logs = tuning_logs\n\n    return model\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.predict","title":"<code>predict(X)</code>","text":"<p>Predicts the target values with uncertainty estimates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>Feature matrix of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray, ndarray], Tuple[Tensor, Tensor, Tensor]]</code> <p>Tuple containing: mean predictions, lower bound of the prediction interval, upper bound of the prediction interval.</p> <p>Note</p> <p>If <code>requires_grad</code> is False, all returned arrays are NumPy arrays. Otherwise, they are PyTorch tensors with gradients.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def predict(self, X): \n    \"\"\"\n    Predicts the target values with uncertainty estimates.\n\n    Args:\n        X (np.ndarray): Feature matrix of shape (n_samples, n_features).\n\n    Returns:\n        (Union[Tuple[np.ndarray, np.ndarray, np.ndarray], Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]): Tuple containing:\n            mean predictions,\n            lower bound of the prediction interval,\n            upper bound of the prediction interval.\n\n    !!! note\n        If `requires_grad` is False, all returned arrays are NumPy arrays.\n        Otherwise, they are PyTorch tensors with gradients.\n    \"\"\"\n    X_tensor = validate_X_input(X, input_dim=self.input_dim, device=self.device, requires_grad=self.requires_grad)\n    n = len(self.residuals)\n    q = int((1 - self.alpha) * (n + 1))\n    q = min(q, n-1)\n\n    res_quantile = n-q\n\n    self.conformal_width = torch.topk(self.residuals, res_quantile).values[-1]\n\n    if self.scale_data: \n        X_tensor = self.input_scaler.transform(X_tensor)\n\n    preds = [] \n\n    with torch.no_grad(): \n        for model in self.models: \n            model.eval()\n            pred = model(X_tensor)\n            preds.append(pred)\n\n    preds = torch.stack(preds)\n\n    means = torch.mean(preds, dim=2) \n    mean = torch.mean(means, dim=0)\n\n    lower_cq = torch.mean(preds[:, :, 0], dim=0)\n    upper_cq = torch.mean(preds[:, :, 1], dim=0)\n\n    lower = lower_cq - self.conformal_width\n    upper = upper_cq + self.conformal_width\n\n    if self.scale_data: \n        mean = self.output_scaler.inverse_transform(mean.view(-1, 1)).squeeze()\n        lower = self.output_scaler.inverse_transform(lower.view(-1, 1)).squeeze()\n        upper = self.output_scaler.inverse_transform(upper.view(-1, 1)).squeeze()\n\n    if not self.requires_grad: \n        return mean.detach().cpu().numpy(), lower.detach().cpu().numpy(), upper.detach().cpu().numpy()\n\n    else: \n        return mean, lower, upper\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.quantile_loss","title":"<code>quantile_loss(preds, y)</code>","text":"<p>Quantile loss used for training the quantile regressors.</p> <p>Parameters:</p> Name Type Description Default <code>preds</code> <code>Tensor</code> <p>Predicted quantiles, shape (batch_size, 2).</p> required <code>y</code> <code>Tensor</code> <p>True target values, shape (batch_size,).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Scalar loss.</p> Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def quantile_loss(self, preds, y): \n    \"\"\"\n    Quantile loss used for training the quantile regressors.\n\n    Args:\n        preds (Tensor): Predicted quantiles, shape (batch_size, 2).\n        y (Tensor): True target values, shape (batch_size,).\n\n    Returns:\n        (Tensor): Scalar loss.\n    \"\"\"\n    error = y.view(-1, 1) - preds\n    return torch.mean(torch.max(self.quantiles * error, (self.quantiles - 1) * error))\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.KFoldCQR.save","title":"<code>save(path)</code>","text":"<p>Save the trained model and associated configuration to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Directory to save model files.</p> required Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>def save(self, path):\n    \"\"\"\n    Save the trained model and associated configuration to disk.\n\n    Args:\n        path (str or Path): Directory to save model files.\n    \"\"\"\n    path = Path(path)\n    path.mkdir(parents=True, exist_ok=True)\n\n    # Save config (exclude non-serializable or large objects)\n    config = {\n        k: v for k, v in self.__dict__.items()\n        if k not in [\"models\", \"quantiles\", \"residuals\", \"conformal_width\", \"optimizer_cls\", \"optimizer_kwargs\", \"scheduler_cls\", \"scheduler_kwargs\", \n                     \"input_scaler\", \"output_scaler\", \"_loggers\", \"training_logs\", \"tuning_loggers\", \"tuning_logs\"]\n        and not callable(v)\n        and not isinstance(v, (torch.nn.Module,))\n    }\n\n    config[\"optimizer\"] = self.optimizer_cls.__class__.__name__ if self.optimizer_cls is not None else None\n    config[\"scheduler\"] = self.scheduler_cls.__class__.__name__ if self.scheduler_cls is not None else None\n    config[\"input_scaler\"] = self.input_scaler.__class__.__name__ if self.input_scaler is not None else None \n    config[\"output_scaler\"] = self.output_scaler.__class__.__name__ if self.output_scaler is not None else None\n\n    with open(path / \"config.json\", \"w\") as f:\n        json.dump(config, f, indent=4)\n\n    # Save model weights\n    for i, model in enumerate(self.models):\n        torch.save(model.state_dict(), path / f\"model_{i}.pt\")\n\n    # Save residuals and conformity score\n    torch.save({\n        \"conformal_width\": self.conformal_width, \n        \"residuals\": self.residuals,\n        \"quantiles\": self.quantiles,\n    }, path / \"extras.pt\")\n\n    with open(path / \"extras.pkl\", 'wb') as f: \n        pickle.dump([self.optimizer_cls, \n                    self.optimizer_kwargs, self.scheduler_cls, self.scheduler_kwargs, self.input_scaler, self.output_scaler], f)\n\n    for i, logger in enumerate(getattr(self, \"_loggers\", [])):\n        logger.save_to_file(path, idx=i, name=\"estimator\")\n\n    for i, logger in enumerate(getattr(self, \"tuning_loggers\", [])): \n        logger.save_to_file(path, name=\"tuning\", idx=i)\n</code></pre>"},{"location":"api/Conformal/k_fold_cqr/#uqregressors.conformal.k_fold_cqr.QuantNN","title":"<code>QuantNN</code>","text":"<p>               Bases: <code>Module</code></p> <p>A simple quantile neural network that estimates the lower and upper quantile when trained with a pinball loss function. </p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Number of input features </p> required <code>hidden_sizes</code> <code>list of int</code> <p>List of hidden layer sizes</p> required <code>dropout</code> <code>None or float</code> <p>The dropout probability - None if no dropout</p> required <code>activation</code> <code>Module</code> <p>Activation function class (e.g., nn.ReLU).</p> required Source code in <code>uqregressors\\conformal\\k_fold_cqr.py</code> <pre><code>class QuantNN(nn.Module): \n    \"\"\"\n    A simple quantile neural network that estimates the lower and upper quantile when trained\n    with a pinball loss function. \n\n    Args: \n        input_dim (int): Number of input features \n        hidden_sizes (list of int): List of hidden layer sizes\n        dropout (None or float): The dropout probability - None if no dropout\n        activation (torch.nn.Module): Activation function class (e.g., nn.ReLU).\n    \"\"\"\n    def __init__(self, input_dim, hidden_sizes, dropout, activation): \n        super().__init__()\n        layers = []\n        for h in hidden_sizes:\n            layers.append(nn.Linear(input_dim, h))\n            layers.append(activation())\n            if dropout is not None: \n                layers.append(nn.Dropout(dropout))\n            input_dim = h\n        output_layer = nn.Linear(hidden_sizes[-1], 2)\n        layers.append(output_layer)\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n</code></pre>"},{"location":"api/Utils/activations/","title":"uqregressors.utils.activations","text":""},{"location":"api/Utils/activations/#uqregressors.utils.activations--activations","title":"activations","text":""},{"location":"api/Utils/activations/#uqregressors.utils.activations.get_activation","title":"<code>get_activation(name)</code>","text":"<p>A simple method to return neural network activations (Pytorch modules) from their name (string)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The activation function to return </p> required <p>Returns:</p> Type Description <code>Module</code> <p>The activation function as a torch module</p> Source code in <code>uqregressors\\utils\\activations.py</code> <pre><code>def get_activation(name: str):\n    \"\"\"\n    A simple method to return neural network activations (Pytorch modules) from their name (string)\n\n    Args: \n        name (str): The activation function to return \n\n    Returns: \n        (Torch.nn.Module): The activation function as a torch module\n    \"\"\"\n    name = name.lower()\n    activations = {\n        \"relu\": nn.ReLU,\n        \"leaky_relu\": nn.LeakyReLU,\n        \"tanh\": nn.Tanh,\n        \"sigmoid\": nn.Sigmoid,\n        \"gelu\": nn.GELU,\n        \"elu\": nn.ELU,\n        \"selu\": nn.SELU,\n        \"none\": nn.Identity,\n    }\n    if name not in activations:\n        raise ValueError(f\"Unsupported activation: {name}\")\n    return activations[name]\n</code></pre>"},{"location":"api/Utils/data_loader/","title":"uqregressors.utils.data_loader","text":""},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader--data_loader","title":"data_loader","text":"<p>A collection of methods meant to help with dataset loading and cleaning. </p> The most useful user-facing methods are <ul> <li>load_unformatted_dataset</li> <li>clean_dataset </li> <li>validate_dataset</li> </ul>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.clean_dataset","title":"<code>clean_dataset(X, y)</code>","text":"<p>A simple helper method to drop missing or NaN values and reshape y to the correct size</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Input features (n_samples, n_features)</p> required <code>y</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Output targets (n_samples,)</p> required <p>Returns:</p> Name Type Description <code>X_clean</code> <code>ndarray</code> <p>Input features cleaned (n_samples, n_features)</p> <code>y_clean</code> <code>ndarray</code> <p>Output targets cleaned (n_samples, 1)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def clean_dataset(X, y): \n    \"\"\"\n    A simple helper method to drop missing or NaN values and reshape y to the correct size\n\n    Args: \n        X (Union[np.ndarray, pd.DataFrame, pd.Series]): Input features (n_samples, n_features)\n        y (Union[np.ndarray, pd.DataFrame, pd.Series]): Output targets (n_samples,)\n\n    Returns: \n        X_clean (np.ndarray): Input features cleaned (n_samples, n_features)\n        y_clean (np.ndarray): Output targets cleaned (n_samples, 1)\n    \"\"\"\n    X_df = pd.DataFrame(X)\n    y_series = pd.Series(y) if isinstance(y, (np.ndarray, list)) else pd.Series(y.values)\n\n    combined = pd.concat([X_df, y_series], axis=1)\n    combined_clean = combined.dropna()\n\n    X_clean = combined_clean.iloc[:, :-1].astype(np.float32).values\n    y_clean = combined_clean.iloc[:, -1].astype(np.float32).values.reshape(-1, 1)\n\n    return X_clean, y_clean\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.load_arff","title":"<code>load_arff(path)</code>","text":"<p>ARFF file loader.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the ARFF file.</p> required <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>Parsed ARFF data as a DataFrame.</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def load_arff(path):\n    \"\"\"\n    ARFF file loader.\n\n    Args:\n        path (str): Path to the ARFF file.\n\n    Returns:\n        df (pd.DataFrame): Parsed ARFF data as a DataFrame.\n    \"\"\"\n    attributes = []\n    data = []\n    reading_data = False\n\n    with open(path, 'r') as file:\n        for line in file:\n            line = line.strip()\n            if not line or line.startswith('%'):\n                continue\n            if line.lower().startswith('@attribute'):\n                # Example: @attribute age numeric\n                parts = line.split()\n                if len(parts) &gt;= 2:\n                    attributes.append(parts[1])\n            elif line.lower() == '@data':\n                reading_data = True\n            elif reading_data:\n                # Data line\n                row = [x.strip().strip('\"') for x in line.split(',')]\n                data.append(row)\n\n    df = pd.DataFrame(data, columns=attributes)\n    df = df.apply(pd.to_numeric, errors='coerce')  # convert to floats where possible\n    return df.dropna()\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.load_unformatted_dataset","title":"<code>load_unformatted_dataset(path, target_column=None, drop_columns=None)</code>","text":"<p>Load and standardize a dataset from a file. Note that the last column is always assumed to be the target.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the dataset file (CSV, XLSX, ARFF, etc.)</p> required <code>target_column</code> <code>Union[str, int, None]</code> <p>Name or index of the target column. If not provided, it is assumed the last column</p> <code>None</code> <code>drop_columns</code> <code>list</code> <p>Columns to drop (e.g., indices, column names).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>X</code> <code>ndarray</code> <p>Input features (n_samples, n_features)</p> <code>y</code> <code>ndarray</code> <p>Target values (n_samples,)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def load_unformatted_dataset(path, target_column=None, drop_columns=None):\n    \"\"\"\n    Load and standardize a dataset from a file. Note that the last column is always assumed to be the target.\n\n    Args:\n        path (str): Path to the dataset file (CSV, XLSX, ARFF, etc.)\n        target_column (Union[str, int, None]): Name or index of the target column. If not provided, it is assumed the last column\n        drop_columns (list): Columns to drop (e.g., indices, column names).\n\n    Returns:\n        X (np.ndarray): Input features (n_samples, n_features)\n        y (np.ndarray): Target values (n_samples,)\n    \"\"\"\n\n    ext = os.path.splitext(path)[-1].lower()\n\n    if ext == \".csv\":\n        try:\n            df = pd.read_csv(path)\n            if df.shape[1] &lt;= 1:\n                raise ValueError(\"Only one column detected; trying semicolon delimiter.\")\n        except Exception:\n            df = pd.read_csv(path, sep=';')\n    elif ext == \".xlsx\" or ext == \".xls\":\n        df = pd.read_excel(path)\n    elif ext == \".arff\":\n        data = load_arff(path)\n        df = pd.DataFrame(data)\n        # Decode bytes to str if needed\n        for col in df.select_dtypes([object]):\n            df[col] = df[col].apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n    elif ext == \".txt\":\n        # Try common delimiters: comma, tab, space\n        for delim in [',', '\\t', r'\\s+']:\n            try:\n                df = pd.read_csv(path, sep=delim, engine='python', header=None)\n                if df.shape[1] &lt; 2:\n                    continue  # unlikely to be valid\n                break\n            except Exception:\n                continue\n        else:\n            raise ValueError(f\"Could not parse .txt file: {path}\")\n    else:\n        raise ValueError(f\"Unsupported file extension: {ext}\")\n\n    df = df.dropna()\n\n    if drop_columns:\n        df.drop(columns=drop_columns, inplace=True)\n\n    if target_column is None:\n        target_column = df.columns[-1]  # default: last column\n\n    y = df[target_column].values.astype(np.float32)\n    X = df.drop(columns=[target_column]).values.astype(np.float32)\n\n    return X, y\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_X_input","title":"<code>validate_X_input(X, input_dim=None, device='cpu', requires_grad=False)</code>","text":"<p>Convert X to a torch.Tensor for inference. Called by regressors before the predict method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Input data to convert, should have shape (n_samples, n_features)</p> required <code>device</code> <code>str</code> <p>Target device ('cpu' or 'cuda').</p> <code>'cpu'</code> <code>requires_grad</code> <code>bool</code> <p>Whether the tensor should track gradients.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Prediction inputs of shape (n_samples, n_features)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_X_input(X, input_dim = None, device=\"cpu\", requires_grad=False):\n    \"\"\"\n    Convert X to a torch.Tensor for inference. Called by regressors before the predict method. \n\n    Args: \n        X (array-like): Input data to convert, should have shape (n_samples, n_features)\n        device (str): Target device ('cpu' or 'cuda').\n        requires_grad (bool): Whether the tensor should track gradients.\n\n    Returns:\n        (torch.Tensor): Prediction inputs of shape (n_samples, n_features)\n    \"\"\"\n    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n        X = X.values\n    elif isinstance(X, list):\n        X = np.array(X)\n    elif isinstance(X, torch.Tensor):\n        pass\n    elif not isinstance(X, np.ndarray):\n        raise TypeError(f\"Unsupported type for X: {type(X)}\")\n\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        elif X.ndim != 2:\n            raise ValueError(f\"X must be 2D. Got shape {X.shape}\")\n        X = torch.tensor(X, dtype=torch.float32)\n\n    if not isinstance(X, torch.Tensor):\n        raise TypeError(\"X could not be converted to a torch.Tensor\")\n\n    if input_dim is not None: \n        if X.shape[1] != input_dim: \n            raise ValueError(f\"Based on the training samples, the number of features of X should be {input_dim}. Got {X.shape[1] = }\")\n\n    X = X.to(device)\n    if requires_grad:\n        X.requires_grad_()\n\n    return X\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_and_prepare_inputs","title":"<code>validate_and_prepare_inputs(X, y, device='cpu', requires_grad=False)</code>","text":"<p>Convert X and y into compatible torch.Tensors for training. Called by regressors before the fit method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like</code> <p>Feature matrix. Supports np.ndarray, pd.DataFrame, list, or torch.Tensor.</p> required <code>y</code> <code>array - like</code> <p>Target vector. Supports np.ndarray, pd.Series, list, or torch.Tensor.</p> required <code>device</code> <code>str</code> <p>Device to place tensors on (e.g., 'cpu' or 'cuda').</p> <code>'cpu'</code> <code>requires_grad</code> <code>bool</code> <p>Whether the X tensor should require gradients (for gradient-based inference).</p> <code>False</code> <p>Returns:</p> Name Type Description <code>X_tensor</code> <code>Tensor</code> <p>Input features of shape (n_samples, n_features)</p> <code>y_tensor</code> <code>Tensor</code> <p>Output targets of shape (n_samples, 1)</p> Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_and_prepare_inputs(X, y, device=\"cpu\", requires_grad=False):\n    \"\"\"\n    Convert X and y into compatible torch.Tensors for training. Called by regressors before the fit method. \n\n    Args:\n        X (array-like): Feature matrix. Supports np.ndarray, pd.DataFrame, list, or torch.Tensor.\n        y (array-like): Target vector. Supports np.ndarray, pd.Series, list, or torch.Tensor.\n        device (str): Device to place tensors on (e.g., 'cpu' or 'cuda').\n        requires_grad (bool): Whether the X tensor should require gradients (for gradient-based inference).\n\n    Returns: \n        X_tensor (torch.Tensor): Input features of shape (n_samples, n_features)\n        y_tensor (torch.Tensor): Output targets of shape (n_samples, 1)\n    \"\"\"\n    # --- Convert X ---\n    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n        X = X.values\n    elif isinstance(X, list):\n        X = np.array(X)\n    elif isinstance(X, torch.Tensor):\n        pass  # leave as is\n    elif not isinstance(X, np.ndarray):\n        raise TypeError(f\"Unsupported type for X: {type(X)}\")\n\n    if isinstance(X, np.ndarray):\n        if X.ndim == 1:\n            X = X.reshape(1, -1)\n        elif X.ndim != 2:\n            raise ValueError(f\"X must be 2D. Got shape {X.shape}\")\n        X = torch.tensor(X, dtype=torch.float32)\n\n    if not isinstance(X, torch.Tensor):\n        raise TypeError(\"X could not be converted to a torch.Tensor\")\n\n    # --- Convert y ---\n    if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n        y = y.values\n    elif isinstance(y, list):\n        y = np.array(y)\n    elif isinstance(y, torch.Tensor):\n        pass\n    elif not isinstance(y, np.ndarray):\n        raise TypeError(f\"Unsupported type for y: {type(y)}\")\n\n    if isinstance(y, np.ndarray):\n        if y.ndim == 1:\n            y = y.reshape(-1, 1)\n        elif y.ndim == 2 and y.shape[1] != 1:\n            raise ValueError(\"y must be 1D or 2D with shape (n, 1)\")\n        y = torch.tensor(y, dtype=torch.float32)\n\n    if not isinstance(y, torch.Tensor):\n        raise TypeError(\"y could not be converted to a torch.Tensor\")\n\n    # --- Final checks ---\n    if y.ndim == 1:\n        y = y.unsqueeze(1)\n    elif y.ndim == 2 and y.shape[1] != 1:\n        raise ValueError(f\"Expected y to have shape (n_samples,) or (n_samples, 1), but got {y.shape}\")\n\n    if X.shape[0] != y.shape[0]:\n        raise ValueError(f\"X and y must have the same number of samples. Got {X.shape[0]} and {y.shape[0]}\")\n\n    X = X.to(device)\n    y = y.to(device)\n\n    if requires_grad:\n        X.requires_grad_()\n\n    return X, y\n</code></pre>"},{"location":"api/Utils/data_loader/#uqregressors.utils.data_loader.validate_dataset","title":"<code>validate_dataset(X, y, name='unnamed')</code>","text":"<p>A simple helper method to validate that a dataset is ready for regression.  Raises errors if X and y are not of the correct shape, or if the dataset contains NaNs or missing values.  If a dataset fails this method, try to apply the clean_dataset method first, and try again. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Input features (n_samples, n_features)</p> required <code>y</code> <code>Union[ndarray, DataFrame, Series]</code> <p>Output targets (n_samples,)</p> required Source code in <code>uqregressors\\utils\\data_loader.py</code> <pre><code>def validate_dataset(X, y, name=\"unnamed\"): \n    \"\"\"\n    A simple helper method to validate that a dataset is ready for regression. \n    Raises errors if X and y are not of the correct shape, or if the dataset contains NaNs or missing values. \n    If a dataset fails this method, try to apply the clean_dataset method first, and try again. \n\n    Args: \n        X (Union[np.ndarray, pd.DataFrame, pd.Series]): Input features (n_samples, n_features)\n        y (Union[np.ndarray, pd.DataFrame, pd.Series]): Output targets (n_samples,)\n    \"\"\"\n    print(f\"Summary for: {name} dataset\")\n    print(\"=\" * (21 + len(name)))\n\n    if isinstance(X, pd.DataFrame): \n        X = X.values \n    if isinstance(y, (pd.Series, pd.DataFrame)): \n        y = y.values \n\n    if X.ndim != 2: \n        raise ValueError(\"X must be a 2D array (n_samples, n_features)\")\n    if y.ndim == 2 and y.shape[1] != 1: \n        raise ValueError(\"y must be 1D or a 2D column vector with shape (n_samples, 1)\")\n    if y.ndim &gt; 2: \n        raise ValueError(\"y must be 1D or 2D with a single output\")\n\n    n_samples, n_features = X.shape \n\n    if y.shape[0] != n_samples: \n        raise ValueError(\"X and y must have the same number of samples\")\n\n    if np.isnan(X).any() or np.isnan(y).any(): \n        raise ValueError(\"Dataset contains NaNs or missing values.\")\n\n    if not np.issubdtype(X.dtype, np.floating):\n        raise ValueError(\"X must contain only float values (use float32 or float64)\")\n\n    print(f\"Number of samples: {n_samples}\")\n    print(f\"Number of features: {n_features}\")\n    print(f\"Output shape: {y.shape}\")\n    print(\"Dataset validation passed.\\n\") \n</code></pre>"},{"location":"api/Utils/file_manager/","title":"uqregressors.utils.file_manager","text":""},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager--file_manager","title":"file_manager","text":"<p>Handles saving paths, including saving and loading models and plots. </p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\n&gt;&gt;&gt; from uqregressors.metrics.metrics import compute_all_metrics\n&gt;&gt;&gt; from uqregressors.utils.file_manager import FileManager\n</code></pre> <pre><code>&gt;&gt;&gt; # Instantiate File Manager\n&gt;&gt;&gt; BASE_PATH = \"C:/.uqregressors\"\n&gt;&gt;&gt; fm = FileManager(BASE_PATH)  # Replace with desired base path\n</code></pre> <pre><code>&gt;&gt;&gt; # Fit a model and compute metrics\n&gt;&gt;&gt; reg = DeepEnsembleRegressor()\n&gt;&gt;&gt; reg.fit(X_train, y_train)\n&gt;&gt;&gt; mean, lower, upper = reg.predict(X_test)\n&gt;&gt;&gt; metrics = compute_all_metrics(\n...     mean, lower, upper, y_test, reg.alpha\n... )\n</code></pre> <pre><code>&gt;&gt;&gt; # Save model and metrics\n&gt;&gt;&gt; save_path = fm.save_model(\n...     name=\"Deep_Ens\",\n...     model=reg,\n...     metrics=metrics,\n...     X_train=X_train,\n...     y_train=y_train,\n...     X_test=X_test,\n...     y_test=y_test\n... )\n&gt;&gt;&gt; # Will save to: BASE_PATH/models/Deep_Ens\n</code></pre> <pre><code>&gt;&gt;&gt; # Alternatively, specify full path directly\n&gt;&gt;&gt; save_path = fm.save_model(path=\"SAVE_PATH\", model=reg, ...)\n</code></pre> <pre><code>&gt;&gt;&gt; # Load model and metrics\n&gt;&gt;&gt; load_dict = fm.load_model(\n...     reg.__class__, save_path, load_logs=True\n... )\n&gt;&gt;&gt; metrics = load_dict[\"metrics\"]\n&gt;&gt;&gt; loaded_model = load_dict[\"model\"]\n&gt;&gt;&gt; X_test = load_dict[\"X_test\"]\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager","title":"<code>FileManager</code>","text":"<p>FileManager class to handle paths and saving.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Base directory to save files to. Defaults to creating a folder \".uqregressors\" within the Users home path. </p> <code>home() / '.uqregressors'</code> <p>Attributes:</p> Name Type Description <code>base_dir</code> <code>Path</code> <p>The base directory as a Path object.</p> <code>model_dir</code> <code>Path</code> <p>The directory \"models\" within the base_dir, where models will be saved and loaded.</p> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>class FileManager:\n    \"\"\"\n    FileManager class to handle paths and saving.\n\n    Args: \n        base_dir (str): Base directory to save files to. Defaults to creating a folder \".uqregressors\" within the Users home path. \n\n    Attributes: \n        base_dir (Path): The base directory as a Path object.\n        model_dir (Path): The directory \"models\" within the base_dir, where models will be saved and loaded.\n    \"\"\"\n    def __init__(self, base_dir=Path.home() / \".uqregressors\"):\n        self.base_dir = Path(base_dir)\n        self.model_dir = self.base_dir / \"models\"\n        self.model_dir.mkdir(parents=True, exist_ok=True)\n\n    def get_timestamped_path(self, model_class_name: str) -&gt; Path:\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        return self.model_dir / f\"{model_class_name}_{timestamp}\"\n\n    def get_named_path(self, name: str) -&gt; Path:\n        return self.model_dir / name\n\n    def save_model(\n        self, model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None\n    ) -&gt; Path:\n        \"\"\"\n        Saves a model, along with metrics, and training and testing data\n\n        Args: \n            model (BaseEstimator): The regressor to save. Note that it must implement the save method.\n            name (str): The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.\n            path (str): The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,\n                        a directory with the model class and timestamp is created. \n            metrics (dict): A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.\n            X_train (array-like): Training features. \n            y_train (array-like): Training targets. \n            X_test (array-like): Testing features. \n            y_test (array-like): Testing targets.\n        \"\"\"\n        if name is not None:\n            if path is not None:\n                warnings.warn(f\"Both name and path given. Using named path: {path}\")\n            else: \n                path = self.get_named_path(name)\n        elif path is None:\n            path = self.get_timestamped_path(model.__class__.__name__)\n        else:\n            path = Path(path)\n\n        path.mkdir(parents=True, exist_ok=True)\n\n        if not hasattr(model, \"save\") or not callable(model.save):\n            raise AttributeError(f\"{model.__class__.__name__} must implement `save(path)`\")\n        model.save(path)\n\n        if metrics:\n            with open(path / \"metrics.json\", \"w\") as f:\n                json.dump(metrics, f, indent=2)\n\n        for name, array in [(\"X_train\", X_train), (\"y_train\", y_train), (\"X_test\", X_test), (\"y_test\", y_test)]:\n            if array is not None:\n                np.save(path / f\"{name}.npy\", np.array(array))\n\n        print(f\"Model and additional artifacts saved to: {path}\")\n        return path\n\n    def load_model(self, model_class, path=None, name=None, device=\"cpu\", load_logs=False):\n        \"\"\"\n        Loads a model and associated metadata from path\n\n        Args: \n            model_class (BaseEstimator): The class of the model to be loaded. This should match with the class of model which was saved. \n            path (str): The path to the directory in which the model and associated metadata is stored. If not given, name must be given. \n            name (str): The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. \n            device (str): The device, \"cpu\" or \"cuda\" to load the model with. \n            load_logs (bool): Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.\n\n        Returns: \n            (dict): Dictionary of loaded objects with the following keys: \n\n                    model: The loaded model,\n\n                    metrics: The loaded metrics or None if there is no metrics.json file, \n\n                    X_train: The loaded training features or None if there is no X_train.npy file, \n\n                    y_train: The loaded training targets or None if there is no y_train.npy file,\n\n                    X_test: The loaded testing features or None if there is no X_test.npy file, \n\n                    y_test: The loaded testing targets or None if there is no y_test.npy file.\n        \"\"\"\n        if name:\n            path = self.get_named_path(name)\n        elif path:\n            path = Path(path)\n        else:\n            raise ValueError(\"Either `name` or `path` must be specified.\")\n\n        if not path.exists():\n            raise FileNotFoundError(f\"Path {path} does not exist\")\n\n        if not hasattr(model_class, \"load\") or not callable(model_class.load):\n            raise AttributeError(f\"{model_class.__name__} must implement `load(path)`\")\n\n        from torch.serialization import safe_globals\n        with safe_globals([np._core.multiarray._reconstruct, np.ndarray, np.dtype]):\n            model = model_class.load(path, device=device, load_logs=load_logs)\n\n        def try_load(name):\n            f = path / f\"{name}.npy\"\n            return np.load(f) if f.exists() else None\n\n        metrics = None\n        metrics_path = path / \"metrics.json\"\n        if metrics_path.exists():\n            with open(metrics_path) as f:\n                metrics = json.load(f)\n\n        return {\n            \"model\": model,\n            \"metrics\": metrics,\n            \"X_train\": try_load(\"X_train\"),\n            \"y_train\": try_load(\"y_train\"),\n            \"X_test\": try_load(\"X_test\"),\n            \"y_test\": try_load(\"y_test\"),\n        }\n\n    def save_plot(self, fig, model_path, filename=\"plot.png\", show=True, subdir=\"plots\"):\n        \"\"\"\n        A helper method to save plots to a subdirectory within the directory in which the model would be saved. \n\n        Args: \n            fig (matplotlib.figure.Figure): The figure to be saved. \n            model_path (str): The directory in which to create a \"plots\" subdirectory where the image will be saved. \n            filename (str): The filename of the plot to be saved, including the file extension.\n            show (bool): Whether to display the plot after saving it. \n            subdir (str): The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .\n        \"\"\"\n\n        path = Path(model_path)\n        plot_dir = path / subdir\n        plot_dir.mkdir(parents=True, exist_ok=True)\n        save_path = plot_dir / filename\n\n        plt.figure(fig.number)\n        fig.savefig(save_path, bbox_inches='tight')\n\n        if show:\n            plt.show(fig)\n        plt.close(fig)\n        print(f\"Plot saved to {save_path}\")\n        return save_path\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.load_model","title":"<code>load_model(model_class, path=None, name=None, device='cpu', load_logs=False)</code>","text":"<p>Loads a model and associated metadata from path</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>BaseEstimator</code> <p>The class of the model to be loaded. This should match with the class of model which was saved. </p> required <code>path</code> <code>str</code> <p>The path to the directory in which the model and associated metadata is stored. If not given, name must be given. </p> <code>None</code> <code>name</code> <code>str</code> <p>The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. </p> <code>None</code> <code>device</code> <code>str</code> <p>The device, \"cpu\" or \"cuda\" to load the model with. </p> <code>'cpu'</code> <code>load_logs</code> <code>bool</code> <p>Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary of loaded objects with the following keys: </p> <pre><code>model: The loaded model,\n\nmetrics: The loaded metrics or None if there is no metrics.json file,\n\nX_train: The loaded training features or None if there is no X_train.npy file,\n\ny_train: The loaded training targets or None if there is no y_train.npy file,\n\nX_test: The loaded testing features or None if there is no X_test.npy file,\n\ny_test: The loaded testing targets or None if there is no y_test.npy file.\n</code></pre> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def load_model(self, model_class, path=None, name=None, device=\"cpu\", load_logs=False):\n    \"\"\"\n    Loads a model and associated metadata from path\n\n    Args: \n        model_class (BaseEstimator): The class of the model to be loaded. This should match with the class of model which was saved. \n        path (str): The path to the directory in which the model and associated metadata is stored. If not given, name must be given. \n        name (str): The name if the directory containing the model is fm.base_dir/models/{name}. If not given, the path must be given. \n        device (str): The device, \"cpu\" or \"cuda\" to load the model with. \n        load_logs (bool): Whether training and hyperparameter logs should be loaded along with the model so they can be accessed by code.\n\n    Returns: \n        (dict): Dictionary of loaded objects with the following keys: \n\n                model: The loaded model,\n\n                metrics: The loaded metrics or None if there is no metrics.json file, \n\n                X_train: The loaded training features or None if there is no X_train.npy file, \n\n                y_train: The loaded training targets or None if there is no y_train.npy file,\n\n                X_test: The loaded testing features or None if there is no X_test.npy file, \n\n                y_test: The loaded testing targets or None if there is no y_test.npy file.\n    \"\"\"\n    if name:\n        path = self.get_named_path(name)\n    elif path:\n        path = Path(path)\n    else:\n        raise ValueError(\"Either `name` or `path` must be specified.\")\n\n    if not path.exists():\n        raise FileNotFoundError(f\"Path {path} does not exist\")\n\n    if not hasattr(model_class, \"load\") or not callable(model_class.load):\n        raise AttributeError(f\"{model_class.__name__} must implement `load(path)`\")\n\n    from torch.serialization import safe_globals\n    with safe_globals([np._core.multiarray._reconstruct, np.ndarray, np.dtype]):\n        model = model_class.load(path, device=device, load_logs=load_logs)\n\n    def try_load(name):\n        f = path / f\"{name}.npy\"\n        return np.load(f) if f.exists() else None\n\n    metrics = None\n    metrics_path = path / \"metrics.json\"\n    if metrics_path.exists():\n        with open(metrics_path) as f:\n            metrics = json.load(f)\n\n    return {\n        \"model\": model,\n        \"metrics\": metrics,\n        \"X_train\": try_load(\"X_train\"),\n        \"y_train\": try_load(\"y_train\"),\n        \"X_test\": try_load(\"X_test\"),\n        \"y_test\": try_load(\"y_test\"),\n    }\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.save_model","title":"<code>save_model(model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None)</code>","text":"<p>Saves a model, along with metrics, and training and testing data</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>BaseEstimator</code> <p>The regressor to save. Note that it must implement the save method.</p> required <code>name</code> <code>str</code> <p>The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.</p> <code>None</code> <code>path</code> <code>str</code> <p>The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,         a directory with the model class and timestamp is created. </p> <code>None</code> <code>metrics</code> <code>dict</code> <p>A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.</p> <code>None</code> <code>X_train</code> <code>array - like</code> <p>Training features. </p> <code>None</code> <code>y_train</code> <code>array - like</code> <p>Training targets. </p> <code>None</code> <code>X_test</code> <code>array - like</code> <p>Testing features. </p> <code>None</code> <code>y_test</code> <code>array - like</code> <p>Testing targets.</p> <code>None</code> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def save_model(\n    self, model, name=None, path=None, metrics=None, X_train=None, y_train=None, X_test=None, y_test=None\n) -&gt; Path:\n    \"\"\"\n    Saves a model, along with metrics, and training and testing data\n\n    Args: \n        model (BaseEstimator): The regressor to save. Note that it must implement the save method.\n        name (str): The name of the model for directory purposes. If given, the model will be saved wihin the directory: fm.base_dir/models/name.\n        path (str): The path to the directory where the model should be saved. Only one of name or path should be given. If neither are given,\n                    a directory with the model class and timestamp is created. \n        metrics (dict): A dictionary of metrics to store. Can be used with uqregressors.metrics.metrics.compute_all_metrics.\n        X_train (array-like): Training features. \n        y_train (array-like): Training targets. \n        X_test (array-like): Testing features. \n        y_test (array-like): Testing targets.\n    \"\"\"\n    if name is not None:\n        if path is not None:\n            warnings.warn(f\"Both name and path given. Using named path: {path}\")\n        else: \n            path = self.get_named_path(name)\n    elif path is None:\n        path = self.get_timestamped_path(model.__class__.__name__)\n    else:\n        path = Path(path)\n\n    path.mkdir(parents=True, exist_ok=True)\n\n    if not hasattr(model, \"save\") or not callable(model.save):\n        raise AttributeError(f\"{model.__class__.__name__} must implement `save(path)`\")\n    model.save(path)\n\n    if metrics:\n        with open(path / \"metrics.json\", \"w\") as f:\n            json.dump(metrics, f, indent=2)\n\n    for name, array in [(\"X_train\", X_train), (\"y_train\", y_train), (\"X_test\", X_test), (\"y_test\", y_test)]:\n        if array is not None:\n            np.save(path / f\"{name}.npy\", np.array(array))\n\n    print(f\"Model and additional artifacts saved to: {path}\")\n    return path\n</code></pre>"},{"location":"api/Utils/file_manager/#uqregressors.utils.file_manager.FileManager.save_plot","title":"<code>save_plot(fig, model_path, filename='plot.png', show=True, subdir='plots')</code>","text":"<p>A helper method to save plots to a subdirectory within the directory in which the model would be saved. </p> <p>Parameters:</p> Name Type Description Default <code>fig</code> <code>Figure</code> <p>The figure to be saved. </p> required <code>model_path</code> <code>str</code> <p>The directory in which to create a \"plots\" subdirectory where the image will be saved. </p> required <code>filename</code> <code>str</code> <p>The filename of the plot to be saved, including the file extension.</p> <code>'plot.png'</code> <code>show</code> <code>bool</code> <p>Whether to display the plot after saving it. </p> <code>True</code> <code>subdir</code> <code>str</code> <p>The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .</p> <code>'plots'</code> Source code in <code>uqregressors\\utils\\file_manager.py</code> <pre><code>def save_plot(self, fig, model_path, filename=\"plot.png\", show=True, subdir=\"plots\"):\n    \"\"\"\n    A helper method to save plots to a subdirectory within the directory in which the model would be saved. \n\n    Args: \n        fig (matplotlib.figure.Figure): The figure to be saved. \n        model_path (str): The directory in which to create a \"plots\" subdirectory where the image will be saved. \n        filename (str): The filename of the plot to be saved, including the file extension.\n        show (bool): Whether to display the plot after saving it. \n        subdir (str): The subdirectory in which the plot will be saved, each image will be saved to model_path/subdir/filename .\n    \"\"\"\n\n    path = Path(model_path)\n    plot_dir = path / subdir\n    plot_dir.mkdir(parents=True, exist_ok=True)\n    save_path = plot_dir / filename\n\n    plt.figure(fig.number)\n    fig.savefig(save_path, bbox_inches='tight')\n\n    if show:\n        plt.show(fig)\n    plt.close(fig)\n    print(f\"Plot saved to {save_path}\")\n    return save_path\n</code></pre>"},{"location":"api/Utils/logging/","title":"uqregressors.utils.logging","text":""},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger","title":"<code>Logger</code>","text":"<p>Base Logging class.</p> <p>Parameters:</p> Name Type Description Default <code>use_wandb</code> <code>bool</code> <p>Whether to use weights and biases for logging (Experimental feature, not validated yet).</p> <code>False</code> <code>project_name</code> <code>str</code> <p>The logger project name.</p> <code>None</code> <code>run_name</code> <code>str</code> <p>The logger run name for a given training run. </p> <code>None</code> <code>config</code> <code>dict</code> <p>Dictionary of relevant training parameters, only used if weights and biases is used.</p> <code>None</code> <code>name</code> <code>str</code> <p>Name of the logger.</p> <code>None</code> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>class Logger:\n    \"\"\"\n    Base Logging class.\n\n    Args: \n        use_wandb (bool): Whether to use weights and biases for logging (Experimental feature, not validated yet).\n        project_name (str): The logger project name.\n        run_name (str): The logger run name for a given training run. \n        config (dict): Dictionary of relevant training parameters, only used if weights and biases is used.\n        name (str): Name of the logger.\n    \"\"\"\n    def __init__(self, use_wandb=False, project_name=None, run_name=None, config=None, name=None):\n        self.use_wandb = use_wandb and _wandb_available\n        self.logs = []\n\n        if self.use_wandb:\n            wandb.init(\n                project=project_name or \"default_project\",\n                name=run_name,\n                config=config or {},\n            )\n            self.wandb = wandb\n        else:\n            self.logger = logging.getLogger(name or f\"Logger-{os.getpid()}\")\n            self.logger.setLevel(logging.INFO)\n\n            if LOGGING_CONFIG[\"print\"] and not self.logger.handlers:\n                ch = logging.StreamHandler()\n                formatter = logging.Formatter(\"[%(name)s] %(message)s\")\n                ch.setFormatter(formatter)\n                self.logger.addHandler(ch)\n\n    def log(self, data: dict):\n        \"\"\"\n        Writes a dictionary to a stored internal log.\n        \"\"\"\n        if self.use_wandb:\n            self.wandb.log(data)\n        else:\n            msg = \", \".join(f\"{k}={v}\" for k, v in data.items())\n            self.logs.append(msg)\n            self.logger.info(msg)\n\n    def save_to_file(self, path, subdir=\"logs\", idx=0, name=\"\"): \n        \"\"\"\n        Saves logs to the logs subdirectory when model.save is called.\n        \"\"\"\n        log_dir = Path(path) / subdir \n        log_dir.mkdir(parents=True, exist_ok=True)\n        with open(log_dir / f\"{name}_{str(idx)}.log\", \"w\", encoding=\"utf-8\") as f: \n            f.write(\"\\n\".join(self.logs))\n\n\n    def finish(self):\n        \"\"\"\n        Finish method for weights and biases logging.\n        \"\"\"\n        if self.use_wandb:\n            self.wandb.finish()\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.finish","title":"<code>finish()</code>","text":"<p>Finish method for weights and biases logging.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def finish(self):\n    \"\"\"\n    Finish method for weights and biases logging.\n    \"\"\"\n    if self.use_wandb:\n        self.wandb.finish()\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.log","title":"<code>log(data)</code>","text":"<p>Writes a dictionary to a stored internal log.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def log(self, data: dict):\n    \"\"\"\n    Writes a dictionary to a stored internal log.\n    \"\"\"\n    if self.use_wandb:\n        self.wandb.log(data)\n    else:\n        msg = \", \".join(f\"{k}={v}\" for k, v in data.items())\n        self.logs.append(msg)\n        self.logger.info(msg)\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.Logger.save_to_file","title":"<code>save_to_file(path, subdir='logs', idx=0, name='')</code>","text":"<p>Saves logs to the logs subdirectory when model.save is called.</p> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def save_to_file(self, path, subdir=\"logs\", idx=0, name=\"\"): \n    \"\"\"\n    Saves logs to the logs subdirectory when model.save is called.\n    \"\"\"\n    log_dir = Path(path) / subdir \n    log_dir.mkdir(parents=True, exist_ok=True)\n    with open(log_dir / f\"{name}_{str(idx)}.log\", \"w\", encoding=\"utf-8\") as f: \n        f.write(\"\\n\".join(self.logs))\n</code></pre>"},{"location":"api/Utils/logging/#uqregressors.utils.logging.set_logging_config","title":"<code>set_logging_config(print=True)</code>","text":"<p>Sets global logging printing configuration. </p> <p>Parameters:</p> Name Type Description Default <code>print</code> <code>bool</code> <p>If False, disables printing to the terminal for all future Logger instances</p> <code>True</code> Source code in <code>uqregressors\\utils\\logging.py</code> <pre><code>def set_logging_config(print=True): \n    \"\"\"\n    Sets global logging printing configuration. \n\n    Args: \n        print (bool): If False, disables printing to the terminal for all future Logger instances\n    \"\"\"\n    LOGGING_CONFIG[\"print\"] = print\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/","title":"uqregressors.utils.torch_sklearn_utils","text":""},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils--torch_sklearn_utils","title":"torch_sklearn_utils","text":"<p>A collection of sklearn utility functions refactored to work with pytorch tensors. </p> The key functions are <ul> <li>TorchStandardScaler (class)</li> <li>TorchKFold (class)</li> <li>train_test_split (function)</li> </ul> <p>Warning</p> <p>TorchKFold returns the indices of each K-Fold, while train_test_split returns the values in each split.</p>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchKFold","title":"<code>TorchKFold</code>","text":"<p>A class meant to split the data into K-folds for conformalization or cross validation. </p> <p>Parameters:</p> Name Type Description Default <code>n_splits</code> <code>int</code> <p>The number of folds for data splitting.</p> <code>5</code> <code>shuffle</code> <code>bool</code> <p>Whether to shuffle the data before splitting. </p> <code>False</code> <code>random_state</code> <code>int or None</code> <p>Controls shuffling for reproducibility.</p> <code>None</code> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>class TorchKFold:\n    \"\"\"\n    A class meant to split the data into K-folds for conformalization or cross validation. \n\n    Args: \n        n_splits (int): The number of folds for data splitting.\n        shuffle (bool): Whether to shuffle the data before splitting. \n        random_state (int or None): Controls shuffling for reproducibility.\n    \"\"\"\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        if n_splits &lt; 2:\n            raise ValueError(\"n_splits must be at least 2.\")\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def split(self, X):\n        \"\"\"\n        Yield train/test indices for each fold.\n\n        Args:\n            X (torch.Tensor, np.ndarray, or list): Input data with shape (n_samples, ...)\n\n        Yields:\n            (tuple[torch.LongTensor, torch.LongTensor]): train_idx, val_idx; the indices of the training and validation sets for each of the splits. \n        \"\"\"\n        if isinstance(X, torch.Tensor):\n            n_samples = X.shape[0]\n        else:\n            X = np.asarray(X)\n            n_samples = len(X)\n\n        indices = np.arange(n_samples)\n\n        if self.shuffle:\n            rng = np.random.default_rng(self.random_state)\n            rng.shuffle(indices)\n\n        fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n        fold_sizes[:n_samples % self.n_splits] += 1\n        current = 0\n\n        for fold_size in fold_sizes:\n            val_idx = indices[current:current + fold_size]\n            train_idx = np.concatenate([indices[:current], indices[current + fold_size:]])\n            current += fold_size\n\n            yield (\n                torch.from_numpy(train_idx).long(),\n                torch.from_numpy(val_idx).long()\n            )\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchKFold.split","title":"<code>split(X)</code>","text":"<p>Yield train/test indices for each fold.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>torch.Tensor, np.ndarray, or list</code> <p>Input data with shape (n_samples, ...)</p> required <p>Yields:</p> Type Description <code>tuple[LongTensor, LongTensor]</code> <p>train_idx, val_idx; the indices of the training and validation sets for each of the splits.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def split(self, X):\n    \"\"\"\n    Yield train/test indices for each fold.\n\n    Args:\n        X (torch.Tensor, np.ndarray, or list): Input data with shape (n_samples, ...)\n\n    Yields:\n        (tuple[torch.LongTensor, torch.LongTensor]): train_idx, val_idx; the indices of the training and validation sets for each of the splits. \n    \"\"\"\n    if isinstance(X, torch.Tensor):\n        n_samples = X.shape[0]\n    else:\n        X = np.asarray(X)\n        n_samples = len(X)\n\n    indices = np.arange(n_samples)\n\n    if self.shuffle:\n        rng = np.random.default_rng(self.random_state)\n        rng.shuffle(indices)\n\n    fold_sizes = np.full(self.n_splits, n_samples // self.n_splits, dtype=int)\n    fold_sizes[:n_samples % self.n_splits] += 1\n    current = 0\n\n    for fold_size in fold_sizes:\n        val_idx = indices[current:current + fold_size]\n        train_idx = np.concatenate([indices[:current], indices[current + fold_size:]])\n        current += fold_size\n\n        yield (\n            torch.from_numpy(train_idx).long(),\n            torch.from_numpy(val_idx).long()\n        )\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler","title":"<code>TorchStandardScaler</code>","text":"<p>Standardized scaling to 0 mean values with unit variance.</p> <p>Attributes:</p> Name Type Description <code>mean_</code> <code>float</code> <p>The mean of the data, subtracted from the data during scaling. </p> <code>std_</code> <code>float</code> <p>The standard deviation of the data, by which the data is divided during scaling.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>class TorchStandardScaler:\n    \"\"\"\n    Standardized scaling to 0 mean values with unit variance.\n\n    Attributes: \n        mean_ (float): The mean of the data, subtracted from the data during scaling. \n        std_ (float): The standard deviation of the data, by which the data is divided during scaling.\n    \"\"\"\n    def __init__(self):\n        self.mean_ = None\n        self.std_ = None\n\n    def fit(self, X):\n        \"\"\"\n        Fits the standard scaler. \n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (TorchStandardScaler): the scaler with updated mean_ and std_ attributes. \n        \"\"\"\n        self.mean_ = X.mean(dim=0, keepdim=True)\n        self.std_ = X.std(dim=0, unbiased=False, keepdim=True)\n        # Avoid division by zero\n        self.std_[self.std_ &lt; 1e-8] = 1.0\n        return self\n\n    def transform(self, X):\n        \"\"\"\n        Transforms the standard scaler based on the attributes obtained with the fit method. \n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The scaled data\n        \"\"\"\n        return (X - self.mean_) / self.std_\n\n    def fit_transform(self, X): \n        \"\"\"\n        Performs the fit and transforms the data. A combination of the fit and transform methods.\n\n        Args: \n            X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The scaled data\n        \"\"\"\n        self.fit(X)\n        return self.transform(X)\n\n    def inverse_transform(self, X_scaled):\n        \"\"\"\n        Transforms scaled data back to the original scale. \n\n        Args: \n            X_scaled (torch.Tensor): scaled data of shape (n_samples, n_features).\n\n        Returns: \n            (torch.Tensor): The unscaled data. \n        \"\"\"\n        return X_scaled * self.std_ + self.mean_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.fit","title":"<code>fit(X)</code>","text":"<p>Fits the standard scaler. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>TorchStandardScaler</code> <p>the scaler with updated mean_ and std_ attributes.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def fit(self, X):\n    \"\"\"\n    Fits the standard scaler. \n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (TorchStandardScaler): the scaler with updated mean_ and std_ attributes. \n    \"\"\"\n    self.mean_ = X.mean(dim=0, keepdim=True)\n    self.std_ = X.std(dim=0, unbiased=False, keepdim=True)\n    # Avoid division by zero\n    self.std_[self.std_ &lt; 1e-8] = 1.0\n    return self\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.fit_transform","title":"<code>fit_transform(X)</code>","text":"<p>Performs the fit and transforms the data. A combination of the fit and transform methods.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The scaled data</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def fit_transform(self, X): \n    \"\"\"\n    Performs the fit and transforms the data. A combination of the fit and transform methods.\n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The scaled data\n    \"\"\"\n    self.fit(X)\n    return self.transform(X)\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.inverse_transform","title":"<code>inverse_transform(X_scaled)</code>","text":"<p>Transforms scaled data back to the original scale. </p> <p>Parameters:</p> Name Type Description Default <code>X_scaled</code> <code>Tensor</code> <p>scaled data of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The unscaled data.</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def inverse_transform(self, X_scaled):\n    \"\"\"\n    Transforms scaled data back to the original scale. \n\n    Args: \n        X_scaled (torch.Tensor): scaled data of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The unscaled data. \n    \"\"\"\n    return X_scaled * self.std_ + self.mean_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.TorchStandardScaler.transform","title":"<code>transform(X)</code>","text":"<p>Transforms the standard scaler based on the attributes obtained with the fit method. </p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>data to be scaled of shape (n_samples, n_features).</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The scaled data</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def transform(self, X):\n    \"\"\"\n    Transforms the standard scaler based on the attributes obtained with the fit method. \n\n    Args: \n        X (torch.Tensor): data to be scaled of shape (n_samples, n_features).\n\n    Returns: \n        (torch.Tensor): The scaled data\n    \"\"\"\n    return (X - self.mean_) / self.std_\n</code></pre>"},{"location":"api/Utils/torch_sklearn_utils/#uqregressors.utils.torch_sklearn_utils.train_test_split","title":"<code>train_test_split(X, y, test_size=0.2, device='cpu', random_state=None, shuffle=True)</code>","text":"<p>Split arrays or tensors into training and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>array - like or Tensor</code> <p>Features to be split. </p> required <code>y</code> <code>array - like or Tensor</code> <p>Targets to be split. </p> required <code>test_size</code> <code>float</code> <p>Proportion of the dataset to include in the test split (between 0 and 1).</p> <code>0.2</code> <code>random_state</code> <code>int or None</code> <p>Controls the shuffling for reproducibility.</p> <code>None</code> <code>shuffle</code> <code>bool</code> <p>Whether or not to shuffle the data before splitting.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray, ndarray, ndarray]</code> <p>X_train, X_test, y_train, y_test; same type as inputs</p> Source code in <code>uqregressors\\utils\\torch_sklearn_utils.py</code> <pre><code>def train_test_split(X, y, test_size=0.2, device=\"cpu\", random_state=None, shuffle=True):\n    \"\"\"\n    Split arrays or tensors into training and test sets.\n\n    Args:\n        X (array-like or torch.Tensor): Features to be split. \n        y (array-like or torch.Tensor): Targets to be split. \n        test_size (float): Proportion of the dataset to include in the test split (between 0 and 1).\n        random_state (int or None): Controls the shuffling for reproducibility.\n        shuffle (bool): Whether or not to shuffle the data before splitting.\n\n    Returns:\n        (Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]): X_train, X_test, y_train, y_test; same type as inputs\n    \"\"\"\n    # Convert to numpy for easy indexing\n    if isinstance(X, torch.Tensor):\n        X_np = X.cpu().numpy()\n        is_tensor = True\n    else:\n        X_np = np.asarray(X)\n        is_tensor = False\n\n    if isinstance(y, torch.Tensor):\n        y_np = y.cpu().numpy()\n    else:\n        y_np = np.asarray(y)\n\n    # Check dimensions\n    if X_np.shape[0] != y_np.shape[0]:\n        raise ValueError(f\"X and y must have the same number of samples. Got {X_np.shape[0]} and {y_np.shape[0]}.\")\n\n    n_samples = X_np.shape[0]\n    n_test = int(n_samples * test_size)\n\n    if random_state is not None:\n        rng = np.random.default_rng(random_state)\n    else:\n        rng = np.random.default_rng()\n\n    indices = np.arange(n_samples)\n    if shuffle:\n        rng.shuffle(indices)\n\n    test_indices = indices[:n_test]\n    train_indices = indices[n_test:]\n\n    X_train, X_test = X_np[train_indices], X_np[test_indices]\n    y_train, y_test = y_np[train_indices], y_np[test_indices]\n\n    if is_tensor:\n        X_train = torch.tensor(X_train, dtype=X.dtype, device=device)\n        X_test = torch.tensor(X_test, dtype=X.dtype, device=device)\n        y_train = torch.tensor(y_train, dtype=y.dtype, device=device)\n        y_test = torch.tensor(y_test, dtype=y.dtype, device=device)\n\n    return X_train, X_test, y_train, y_test\n</code></pre>"},{"location":"examples/bayesian_demo-checkpoint/","title":"Bayesian demo checkpoint","text":""},{"location":"examples/bayesian_demo-checkpoint/#dataset-creation","title":"Dataset Creation","text":"<pre><code>import numpy as np\n\n\nrng = np.random.RandomState(42)\ndef true_function(x):\n    return np.sin(2 * np.pi * x)\n\nX_test = np.linspace(0, 1, 200).reshape(-1, 1)\ny_true = true_function(X_test)\n\nX_train = np.sort(rng.rand(10, 1))\ny_train = true_function(X_train).ravel() \n</code></pre>"},{"location":"examples/bayesian_demo-checkpoint/#plotting","title":"Plotting","text":"<pre><code>import matplotlib.pyplot as plt\n\ndef plot_uncertainty_results(mean, lower, upper, model_name): \n    plt.figure(figsize=(10, 6))\n    plt.plot(X_test, y_true, 'g--', label=\"True Function\")\n    plt.scatter(X_train, y_train, color='black', label=\"Training data\", alpha=0.6)\n    plt.plot(X_test, mean, label=\"Predicted Mean\", color=\"blue\")\n    plt.fill_between(X_test.ravel(), lower, upper, alpha=0.3, color=\"blue\", label = \"Uncertainty Interval\")\n    plt.legend()\n    plt.title(f\"{model_name} Uncertainty Test\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.show()\n</code></pre>"},{"location":"examples/bayesian_demo-checkpoint/#mc-dropout","title":"MC Dropout","text":"<pre><code>from src.uqregressors.bayesian.dropout import MCDropoutRegressor\n\nmodel = MCDropoutRegressor(\n    hidden_sizes=[100, 100],\n    dropout=0.1,\n    alpha=0.1,  # 90% confidence\n    n_samples=100,\n    epochs=1000,\n    learning_rate=1e-3,\n    device=\"cpu\",  # use \"cuda\" if GPU available\n    use_wandb=False\n)\n\nmodel.fit(X_train, y_train)\nmean, lower, upper = model.predict(X_test)\n\nplot_uncertainty_results(mean, lower, upper, \"MC Dropout Regressor\")\n</code></pre>"},{"location":"examples/coverage_validation/","title":"Coverage validation","text":""},{"location":"examples/coverage_validation/#data-generation","title":"Data Generation","text":"<pre><code>import numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler  \n\ndef process_data(X, y, train_size, random_state=None):\n    \"\"\" \n    Splits data into training, calibration, and test sets, and normalizes the inputs\n\n    Parameters:\n    ------------\n    X: array \n        The inputs of the dataset to be split \n    y: array \n        The outputs of the dataset to be split\n    train_size: float\n        fraction of the dataset to use for training (bounded between 0 and 1)\n    calibrate_size: float \n        fraction of the dataset to use for calibration (bounded between 0 and 1)\n    random_state: float \n        The split state to fix for consistent results between function calls (defaults to None)\n    \"\"\"\n\n    # Split into train/calibration and validation sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-(train_size), random_state=random_state)\n\n    return X_train, X_test, y_train, y_test\n\ndef recombine_data(X_calibrate, X_test, y_calibrate, y_test):\n    \"\"\"\n    Combines and then splits calibration and testing data for conformal prediction validation  \n\n    Parameters:\n    ----------\n    X_calibrate: array \n        The inputs of the calibration dataset \n    X_test: array \n        The inputs of the test dataset \n    y_calibrate: array \n        The outputs of the calibration dataset \n    y_test: array \n        The outputs of the test dataset \n    \"\"\"\n    test_size = y_test.size / (y_test.size + y_calibrate.size)\n    combined_X = np.append(X_calibrate, X_test, axis=0)\n    combined_y = np.append(y_calibrate, y_test)\n    new_X_calibrate, new_X_test, new_y_calibrate, new_y_test = train_test_split(combined_X, combined_y, test_size=test_size)\n    return new_X_calibrate, new_X_test, new_y_calibrate, new_y_test \n\ndef generate_unvaried_data(n):\n    \"\"\" \n    Generates synthetic data with no variance according to the sine function described above \n\n    Parameters: \n    -----------\n    n: int \n        amount of data to generate \n    \"\"\"\n    x = np.linspace(0, 1, n)\n    y = 0.1 * np.sin(2 * np.pi * x)\n    return(x.reshape(-1, 1), y)\n\ndef generate_varied_data(n):\n    \"\"\" \n    Generates synthetic data with variance according to the sine function described above\n\n    Parameters: \n    -----------\n    n: int \n        amount of data to generate \n    \"\"\"\n    rng = np.random.default_rng() \n    x = rng.random(n)\n    epsilon = rng.standard_normal(n)\n    y = 0.1 * (np.sin(2 * np.pi * x) + 0.4*epsilon * (0.1 + x)) \n    return x.reshape(-1, 1), y \n\ndef plot_data(X, y, ax, scatter = False, **kwargs):\n    \"\"\" \n    A generic plotting function \n\n    Parameters: \n    ----------- \n    X: np.array\n        The inputs to be plotted \n    y: np.array \n        The outputs to be plotted \n    ax: Axes \n        The axes on which to plot the data \n    scatter: bool \n        True if a scatter plot is desired, false if a line graph is desired \n    **kwargs \n        matplotlib additional plotting parameters \n    \"\"\"\n    if scatter == True: \n        ax.scatter(X, y, **kwargs)\n    else: \n        ax.plot(X, y, **kwargs)\n    ax.set_xlabel(\"x\")\n    ax.set_ylabel(\"y\")\n\nfig, ax = plt.subplots() \nfun_x, fun_y = generate_unvaried_data(100)\nx,y = generate_varied_data(1000)\nX_train, X_test, y_train, y_test = process_data(x, y, 0.25)\nplot_data(X_train, y_train, ax, scatter=True, color='blue', alpha = 0.3, label=\"Train\", s=8)\nplot_data(X_test, y_test, ax, scatter=True, color='orange', alpha = 0.3, label=\"Test\", s=8)\nplot_data(fun_x, fun_y, ax, color='red', linestyle = 'dashed', label='Function')\nplt.legend()\nplt.show()\n</code></pre> <pre><code>from uqregressors.conformal.conformal_split import ConformalQuantileRegressor\nfrom uqregressors.metrics.metrics import coverage\n\ndef run_prediction_trials(model, alpha, num_trials, dataset_size, train_size, calibrate_size=0):\n    \"\"\" \n    Runs a number of trials of conformal prediction. The model is trained once, and the \n    calibration and test set are combined and re-split for every iteration. The coverage is calculated, \n    and the distribution of coverage is output. \n\n    Parameters: \n    -----------\n    num_trials: int \n        The number of trials to carry out \n    dataset_size: int \n        The number of data points to generate \n    train_size: float \n        The size of the training dataset relative to the entire dataset size (bounded between 0 and 1)\n    calibrate_size: float \n        The size of the calibration dataset relative to the entire dataset size (bounded between 0 and 1)\n    poly_order: int \n        The order of the polynomials used to fit the quantile regression functions\n    alpha: float\n        One minus the desired confidence interval bounded between 0 and 1 (i.e., 0.05)\n    \"\"\"\n    coverage_dist = np.empty(num_trials)\n    X, y = generate_unvaried_data(dataset_size)\n    X_train, X_test, y_train, y_test = process_data(X, y, train_size)\n    for i in range(num_trials): \n        X_train, X_test, y_train, y_test = process_data(X, y, train_size)\n        model.fit(X_train, y_train)\n        mean, lower, upper = model.predict(X_test)\n        coverage_dist[i] = coverage(lower, upper, y_test)\n        print(i)\n        print(f\"coverage: {coverage_dist[i]}\")\n    return coverage_dist\n\ndef plot_histogram(distribution, alpha, bin_width=0.005, n2 = None):\n    \"\"\"\n    Plots a histogram of the coverage distribution \n\n    Parameters: \n    ----------\n    distribution: array\n        The distribution to plot a histogram for\n    bin_width: float \n        The width of the histogram bins bounded between 0 and 1 \n        Defaults to 0.005 (half a percent)\n\n    \"\"\"\n    fig, ax = plt.subplots(tight_layout=True)\n    plt.hist(distribution, bins=int(1/bin_width))\n    #ax.set_xlim(0, 1)\n    ax.set_xlabel(\"Coverage\")\n    ax.set_ylabel(\"Frequency\")\n    average = np.mean(distribution)\n    if n2 is not None: \n        ka = np.ceil((1-alpha) * (n2+1))\n        kb = n2 + 1 - ka\n        gen = np.random.default_rng()\n        distr = gen.beta(ka, kb, 10000)\n        plt.hist(distr, bins = int(100/(bin_width)), alpha=0.5, label = 'Beta($k_a, n_2+1-k_a$)')\n\n    plt.axvline(x=average, color='red', linestyle='--', label = 'Average Coverage')\n    plt.axvline(x=1-alpha, color = 'blue', linestyle = ':', label = 'Desired Coverage')\n    plt.legend()\n    plt.show()\n\nnum_trials=100\ndataset_size = 1000\nbin_width = 0.04\ntrain_size = 0.5 \ncalibrate_size = 0.5\nn2 = int(calibrate_size * dataset_size * train_size)\nalpha = 0.1\n\nmodel = ConformalQuantileRegressor(\n    hidden_sizes = [128, 128], \n    cal_size=calibrate_size,\n    alpha=alpha,\n    dropout=None, \n    epochs=100, \n    batch_size=256, \n    learning_rate=5e-4, \n    optimizer_kwargs = {\"weight_decay\": 1e-6}, \n    device=\"cuda\", \n    scale_data=True, \n    use_wandb=False\n)\n\ncoverage_distribution = run_prediction_trials(model, alpha, num_trials, dataset_size, train_size, calibrate_size)\nplot_histogram(coverage_distribution, alpha, bin_width, n2=n2)\n</code></pre> <pre><code>[Logger-12256] epoch=0, train_loss=0.33394166827201843\n\n\n[Logger-12256] epoch=5, train_loss=0.22572878003120422\n[Logger-12256] epoch=10, train_loss=0.1418345868587494\n[Logger-12256] epoch=15, train_loss=0.08290089666843414\n[Logger-12256] epoch=20, train_loss=0.05647741258144379\n[Logger-12256] epoch=25, train_loss=0.05324038118124008\n[Logger-12256] epoch=30, train_loss=0.053915835916996\n[Logger-12256] epoch=35, train_loss=0.05143670737743378\n[Logger-12256] epoch=40, train_loss=0.04711431264877319\n[Logger-12256] epoch=45, train_loss=0.043042268604040146\n[Logger-12256] epoch=50, train_loss=0.04160427302122116\n[Logger-12256] epoch=55, train_loss=0.04088420420885086\n[Logger-12256] epoch=60, train_loss=0.03980352729558945\n[Logger-12256] epoch=65, train_loss=0.03921408951282501\n[Logger-12256] epoch=70, train_loss=0.03847067058086395\n[Logger-12256] epoch=75, train_loss=0.037756484001874924\n[Logger-12256] epoch=80, train_loss=0.03716643154621124\n[Logger-12256] epoch=85, train_loss=0.03648632764816284\n[Logger-12256] epoch=90, train_loss=0.03577606752514839\n[Logger-12256] epoch=95, train_loss=0.03505394235253334\n[Logger-12256] epoch=0, train_loss=0.4579327404499054\n[Logger-12256] epoch=5, train_loss=0.298176109790802\n[Logger-12256] epoch=10, train_loss=0.18127408623695374\n[Logger-12256] epoch=15, train_loss=0.1047208160161972\n[Logger-12256] epoch=20, train_loss=0.07089071720838547\n[Logger-12256] epoch=25, train_loss=0.058242615312337875\n\n\n0\ncoverage: 0.876\n\n\n[Logger-12256] epoch=30, train_loss=0.06011021509766579\n[Logger-12256] epoch=35, train_loss=0.058747973293066025\n[Logger-12256] epoch=40, train_loss=0.05487891286611557\n[Logger-12256] epoch=45, train_loss=0.049597304314374924\n[Logger-12256] epoch=50, train_loss=0.044612713158130646\n[Logger-12256] epoch=55, train_loss=0.04365474358201027\n[Logger-12256] epoch=60, train_loss=0.041931163519620895\n[Logger-12256] epoch=65, train_loss=0.040652524679899216\n[Logger-12256] epoch=70, train_loss=0.04011562466621399\n[Logger-12256] epoch=75, train_loss=0.03959884122014046\n[Logger-12256] epoch=80, train_loss=0.03891214355826378\n[Logger-12256] epoch=85, train_loss=0.03848185017704964\n[Logger-12256] epoch=90, train_loss=0.03796885162591934\n[Logger-12256] epoch=95, train_loss=0.03745894506573677\n[Logger-12256] epoch=0, train_loss=0.46700233221054077\n[Logger-12256] epoch=5, train_loss=0.3514181077480316\n[Logger-12256] epoch=10, train_loss=0.26130008697509766\n[Logger-12256] epoch=15, train_loss=0.18561401963233948\n[Logger-12256] epoch=20, train_loss=0.12487359344959259\n[Logger-12256] epoch=25, train_loss=0.08129637688398361\n\n\n1\ncoverage: 0.88\n\n\n[Logger-12256] epoch=30, train_loss=0.05519172549247742\n[Logger-12256] epoch=35, train_loss=0.053641900420188904\n[Logger-12256] epoch=40, train_loss=0.054801225662231445\n[Logger-12256] epoch=45, train_loss=0.05332517623901367\n[Logger-12256] epoch=50, train_loss=0.05025111138820648\n[Logger-12256] epoch=55, train_loss=0.047820087522268295\n[Logger-12256] epoch=60, train_loss=0.046231627464294434\n[Logger-12256] epoch=65, train_loss=0.043885041028261185\n[Logger-12256] epoch=70, train_loss=0.04292769730091095\n[Logger-12256] epoch=75, train_loss=0.042452793568372726\n[Logger-12256] epoch=80, train_loss=0.04187535494565964\n[Logger-12256] epoch=85, train_loss=0.04126426950097084\n[Logger-12256] epoch=90, train_loss=0.04091954231262207\n[Logger-12256] epoch=95, train_loss=0.04049545153975487\n[Logger-12256] epoch=0, train_loss=0.4052925705909729\n[Logger-12256] epoch=5, train_loss=0.2866857051849365\n[Logger-12256] epoch=10, train_loss=0.18938535451889038\n[Logger-12256] epoch=15, train_loss=0.11752051115036011\n[Logger-12256] epoch=20, train_loss=0.06858093291521072\n[Logger-12256] epoch=25, train_loss=0.050721071660518646\n\n\n2\ncoverage: 0.902\n\n\n[Logger-12256] epoch=30, train_loss=0.051876332610845566\n[Logger-12256] epoch=35, train_loss=0.051751069724559784\n[Logger-12256] epoch=40, train_loss=0.049531497061252594\n[Logger-12256] epoch=45, train_loss=0.04848005622625351\n[Logger-12256] epoch=50, train_loss=0.046381451189517975\n[Logger-12256] epoch=55, train_loss=0.04348361864686012\n[Logger-12256] epoch=60, train_loss=0.04136310890316963\n[Logger-12256] epoch=65, train_loss=0.04097980633378029\n[Logger-12256] epoch=70, train_loss=0.039436791092157364\n[Logger-12256] epoch=75, train_loss=0.03898883983492851\n[Logger-12256] epoch=80, train_loss=0.03796379640698433\n[Logger-12256] epoch=85, train_loss=0.03735342249274254\n[Logger-12256] epoch=90, train_loss=0.03661832585930824\n[Logger-12256] epoch=95, train_loss=0.03578880801796913\n[Logger-12256] epoch=0, train_loss=0.4230755865573883\n[Logger-12256] epoch=5, train_loss=0.30719175934791565\n[Logger-12256] epoch=10, train_loss=0.21513721346855164\n[Logger-12256] epoch=15, train_loss=0.1405431181192398\n[Logger-12256] epoch=20, train_loss=0.08177884668111801\n[Logger-12256] epoch=25, train_loss=0.05045656859874725\n\n\n3\ncoverage: 0.894\n\n\n[Logger-12256] epoch=30, train_loss=0.05081334710121155\n[Logger-12256] epoch=35, train_loss=0.051839105784893036\n[Logger-12256] epoch=40, train_loss=0.05098915845155716\n[Logger-12256] epoch=45, train_loss=0.04848653823137283\n[Logger-12256] epoch=50, train_loss=0.045025259256362915\n[Logger-12256] epoch=55, train_loss=0.04282280057668686\n[Logger-12256] epoch=60, train_loss=0.04144277423620224\n[Logger-12256] epoch=65, train_loss=0.04080062359571457\n[Logger-12256] epoch=70, train_loss=0.03960222005844116\n[Logger-12256] epoch=75, train_loss=0.03882706165313721\n[Logger-12256] epoch=80, train_loss=0.03836583346128464\n[Logger-12256] epoch=85, train_loss=0.03776262700557709\n[Logger-12256] epoch=90, train_loss=0.03721357882022858\n[Logger-12256] epoch=95, train_loss=0.03664689138531685\n[Logger-12256] epoch=0, train_loss=0.4417931139469147\n[Logger-12256] epoch=5, train_loss=0.303796648979187\n[Logger-12256] epoch=10, train_loss=0.19901160895824432\n[Logger-12256] epoch=15, train_loss=0.12370622158050537\n[Logger-12256] epoch=20, train_loss=0.0751764178276062\n[Logger-12256] epoch=25, train_loss=0.05859098955988884\n\n\n4\ncoverage: 0.904\n\n\n[Logger-12256] epoch=30, train_loss=0.05612083524465561\n[Logger-12256] epoch=35, train_loss=0.05654577165842056\n[Logger-12256] epoch=40, train_loss=0.05396697670221329\n[Logger-12256] epoch=45, train_loss=0.050640299916267395\n[Logger-12256] epoch=50, train_loss=0.04762575030326843\n[Logger-12256] epoch=55, train_loss=0.04510403051972389\n[Logger-12256] epoch=60, train_loss=0.04377949610352516\n[Logger-12256] epoch=65, train_loss=0.04341172054409981\n[Logger-12256] epoch=70, train_loss=0.042795006185770035\n[Logger-12256] epoch=75, train_loss=0.04199659079313278\n[Logger-12256] epoch=80, train_loss=0.04145028814673424\n[Logger-12256] epoch=85, train_loss=0.04108443483710289\n[Logger-12256] epoch=90, train_loss=0.040640801191329956\n[Logger-12256] epoch=95, train_loss=0.04010618105530739\n[Logger-12256] epoch=0, train_loss=0.4707830548286438\n[Logger-12256] epoch=5, train_loss=0.33859071135520935\n[Logger-12256] epoch=10, train_loss=0.23705217242240906\n[Logger-12256] epoch=15, train_loss=0.16010096669197083\n[Logger-12256] epoch=20, train_loss=0.09982089698314667\n[Logger-12256] epoch=25, train_loss=0.0607818104326725\n\n\n5\ncoverage: 0.904\n\n\n[Logger-12256] epoch=30, train_loss=0.051301829516887665\n[Logger-12256] epoch=35, train_loss=0.05434068664908409\n[Logger-12256] epoch=40, train_loss=0.05451461672782898\n[Logger-12256] epoch=45, train_loss=0.05276027321815491\n[Logger-12256] epoch=50, train_loss=0.04976188391447067\n[Logger-12256] epoch=55, train_loss=0.04591844975948334\n[Logger-12256] epoch=60, train_loss=0.04321767017245293\n[Logger-12256] epoch=65, train_loss=0.041957102715969086\n[Logger-12256] epoch=70, train_loss=0.040057871490716934\n[Logger-12256] epoch=75, train_loss=0.03917588293552399\n[Logger-12256] epoch=80, train_loss=0.038503386080265045\n[Logger-12256] epoch=85, train_loss=0.03816106542944908\n[Logger-12256] epoch=90, train_loss=0.037584833800792694\n[Logger-12256] epoch=95, train_loss=0.03713864088058472\n[Logger-12256] epoch=0, train_loss=0.4430817663669586\n[Logger-12256] epoch=5, train_loss=0.30982181429862976\n[Logger-12256] epoch=10, train_loss=0.20239517092704773\n[Logger-12256] epoch=15, train_loss=0.12124054878950119\n[Logger-12256] epoch=20, train_loss=0.06768611073493958\n[Logger-12256] epoch=25, train_loss=0.055319253355264664\n\n\n6\ncoverage: 0.906\n\n\n[Logger-12256] epoch=30, train_loss=0.05916263908147812\n[Logger-12256] epoch=35, train_loss=0.05868164822459221\n[Logger-12256] epoch=40, train_loss=0.05562475696206093\n[Logger-12256] epoch=45, train_loss=0.05251235514879227\n[Logger-12256] epoch=50, train_loss=0.04876844212412834\n[Logger-12256] epoch=55, train_loss=0.04412331432104111\n[Logger-12256] epoch=60, train_loss=0.043410226702690125\n[Logger-12256] epoch=65, train_loss=0.041983697563409805\n[Logger-12256] epoch=70, train_loss=0.04175342619419098\n[Logger-12256] epoch=75, train_loss=0.04105975478887558\n[Logger-12256] epoch=80, train_loss=0.04044853150844574\n[Logger-12256] epoch=85, train_loss=0.03995152562856674\n[Logger-12256] epoch=90, train_loss=0.039470821619033813\n[Logger-12256] epoch=95, train_loss=0.03901049867272377\n[Logger-12256] epoch=0, train_loss=0.4578493535518646\n[Logger-12256] epoch=5, train_loss=0.34747323393821716\n[Logger-12256] epoch=10, train_loss=0.25813373923301697\n[Logger-12256] epoch=15, train_loss=0.18357324600219727\n[Logger-12256] epoch=20, train_loss=0.12262773513793945\n[Logger-12256] epoch=25, train_loss=0.0818004459142685\n\n\n7\ncoverage: 0.93\n\n\n[Logger-12256] epoch=30, train_loss=0.06365722417831421\n[Logger-12256] epoch=35, train_loss=0.05328615754842758\n[Logger-12256] epoch=40, train_loss=0.051623910665512085\n[Logger-12256] epoch=45, train_loss=0.05021991953253746\n[Logger-12256] epoch=50, train_loss=0.047905851155519485\n[Logger-12256] epoch=55, train_loss=0.04544365033507347\n[Logger-12256] epoch=60, train_loss=0.04421808943152428\n[Logger-12256] epoch=65, train_loss=0.04200667887926102\n[Logger-12256] epoch=70, train_loss=0.04138593748211861\n[Logger-12256] epoch=75, train_loss=0.04033143073320389\n[Logger-12256] epoch=80, train_loss=0.0396508127450943\n[Logger-12256] epoch=85, train_loss=0.038971398025751114\n[Logger-12256] epoch=90, train_loss=0.038336992263793945\n[Logger-12256] epoch=95, train_loss=0.037706419825553894\n[Logger-12256] epoch=0, train_loss=0.4035841226577759\n[Logger-12256] epoch=5, train_loss=0.26731520891189575\n[Logger-12256] epoch=10, train_loss=0.1639419049024582\n[Logger-12256] epoch=15, train_loss=0.09016336500644684\n[Logger-12256] epoch=20, train_loss=0.05397988110780716\n[Logger-12256] epoch=25, train_loss=0.05702410265803337\n\n\n8\ncoverage: 0.888\n\n\n[Logger-12256] epoch=30, train_loss=0.05798063054680824\n[Logger-12256] epoch=35, train_loss=0.055262621492147446\n[Logger-12256] epoch=40, train_loss=0.05156954750418663\n[Logger-12256] epoch=45, train_loss=0.047640610486269\n[Logger-12256] epoch=50, train_loss=0.04278697445988655\n[Logger-12256] epoch=55, train_loss=0.04270198941230774\n[Logger-12256] epoch=60, train_loss=0.04155401885509491\n[Logger-12256] epoch=65, train_loss=0.04070286080241203\n[Logger-12256] epoch=70, train_loss=0.03978952392935753\n[Logger-12256] epoch=75, train_loss=0.039327580481767654\n[Logger-12256] epoch=80, train_loss=0.038759201765060425\n[Logger-12256] epoch=85, train_loss=0.038179848343133926\n[Logger-12256] epoch=90, train_loss=0.03759804740548134\n[Logger-12256] epoch=95, train_loss=0.037074580788612366\n[Logger-12256] epoch=0, train_loss=0.4184277057647705\n[Logger-12256] epoch=5, train_loss=0.3137521743774414\n[Logger-12256] epoch=10, train_loss=0.23106852173805237\n[Logger-12256] epoch=15, train_loss=0.1634887456893921\n[Logger-12256] epoch=20, train_loss=0.11197890341281891\n[Logger-12256] epoch=25, train_loss=0.08003143221139908\n[Logger-12256] epoch=30, train_loss=0.060313645750284195\n\n\n9\ncoverage: 0.898\n\n\n[Logger-12256] epoch=35, train_loss=0.049323052167892456\n[Logger-12256] epoch=40, train_loss=0.0496334470808506\n[Logger-12256] epoch=45, train_loss=0.04874970391392708\n[Logger-12256] epoch=50, train_loss=0.04636212810873985\n[Logger-12256] epoch=55, train_loss=0.04536985978484154\n[Logger-12256] epoch=60, train_loss=0.04374296963214874\n[Logger-12256] epoch=65, train_loss=0.04244004935026169\n[Logger-12256] epoch=70, train_loss=0.04070959985256195\n[Logger-12256] epoch=75, train_loss=0.040438782423734665\n[Logger-12256] epoch=80, train_loss=0.039544522762298584\n[Logger-12256] epoch=85, train_loss=0.03901215270161629\n[Logger-12256] epoch=90, train_loss=0.03833196684718132\n[Logger-12256] epoch=95, train_loss=0.037848878651857376\n[Logger-12256] epoch=0, train_loss=0.5110581517219543\n[Logger-12256] epoch=5, train_loss=0.3525412678718567\n[Logger-12256] epoch=10, train_loss=0.22809891402721405\n[Logger-12256] epoch=15, train_loss=0.1348511427640915\n[Logger-12256] epoch=20, train_loss=0.07149505615234375\n\n\n10\ncoverage: 0.924\n\n\n[Logger-12256] epoch=25, train_loss=0.05893419310450554\n[Logger-12256] epoch=30, train_loss=0.06342131644487381\n[Logger-12256] epoch=35, train_loss=0.06308244168758392\n[Logger-12256] epoch=40, train_loss=0.059755176305770874\n[Logger-12256] epoch=45, train_loss=0.054685428738594055\n[Logger-12256] epoch=50, train_loss=0.04870741814374924\n[Logger-12256] epoch=55, train_loss=0.04437125474214554\n[Logger-12256] epoch=60, train_loss=0.042276423424482346\n[Logger-12256] epoch=65, train_loss=0.04214318469166756\n[Logger-12256] epoch=70, train_loss=0.04107102006673813\n[Logger-12256] epoch=75, train_loss=0.040450699627399445\n[Logger-12256] epoch=80, train_loss=0.039956338703632355\n[Logger-12256] epoch=85, train_loss=0.03952188417315483\n[Logger-12256] epoch=90, train_loss=0.038942255079746246\n[Logger-12256] epoch=95, train_loss=0.038529518991708755\n[Logger-12256] epoch=0, train_loss=0.4805208742618561\n[Logger-12256] epoch=5, train_loss=0.3595598340034485\n[Logger-12256] epoch=10, train_loss=0.26744794845581055\n[Logger-12256] epoch=15, train_loss=0.19781138002872467\n[Logger-12256] epoch=20, train_loss=0.14025624096393585\n[Logger-12256] epoch=25, train_loss=0.09695538878440857\n\n\n11\ncoverage: 0.83\n\n\n[Logger-12256] epoch=30, train_loss=0.0672072172164917\n[Logger-12256] epoch=35, train_loss=0.05451824516057968\n[Logger-12256] epoch=40, train_loss=0.05179238319396973\n[Logger-12256] epoch=45, train_loss=0.05160314217209816\n[Logger-12256] epoch=50, train_loss=0.049237899482250214\n[Logger-12256] epoch=55, train_loss=0.04582656919956207\n[Logger-12256] epoch=60, train_loss=0.043793290853500366\n[Logger-12256] epoch=65, train_loss=0.04135841503739357\n[Logger-12256] epoch=70, train_loss=0.041513651609420776\n[Logger-12256] epoch=75, train_loss=0.04064660146832466\n[Logger-12256] epoch=80, train_loss=0.039776988327503204\n[Logger-12256] epoch=85, train_loss=0.03925645351409912\n[Logger-12256] epoch=90, train_loss=0.03882026672363281\n[Logger-12256] epoch=95, train_loss=0.03841616213321686\n[Logger-12256] epoch=0, train_loss=0.3978217542171478\n[Logger-12256] epoch=5, train_loss=0.2832140028476715\n[Logger-12256] epoch=10, train_loss=0.19167038798332214\n[Logger-12256] epoch=15, train_loss=0.12243583053350449\n[Logger-12256] epoch=20, train_loss=0.07618235796689987\n[Logger-12256] epoch=25, train_loss=0.05533194541931152\n[Logger-12256] epoch=30, train_loss=0.04690377041697502\n\n\n12\ncoverage: 0.904\n\n\n[Logger-12256] epoch=35, train_loss=0.04725082218647003\n[Logger-12256] epoch=40, train_loss=0.0464194156229496\n[Logger-12256] epoch=45, train_loss=0.04511246085166931\n[Logger-12256] epoch=50, train_loss=0.043711498379707336\n[Logger-12256] epoch=55, train_loss=0.04209993779659271\n[Logger-12256] epoch=60, train_loss=0.04046166315674782\n[Logger-12256] epoch=65, train_loss=0.039197083562612534\n[Logger-12256] epoch=70, train_loss=0.03912224620580673\n[Logger-12256] epoch=75, train_loss=0.03818916901946068\n[Logger-12256] epoch=80, train_loss=0.03767184540629387\n[Logger-12256] epoch=85, train_loss=0.03703077509999275\n[Logger-12256] epoch=90, train_loss=0.03646078705787659\n[Logger-12256] epoch=95, train_loss=0.035879578441381454\n[Logger-12256] epoch=0, train_loss=0.40841981768608093\n[Logger-12256] epoch=5, train_loss=0.28849929571151733\n[Logger-12256] epoch=10, train_loss=0.19423699378967285\n[Logger-12256] epoch=15, train_loss=0.1193569004535675\n[Logger-12256] epoch=20, train_loss=0.06706659495830536\n[Logger-12256] epoch=25, train_loss=0.046403661370277405\n\n\n13\ncoverage: 0.91\n\n\n[Logger-12256] epoch=30, train_loss=0.04993565380573273\n[Logger-12256] epoch=35, train_loss=0.05013219267129898\n[Logger-12256] epoch=40, train_loss=0.04923992231488228\n[Logger-12256] epoch=45, train_loss=0.04840020835399628\n[Logger-12256] epoch=50, train_loss=0.0464676171541214\n[Logger-12256] epoch=55, train_loss=0.04388323426246643\n[Logger-12256] epoch=60, train_loss=0.042579539120197296\n[Logger-12256] epoch=65, train_loss=0.04081381857395172\n[Logger-12256] epoch=70, train_loss=0.03925364091992378\n[Logger-12256] epoch=75, train_loss=0.03921360522508621\n[Logger-12256] epoch=80, train_loss=0.03823848068714142\n[Logger-12256] epoch=85, train_loss=0.03777641803026199\n[Logger-12256] epoch=90, train_loss=0.036938317120075226\n[Logger-12256] epoch=95, train_loss=0.036304835230112076\n[Logger-12256] epoch=0, train_loss=0.505885899066925\n[Logger-12256] epoch=5, train_loss=0.39254263043403625\n[Logger-12256] epoch=10, train_loss=0.2975737154483795\n[Logger-12256] epoch=15, train_loss=0.21467450261116028\n[Logger-12256] epoch=20, train_loss=0.1397470384836197\n[Logger-12256] epoch=25, train_loss=0.07921264320611954\n[Logger-12256] epoch=30, train_loss=0.046783603727817535\n\n\n14\ncoverage: 0.868\n\n\n[Logger-12256] epoch=35, train_loss=0.04756186529994011\n[Logger-12256] epoch=40, train_loss=0.04877442494034767\n[Logger-12256] epoch=45, train_loss=0.04828802868723869\n[Logger-12256] epoch=50, train_loss=0.04757646471261978\n[Logger-12256] epoch=55, train_loss=0.04558748006820679\n[Logger-12256] epoch=60, train_loss=0.04315613582730293\n[Logger-12256] epoch=65, train_loss=0.04075997322797775\n[Logger-12256] epoch=70, train_loss=0.04010395333170891\n[Logger-12256] epoch=75, train_loss=0.03909994661808014\n[Logger-12256] epoch=80, train_loss=0.03870677947998047\n[Logger-12256] epoch=85, train_loss=0.03768144175410271\n[Logger-12256] epoch=90, train_loss=0.03721576929092407\n[Logger-12256] epoch=95, train_loss=0.036575399339199066\n[Logger-12256] epoch=0, train_loss=0.42545247077941895\n[Logger-12256] epoch=5, train_loss=0.29943040013313293\n[Logger-12256] epoch=10, train_loss=0.2021927535533905\n[Logger-12256] epoch=15, train_loss=0.1255113184452057\n[Logger-12256] epoch=20, train_loss=0.07043492794036865\n[Logger-12256] epoch=25, train_loss=0.048940423876047134\n\n\n15\ncoverage: 0.876\n\n\n[Logger-12256] epoch=30, train_loss=0.052767977118492126\n[Logger-12256] epoch=35, train_loss=0.05301469936966896\n[Logger-12256] epoch=40, train_loss=0.05130397155880928\n[Logger-12256] epoch=45, train_loss=0.04895634576678276\n[Logger-12256] epoch=50, train_loss=0.04610363394021988\n[Logger-12256] epoch=55, train_loss=0.04375648871064186\n[Logger-12256] epoch=60, train_loss=0.041762884706258774\n[Logger-12256] epoch=65, train_loss=0.040416788309812546\n[Logger-12256] epoch=70, train_loss=0.04013833776116371\n[Logger-12256] epoch=75, train_loss=0.039136216044425964\n[Logger-12256] epoch=80, train_loss=0.038897909224033356\n[Logger-12256] epoch=85, train_loss=0.03802688792347908\n[Logger-12256] epoch=90, train_loss=0.03762972727417946\n[Logger-12256] epoch=95, train_loss=0.037025172263383865\n[Logger-12256] epoch=0, train_loss=0.38566407561302185\n[Logger-12256] epoch=5, train_loss=0.2629270851612091\n[Logger-12256] epoch=10, train_loss=0.16759547591209412\n[Logger-12256] epoch=15, train_loss=0.09689392894506454\n[Logger-12256] epoch=20, train_loss=0.06386503577232361\n[Logger-12256] epoch=25, train_loss=0.06280487030744553\n\n\n16\ncoverage: 0.882\n\n\n[Logger-12256] epoch=30, train_loss=0.06456511467695236\n[Logger-12256] epoch=35, train_loss=0.06265425682067871\n[Logger-12256] epoch=40, train_loss=0.05855628475546837\n[Logger-12256] epoch=45, train_loss=0.05299445241689682\n[Logger-12256] epoch=50, train_loss=0.04629424214363098\n[Logger-12256] epoch=55, train_loss=0.04516388848423958\n[Logger-12256] epoch=60, train_loss=0.04270930588245392\n[Logger-12256] epoch=65, train_loss=0.041767384856939316\n[Logger-12256] epoch=70, train_loss=0.041089776903390884\n[Logger-12256] epoch=75, train_loss=0.04005603864789009\n[Logger-12256] epoch=80, train_loss=0.03983568027615547\n[Logger-12256] epoch=85, train_loss=0.03919080272316933\n[Logger-12256] epoch=90, train_loss=0.038721468299627304\n[Logger-12256] epoch=95, train_loss=0.038249216973781586\n[Logger-12256] epoch=0, train_loss=0.40864142775535583\n[Logger-12256] epoch=5, train_loss=0.28915759921073914\n[Logger-12256] epoch=10, train_loss=0.18958646059036255\n[Logger-12256] epoch=15, train_loss=0.11169217526912689\n[Logger-12256] epoch=20, train_loss=0.06532174348831177\n[Logger-12256] epoch=25, train_loss=0.05366978049278259\n\n\n17\ncoverage: 0.938\n\n\n[Logger-12256] epoch=30, train_loss=0.056303203105926514\n[Logger-12256] epoch=35, train_loss=0.055029574781656265\n[Logger-12256] epoch=40, train_loss=0.051758114248514175\n[Logger-12256] epoch=45, train_loss=0.04822610691189766\n[Logger-12256] epoch=50, train_loss=0.045483317226171494\n[Logger-12256] epoch=55, train_loss=0.043346453458070755\n[Logger-12256] epoch=60, train_loss=0.04313034936785698\n[Logger-12256] epoch=65, train_loss=0.04130527749657631\n[Logger-12256] epoch=70, train_loss=0.04102477431297302\n[Logger-12256] epoch=75, train_loss=0.04018363356590271\n[Logger-12256] epoch=80, train_loss=0.0395657941699028\n[Logger-12256] epoch=85, train_loss=0.03894928842782974\n[Logger-12256] epoch=90, train_loss=0.038342591375112534\n[Logger-12256] epoch=95, train_loss=0.037670474499464035\n[Logger-12256] epoch=0, train_loss=0.3796504735946655\n[Logger-12256] epoch=5, train_loss=0.25233885645866394\n[Logger-12256] epoch=10, train_loss=0.15911805629730225\n[Logger-12256] epoch=15, train_loss=0.09316622465848923\n[Logger-12256] epoch=20, train_loss=0.05799255147576332\n[Logger-12256] epoch=25, train_loss=0.05444485694169998\n\n\n18\ncoverage: 0.914\n\n\n[Logger-12256] epoch=30, train_loss=0.055930159986019135\n[Logger-12256] epoch=35, train_loss=0.05353245511651039\n[Logger-12256] epoch=40, train_loss=0.04888696223497391\n[Logger-12256] epoch=45, train_loss=0.04509972780942917\n[Logger-12256] epoch=50, train_loss=0.042675238102674484\n[Logger-12256] epoch=55, train_loss=0.04134593531489372\n[Logger-12256] epoch=60, train_loss=0.04065074026584625\n[Logger-12256] epoch=65, train_loss=0.03992433473467827\n[Logger-12256] epoch=70, train_loss=0.03920048102736473\n[Logger-12256] epoch=75, train_loss=0.03865467384457588\n[Logger-12256] epoch=80, train_loss=0.03801831975579262\n[Logger-12256] epoch=85, train_loss=0.03740604594349861\n[Logger-12256] epoch=90, train_loss=0.036793291568756104\n[Logger-12256] epoch=95, train_loss=0.03613382205367088\n[Logger-12256] epoch=0, train_loss=0.4305196702480316\n[Logger-12256] epoch=5, train_loss=0.2874975800514221\n[Logger-12256] epoch=10, train_loss=0.17315086722373962\n[Logger-12256] epoch=15, train_loss=0.08857454359531403\n[Logger-12256] epoch=20, train_loss=0.04786679893732071\n[Logger-12256] epoch=25, train_loss=0.05357085540890694\n\n\n19\ncoverage: 0.91\n\n\n[Logger-12256] epoch=30, train_loss=0.05426793918013573\n[Logger-12256] epoch=35, train_loss=0.05159677565097809\n[Logger-12256] epoch=40, train_loss=0.049672629684209824\n[Logger-12256] epoch=45, train_loss=0.04749782010912895\n[Logger-12256] epoch=50, train_loss=0.043970126658678055\n[Logger-12256] epoch=55, train_loss=0.040567345917224884\n[Logger-12256] epoch=60, train_loss=0.0399918369948864\n[Logger-12256] epoch=65, train_loss=0.03942794352769852\n[Logger-12256] epoch=70, train_loss=0.03850001096725464\n[Logger-12256] epoch=75, train_loss=0.03796840459108353\n[Logger-12256] epoch=80, train_loss=0.03732091560959816\n[Logger-12256] epoch=85, train_loss=0.036733392626047134\n[Logger-12256] epoch=90, train_loss=0.036121949553489685\n[Logger-12256] epoch=95, train_loss=0.035527780652046204\n[Logger-12256] epoch=0, train_loss=0.4947355091571808\n[Logger-12256] epoch=5, train_loss=0.35328203439712524\n[Logger-12256] epoch=10, train_loss=0.2392706573009491\n[Logger-12256] epoch=15, train_loss=0.15470191836357117\n[Logger-12256] epoch=20, train_loss=0.09748831391334534\n[Logger-12256] epoch=25, train_loss=0.06550861150026321\n\n\n20\ncoverage: 0.902\n\n\n[Logger-12256] epoch=30, train_loss=0.05996023118495941\n[Logger-12256] epoch=35, train_loss=0.06248693913221359\n[Logger-12256] epoch=40, train_loss=0.0612901970744133\n[Logger-12256] epoch=45, train_loss=0.05775889754295349\n[Logger-12256] epoch=50, train_loss=0.052818506956100464\n[Logger-12256] epoch=55, train_loss=0.047126058489084244\n[Logger-12256] epoch=60, train_loss=0.045440539717674255\n[Logger-12256] epoch=65, train_loss=0.043807487934827805\n[Logger-12256] epoch=70, train_loss=0.04345493018627167\n[Logger-12256] epoch=75, train_loss=0.042712315917015076\n[Logger-12256] epoch=80, train_loss=0.04203745350241661\n[Logger-12256] epoch=85, train_loss=0.04151947796344757\n[Logger-12256] epoch=90, train_loss=0.041050706058740616\n[Logger-12256] epoch=95, train_loss=0.04043428227305412\n[Logger-12256] epoch=0, train_loss=0.44800031185150146\n[Logger-12256] epoch=5, train_loss=0.3076123893260956\n[Logger-12256] epoch=10, train_loss=0.1996099352836609\n[Logger-12256] epoch=15, train_loss=0.12021122127771378\n[Logger-12256] epoch=20, train_loss=0.06462258100509644\n[Logger-12256] epoch=25, train_loss=0.04673934355378151\n[Logger-12256] epoch=30, train_loss=0.04994460567831993\n\n\n21\ncoverage: 0.918\n\n\n[Logger-12256] epoch=35, train_loss=0.049634844064712524\n[Logger-12256] epoch=40, train_loss=0.048037394881248474\n[Logger-12256] epoch=45, train_loss=0.04695679992437363\n[Logger-12256] epoch=50, train_loss=0.04470736160874367\n[Logger-12256] epoch=55, train_loss=0.04161956161260605\n[Logger-12256] epoch=60, train_loss=0.04016917571425438\n[Logger-12256] epoch=65, train_loss=0.03932994604110718\n[Logger-12256] epoch=70, train_loss=0.03873060643672943\n[Logger-12256] epoch=75, train_loss=0.03799925744533539\n[Logger-12256] epoch=80, train_loss=0.03749270737171173\n[Logger-12256] epoch=85, train_loss=0.03696958348155022\n[Logger-12256] epoch=90, train_loss=0.0363447405397892\n[Logger-12256] epoch=95, train_loss=0.0358111634850502\n[Logger-12256] epoch=0, train_loss=0.4349273145198822\n[Logger-12256] epoch=5, train_loss=0.29446396231651306\n[Logger-12256] epoch=10, train_loss=0.18983164429664612\n[Logger-12256] epoch=15, train_loss=0.1126762330532074\n[Logger-12256] epoch=20, train_loss=0.06232435628771782\n\n\n22\ncoverage: 0.884\n\n\n[Logger-12256] epoch=25, train_loss=0.047812771052122116\n[Logger-12256] epoch=30, train_loss=0.050650905817747116\n[Logger-12256] epoch=35, train_loss=0.04976498335599899\n[Logger-12256] epoch=40, train_loss=0.04815013334155083\n[Logger-12256] epoch=45, train_loss=0.04636288806796074\n[Logger-12256] epoch=50, train_loss=0.04374321177601814\n[Logger-12256] epoch=55, train_loss=0.04204052686691284\n[Logger-12256] epoch=60, train_loss=0.0399789921939373\n[Logger-12256] epoch=65, train_loss=0.039219025522470474\n[Logger-12256] epoch=70, train_loss=0.038435664027929306\n[Logger-12256] epoch=75, train_loss=0.037954412400722504\n[Logger-12256] epoch=80, train_loss=0.03719484061002731\n[Logger-12256] epoch=85, train_loss=0.036593757569789886\n[Logger-12256] epoch=90, train_loss=0.0360068753361702\n[Logger-12256] epoch=95, train_loss=0.03531627729535103\n[Logger-12256] epoch=0, train_loss=0.5072709321975708\n[Logger-12256] epoch=5, train_loss=0.3663739264011383\n[Logger-12256] epoch=10, train_loss=0.25395962595939636\n[Logger-12256] epoch=15, train_loss=0.1646871268749237\n[Logger-12256] epoch=20, train_loss=0.09860191494226456\n[Logger-12256] epoch=25, train_loss=0.06535381823778152\n\n\n23\ncoverage: 0.876\n\n\n[Logger-12256] epoch=30, train_loss=0.05148831754922867\n[Logger-12256] epoch=35, train_loss=0.052081398665905\n[Logger-12256] epoch=40, train_loss=0.05167888104915619\n[Logger-12256] epoch=45, train_loss=0.04910456761717796\n[Logger-12256] epoch=50, train_loss=0.04566577449440956\n[Logger-12256] epoch=55, train_loss=0.043021026998758316\n[Logger-12256] epoch=60, train_loss=0.04213380813598633\n[Logger-12256] epoch=65, train_loss=0.040603622794151306\n[Logger-12256] epoch=70, train_loss=0.03929123282432556\n[Logger-12256] epoch=75, train_loss=0.0385952852666378\n[Logger-12256] epoch=80, train_loss=0.03823232278227806\n[Logger-12256] epoch=85, train_loss=0.03767487779259682\n[Logger-12256] epoch=90, train_loss=0.03716062381863594\n[Logger-12256] epoch=95, train_loss=0.036815058439970016\n[Logger-12256] epoch=0, train_loss=0.39145949482917786\n[Logger-12256] epoch=5, train_loss=0.26243263483047485\n[Logger-12256] epoch=10, train_loss=0.16473492980003357\n[Logger-12256] epoch=15, train_loss=0.0926884338259697\n[Logger-12256] epoch=20, train_loss=0.054835688322782516\n[Logger-12256] epoch=25, train_loss=0.055125266313552856\n\n\n24\ncoverage: 0.906\n\n\n[Logger-12256] epoch=30, train_loss=0.05553901940584183\n[Logger-12256] epoch=35, train_loss=0.052809517830610275\n[Logger-12256] epoch=40, train_loss=0.04862932115793228\n[Logger-12256] epoch=45, train_loss=0.0449528843164444\n[Logger-12256] epoch=50, train_loss=0.04296111315488815\n[Logger-12256] epoch=55, train_loss=0.04207223653793335\n[Logger-12256] epoch=60, train_loss=0.040437836199998856\n[Logger-12256] epoch=65, train_loss=0.04007219523191452\n[Logger-12256] epoch=70, train_loss=0.03932356834411621\n[Logger-12256] epoch=75, train_loss=0.03849954530596733\n[Logger-12256] epoch=80, train_loss=0.0377749539911747\n[Logger-12256] epoch=85, train_loss=0.03704782947897911\n[Logger-12256] epoch=90, train_loss=0.03631439059972763\n[Logger-12256] epoch=95, train_loss=0.03554777055978775\n[Logger-12256] epoch=0, train_loss=0.42070505023002625\n[Logger-12256] epoch=5, train_loss=0.27648407220840454\n[Logger-12256] epoch=10, train_loss=0.16747623682022095\n[Logger-12256] epoch=15, train_loss=0.09784375131130219\n[Logger-12256] epoch=20, train_loss=0.06534639000892639\n[Logger-12256] epoch=25, train_loss=0.056245945394039154\n[Logger-12256] epoch=30, train_loss=0.05846787244081497\n\n\n25\ncoverage: 0.868\n\n\n[Logger-12256] epoch=35, train_loss=0.05668824166059494\n[Logger-12256] epoch=40, train_loss=0.052324771881103516\n[Logger-12256] epoch=45, train_loss=0.04747459664940834\n[Logger-12256] epoch=50, train_loss=0.04391636699438095\n[Logger-12256] epoch=55, train_loss=0.044003330171108246\n[Logger-12256] epoch=60, train_loss=0.04202618449926376\n[Logger-12256] epoch=65, train_loss=0.04168778657913208\n[Logger-12256] epoch=70, train_loss=0.04098387435078621\n[Logger-12256] epoch=75, train_loss=0.04015885666012764\n[Logger-12256] epoch=80, train_loss=0.0396137498319149\n[Logger-12256] epoch=85, train_loss=0.03902098163962364\n[Logger-12256] epoch=90, train_loss=0.03834524750709534\n[Logger-12256] epoch=95, train_loss=0.03774157911539078\n[Logger-12256] epoch=0, train_loss=0.5425097346305847\n[Logger-12256] epoch=5, train_loss=0.4024117588996887\n[Logger-12256] epoch=10, train_loss=0.283223956823349\n[Logger-12256] epoch=15, train_loss=0.19524605572223663\n[Logger-12256] epoch=20, train_loss=0.12816235423088074\n\n\n26\ncoverage: 0.886\n\n\n[Logger-12256] epoch=25, train_loss=0.07562720030546188\n[Logger-12256] epoch=30, train_loss=0.043714459985494614\n[Logger-12256] epoch=35, train_loss=0.04399867355823517\n[Logger-12256] epoch=40, train_loss=0.0461871437728405\n[Logger-12256] epoch=45, train_loss=0.04635843634605408\n[Logger-12256] epoch=50, train_loss=0.04524813964962959\n[Logger-12256] epoch=55, train_loss=0.04327714815735817\n[Logger-12256] epoch=60, train_loss=0.04090025648474693\n[Logger-12256] epoch=65, train_loss=0.03938239440321922\n[Logger-12256] epoch=70, train_loss=0.038047343492507935\n[Logger-12256] epoch=75, train_loss=0.036740709096193314\n[Logger-12256] epoch=80, train_loss=0.036021944135427475\n[Logger-12256] epoch=85, train_loss=0.03536895290017128\n[Logger-12256] epoch=90, train_loss=0.03464746102690697\n[Logger-12256] epoch=95, train_loss=0.03404906019568443\n[Logger-12256] epoch=0, train_loss=0.5046543478965759\n[Logger-12256] epoch=5, train_loss=0.356716513633728\n[Logger-12256] epoch=10, train_loss=0.2378430962562561\n[Logger-12256] epoch=15, train_loss=0.1455048769712448\n[Logger-12256] epoch=20, train_loss=0.07916359603404999\n[Logger-12256] epoch=25, train_loss=0.05174315348267555\n\n\n27\ncoverage: 0.922\n\n\n[Logger-12256] epoch=30, train_loss=0.05070463940501213\n[Logger-12256] epoch=35, train_loss=0.05183738097548485\n[Logger-12256] epoch=40, train_loss=0.0508846677839756\n[Logger-12256] epoch=45, train_loss=0.04804512858390808\n[Logger-12256] epoch=50, train_loss=0.044580284506082535\n[Logger-12256] epoch=55, train_loss=0.042568717151880264\n[Logger-12256] epoch=60, train_loss=0.040327273309230804\n[Logger-12256] epoch=65, train_loss=0.03926702216267586\n[Logger-12256] epoch=70, train_loss=0.0380355641245842\n[Logger-12256] epoch=75, train_loss=0.037609733641147614\n[Logger-12256] epoch=80, train_loss=0.0365474559366703\n[Logger-12256] epoch=85, train_loss=0.036047257483005524\n[Logger-12256] epoch=90, train_loss=0.03556974604725838\n[Logger-12256] epoch=95, train_loss=0.035031724721193314\n[Logger-12256] epoch=0, train_loss=0.4200776517391205\n[Logger-12256] epoch=5, train_loss=0.29152095317840576\n[Logger-12256] epoch=10, train_loss=0.18861575424671173\n[Logger-12256] epoch=15, train_loss=0.1053488478064537\n[Logger-12256] epoch=20, train_loss=0.05282897129654884\n[Logger-12256] epoch=25, train_loss=0.04702509939670563\n\n\n28\ncoverage: 0.928\n\n\n[Logger-12256] epoch=30, train_loss=0.048836033791303635\n[Logger-12256] epoch=35, train_loss=0.048775993287563324\n[Logger-12256] epoch=40, train_loss=0.04782148078083992\n[Logger-12256] epoch=45, train_loss=0.04594999924302101\n[Logger-12256] epoch=50, train_loss=0.04335013031959534\n[Logger-12256] epoch=55, train_loss=0.041544362902641296\n[Logger-12256] epoch=60, train_loss=0.03884059190750122\n[Logger-12256] epoch=65, train_loss=0.03832968324422836\n[Logger-12256] epoch=70, train_loss=0.03777972608804703\n[Logger-12256] epoch=75, train_loss=0.03665192425251007\n[Logger-12256] epoch=80, train_loss=0.03601181134581566\n[Logger-12256] epoch=85, train_loss=0.035331130027770996\n[Logger-12256] epoch=90, train_loss=0.03461915999650955\n[Logger-12256] epoch=95, train_loss=0.03390628844499588\n[Logger-12256] epoch=0, train_loss=0.4444371461868286\n[Logger-12256] epoch=5, train_loss=0.312029629945755\n[Logger-12256] epoch=10, train_loss=0.20869998633861542\n[Logger-12256] epoch=15, train_loss=0.1333385407924652\n[Logger-12256] epoch=20, train_loss=0.08261065930128098\n[Logger-12256] epoch=25, train_loss=0.05991183966398239\n\n\n29\ncoverage: 0.918\n\n\n[Logger-12256] epoch=30, train_loss=0.0578494668006897\n[Logger-12256] epoch=35, train_loss=0.05833262577652931\n[Logger-12256] epoch=40, train_loss=0.055732518434524536\n[Logger-12256] epoch=45, train_loss=0.05142706632614136\n[Logger-12256] epoch=50, train_loss=0.04729042947292328\n[Logger-12256] epoch=55, train_loss=0.04521041363477707\n[Logger-12256] epoch=60, train_loss=0.044819965958595276\n[Logger-12256] epoch=65, train_loss=0.04296273738145828\n[Logger-12256] epoch=70, train_loss=0.04275815933942795\n[Logger-12256] epoch=75, train_loss=0.04176012799143791\n[Logger-12256] epoch=80, train_loss=0.04141638055443764\n[Logger-12256] epoch=85, train_loss=0.040825989097356796\n[Logger-12256] epoch=90, train_loss=0.04022158682346344\n[Logger-12256] epoch=95, train_loss=0.039785295724868774\n[Logger-12256] epoch=0, train_loss=0.4163255989551544\n[Logger-12256] epoch=5, train_loss=0.29757753014564514\n[Logger-12256] epoch=10, train_loss=0.20676682889461517\n[Logger-12256] epoch=15, train_loss=0.1400613784790039\n[Logger-12256] epoch=20, train_loss=0.09245586395263672\n[Logger-12256] epoch=25, train_loss=0.0652284175157547\n\n\n30\ncoverage: 0.946\n\n\n[Logger-12256] epoch=30, train_loss=0.061260856688022614\n[Logger-12256] epoch=35, train_loss=0.06362334638834\n[Logger-12256] epoch=40, train_loss=0.06293760240077972\n[Logger-12256] epoch=45, train_loss=0.060412004590034485\n[Logger-12256] epoch=50, train_loss=0.05688117444515228\n[Logger-12256] epoch=55, train_loss=0.05324835330247879\n[Logger-12256] epoch=60, train_loss=0.05030644312500954\n[Logger-12256] epoch=65, train_loss=0.04859648272395134\n[Logger-12256] epoch=70, train_loss=0.047132883220911026\n[Logger-12256] epoch=75, train_loss=0.0451105535030365\n[Logger-12256] epoch=80, train_loss=0.044076066464185715\n[Logger-12256] epoch=85, train_loss=0.04292862117290497\n[Logger-12256] epoch=90, train_loss=0.04186475649476051\n[Logger-12256] epoch=95, train_loss=0.0405043326318264\n[Logger-12256] epoch=0, train_loss=0.5605207681655884\n[Logger-12256] epoch=5, train_loss=0.407916396856308\n[Logger-12256] epoch=10, train_loss=0.28983038663864136\n[Logger-12256] epoch=15, train_loss=0.20084767043590546\n\n\n31\ncoverage: 0.93\n\n\n[Logger-12256] epoch=20, train_loss=0.13271939754486084\n[Logger-12256] epoch=25, train_loss=0.08436942100524902\n[Logger-12256] epoch=30, train_loss=0.054511819034814835\n[Logger-12256] epoch=35, train_loss=0.04835306853055954\n[Logger-12256] epoch=40, train_loss=0.05055181309580803\n[Logger-12256] epoch=45, train_loss=0.04975785315036774\n[Logger-12256] epoch=50, train_loss=0.04738212749361992\n[Logger-12256] epoch=55, train_loss=0.04520946741104126\n[Logger-12256] epoch=60, train_loss=0.04284415394067764\n[Logger-12256] epoch=65, train_loss=0.04036499932408333\n[Logger-12256] epoch=70, train_loss=0.03982064127922058\n[Logger-12256] epoch=75, train_loss=0.03914721682667732\n[Logger-12256] epoch=80, train_loss=0.038599640130996704\n[Logger-12256] epoch=85, train_loss=0.038287267088890076\n[Logger-12256] epoch=90, train_loss=0.03774430975317955\n[Logger-12256] epoch=95, train_loss=0.03731302171945572\n[Logger-12256] epoch=0, train_loss=0.48266053199768066\n[Logger-12256] epoch=5, train_loss=0.3359563648700714\n[Logger-12256] epoch=10, train_loss=0.21873047947883606\n[Logger-12256] epoch=15, train_loss=0.12866440415382385\n[Logger-12256] epoch=20, train_loss=0.0692535787820816\n\n\n32\ncoverage: 0.91\n\n\n[Logger-12256] epoch=25, train_loss=0.044892817735672\n[Logger-12256] epoch=30, train_loss=0.047247447073459625\n[Logger-12256] epoch=35, train_loss=0.04845660552382469\n[Logger-12256] epoch=40, train_loss=0.047482799738645554\n[Logger-12256] epoch=45, train_loss=0.04546396806836128\n[Logger-12256] epoch=50, train_loss=0.04320141673088074\n[Logger-12256] epoch=55, train_loss=0.04080215469002724\n[Logger-12256] epoch=60, train_loss=0.039099451154470444\n[Logger-12256] epoch=65, train_loss=0.037386637181043625\n[Logger-12256] epoch=70, train_loss=0.03682669252157211\n[Logger-12256] epoch=75, train_loss=0.03578102961182594\n[Logger-12256] epoch=80, train_loss=0.03523728996515274\n[Logger-12256] epoch=85, train_loss=0.03455526754260063\n[Logger-12256] epoch=90, train_loss=0.0338481068611145\n[Logger-12256] epoch=95, train_loss=0.03304889425635338\n[Logger-12256] epoch=0, train_loss=0.4128704369068146\n[Logger-12256] epoch=5, train_loss=0.2686435878276825\n[Logger-12256] epoch=10, train_loss=0.16472181677818298\n[Logger-12256] epoch=15, train_loss=0.09434118866920471\n\n\n33\ncoverage: 0.882\n\n\n[Logger-12256] epoch=20, train_loss=0.05165843665599823\n[Logger-12256] epoch=25, train_loss=0.04761239141225815\n[Logger-12256] epoch=30, train_loss=0.04993284121155739\n[Logger-12256] epoch=35, train_loss=0.05026773363351822\n[Logger-12256] epoch=40, train_loss=0.049341969192028046\n[Logger-12256] epoch=45, train_loss=0.047623954713344574\n[Logger-12256] epoch=50, train_loss=0.04507865756750107\n[Logger-12256] epoch=55, train_loss=0.04188348352909088\n[Logger-12256] epoch=60, train_loss=0.041237980127334595\n[Logger-12256] epoch=65, train_loss=0.040251996368169785\n[Logger-12256] epoch=70, train_loss=0.03951583057641983\n[Logger-12256] epoch=75, train_loss=0.0388307049870491\n[Logger-12256] epoch=80, train_loss=0.03815588355064392\n[Logger-12256] epoch=85, train_loss=0.03746950253844261\n[Logger-12256] epoch=90, train_loss=0.03672385588288307\n[Logger-12256] epoch=95, train_loss=0.036049045622348785\n[Logger-12256] epoch=0, train_loss=0.4341554641723633\n[Logger-12256] epoch=5, train_loss=0.30144912004470825\n[Logger-12256] epoch=10, train_loss=0.20140153169631958\n[Logger-12256] epoch=15, train_loss=0.1350809633731842\n\n\n34\ncoverage: 0.864\n\n\n[Logger-12256] epoch=20, train_loss=0.09621797502040863\n[Logger-12256] epoch=25, train_loss=0.07419459521770477\n[Logger-12256] epoch=30, train_loss=0.061029527336359024\n[Logger-12256] epoch=35, train_loss=0.06026267260313034\n[Logger-12256] epoch=40, train_loss=0.058991607278585434\n[Logger-12256] epoch=45, train_loss=0.055517300963401794\n[Logger-12256] epoch=50, train_loss=0.05100903660058975\n[Logger-12256] epoch=55, train_loss=0.04774206876754761\n[Logger-12256] epoch=60, train_loss=0.04469512030482292\n[Logger-12256] epoch=65, train_loss=0.04407878220081329\n[Logger-12256] epoch=70, train_loss=0.04221651330590248\n[Logger-12256] epoch=75, train_loss=0.04083305969834328\n[Logger-12256] epoch=80, train_loss=0.03996530547738075\n[Logger-12256] epoch=85, train_loss=0.039545271545648575\n[Logger-12256] epoch=90, train_loss=0.038799066096544266\n[Logger-12256] epoch=95, train_loss=0.038303568959236145\n[Logger-12256] epoch=0, train_loss=0.46219557523727417\n[Logger-12256] epoch=5, train_loss=0.3359295129776001\n[Logger-12256] epoch=10, train_loss=0.2367042601108551\n[Logger-12256] epoch=15, train_loss=0.15498653054237366\n[Logger-12256] epoch=20, train_loss=0.09341361373662949\n\n\n35\ncoverage: 0.892\n\n\n[Logger-12256] epoch=25, train_loss=0.07087967544794083\n[Logger-12256] epoch=30, train_loss=0.06356092542409897\n[Logger-12256] epoch=35, train_loss=0.06507740169763565\n[Logger-12256] epoch=40, train_loss=0.06281492859125137\n[Logger-12256] epoch=45, train_loss=0.058270275592803955\n[Logger-12256] epoch=50, train_loss=0.05246330425143242\n[Logger-12256] epoch=55, train_loss=0.04867670685052872\n[Logger-12256] epoch=60, train_loss=0.043944794684648514\n[Logger-12256] epoch=65, train_loss=0.043043412268161774\n[Logger-12256] epoch=70, train_loss=0.0411578007042408\n[Logger-12256] epoch=75, train_loss=0.0411389097571373\n[Logger-12256] epoch=80, train_loss=0.040116969496011734\n[Logger-12256] epoch=85, train_loss=0.03960525617003441\n[Logger-12256] epoch=90, train_loss=0.039155855774879456\n[Logger-12256] epoch=95, train_loss=0.038669995963573456\n[Logger-12256] epoch=0, train_loss=0.4535021483898163\n[Logger-12256] epoch=5, train_loss=0.32564231753349304\n[Logger-12256] epoch=10, train_loss=0.22211068868637085\n[Logger-12256] epoch=15, train_loss=0.14309608936309814\n[Logger-12256] epoch=20, train_loss=0.09038134664297104\n\n\n36\ncoverage: 0.894\n\n\n[Logger-12256] epoch=25, train_loss=0.05929521098732948\n[Logger-12256] epoch=30, train_loss=0.05024760216474533\n[Logger-12256] epoch=35, train_loss=0.051435891538858414\n[Logger-12256] epoch=40, train_loss=0.05005798488855362\n[Logger-12256] epoch=45, train_loss=0.04779885336756706\n[Logger-12256] epoch=50, train_loss=0.04579741880297661\n[Logger-12256] epoch=55, train_loss=0.04376466944813728\n[Logger-12256] epoch=60, train_loss=0.04224323481321335\n[Logger-12256] epoch=65, train_loss=0.041386913508176804\n[Logger-12256] epoch=70, train_loss=0.040597304701805115\n[Logger-12256] epoch=75, train_loss=0.040112800896167755\n[Logger-12256] epoch=80, train_loss=0.03971917927265167\n[Logger-12256] epoch=85, train_loss=0.039326462894678116\n[Logger-12256] epoch=90, train_loss=0.038779716938734055\n[Logger-12256] epoch=95, train_loss=0.03831073269248009\n[Logger-12256] epoch=0, train_loss=0.39786648750305176\n[Logger-12256] epoch=5, train_loss=0.28480806946754456\n[Logger-12256] epoch=10, train_loss=0.19432097673416138\n[Logger-12256] epoch=15, train_loss=0.12379878759384155\n[Logger-12256] epoch=20, train_loss=0.07374518364667892\n[Logger-12256] epoch=25, train_loss=0.055708061903715134\n\n\n37\ncoverage: 0.89\n\n\n[Logger-12256] epoch=30, train_loss=0.060159832239151\n[Logger-12256] epoch=35, train_loss=0.05985484644770622\n[Logger-12256] epoch=40, train_loss=0.05636192485690117\n[Logger-12256] epoch=45, train_loss=0.05080397054553032\n[Logger-12256] epoch=50, train_loss=0.04399807006120682\n[Logger-12256] epoch=55, train_loss=0.045082975178956985\n[Logger-12256] epoch=60, train_loss=0.04243215546011925\n[Logger-12256] epoch=65, train_loss=0.04248056560754776\n[Logger-12256] epoch=70, train_loss=0.04134345054626465\n[Logger-12256] epoch=75, train_loss=0.04102218896150589\n[Logger-12256] epoch=80, train_loss=0.04034293442964554\n[Logger-12256] epoch=85, train_loss=0.03983395919203758\n[Logger-12256] epoch=90, train_loss=0.03927944228053093\n[Logger-12256] epoch=95, train_loss=0.038732342422008514\n[Logger-12256] epoch=0, train_loss=0.424286425113678\n[Logger-12256] epoch=5, train_loss=0.2862280607223511\n[Logger-12256] epoch=10, train_loss=0.18107062578201294\n[Logger-12256] epoch=15, train_loss=0.1101471334695816\n[Logger-12256] epoch=20, train_loss=0.06712749600410461\n[Logger-12256] epoch=25, train_loss=0.04853905737400055\n\n\n38\ncoverage: 0.922\n\n\n[Logger-12256] epoch=30, train_loss=0.051041726022958755\n[Logger-12256] epoch=35, train_loss=0.05156157165765762\n[Logger-12256] epoch=40, train_loss=0.05005109682679176\n[Logger-12256] epoch=45, train_loss=0.04721281677484512\n[Logger-12256] epoch=50, train_loss=0.04439806938171387\n[Logger-12256] epoch=55, train_loss=0.042937271296978\n[Logger-12256] epoch=60, train_loss=0.04142998531460762\n[Logger-12256] epoch=65, train_loss=0.03949947655200958\n[Logger-12256] epoch=70, train_loss=0.03885699808597565\n[Logger-12256] epoch=75, train_loss=0.03845925256609917\n[Logger-12256] epoch=80, train_loss=0.03776385635137558\n[Logger-12256] epoch=85, train_loss=0.03706606477499008\n[Logger-12256] epoch=90, train_loss=0.036586418747901917\n[Logger-12256] epoch=95, train_loss=0.035986945033073425\n[Logger-12256] epoch=0, train_loss=0.45807868242263794\n[Logger-12256] epoch=5, train_loss=0.3142879903316498\n[Logger-12256] epoch=10, train_loss=0.2057667076587677\n[Logger-12256] epoch=15, train_loss=0.12287097424268723\n[Logger-12256] epoch=20, train_loss=0.0666477158665657\n\n\n39\ncoverage: 0.882\n\n\n[Logger-12256] epoch=25, train_loss=0.049037858843803406\n[Logger-12256] epoch=30, train_loss=0.05252290144562721\n[Logger-12256] epoch=35, train_loss=0.05229317024350166\n[Logger-12256] epoch=40, train_loss=0.05065528303384781\n[Logger-12256] epoch=45, train_loss=0.049224622547626495\n[Logger-12256] epoch=50, train_loss=0.04762019217014313\n[Logger-12256] epoch=55, train_loss=0.045062676072120667\n[Logger-12256] epoch=60, train_loss=0.042602814733982086\n[Logger-12256] epoch=65, train_loss=0.04121869057416916\n[Logger-12256] epoch=70, train_loss=0.04043807089328766\n[Logger-12256] epoch=75, train_loss=0.0396655835211277\n[Logger-12256] epoch=80, train_loss=0.039354365319013596\n[Logger-12256] epoch=85, train_loss=0.03866303339600563\n[Logger-12256] epoch=90, train_loss=0.038195811212062836\n[Logger-12256] epoch=95, train_loss=0.0376126654446125\n[Logger-12256] epoch=0, train_loss=0.4069259762763977\n[Logger-12256] epoch=5, train_loss=0.29745808243751526\n\n\n40\ncoverage: 0.902\n\n\n[Logger-12256] epoch=10, train_loss=0.2067611962556839\n[Logger-12256] epoch=15, train_loss=0.13442233204841614\n[Logger-12256] epoch=20, train_loss=0.08044206351041794\n[Logger-12256] epoch=25, train_loss=0.055156175047159195\n[Logger-12256] epoch=30, train_loss=0.05172087997198105\n[Logger-12256] epoch=35, train_loss=0.052716661244630814\n[Logger-12256] epoch=40, train_loss=0.05167519673705101\n[Logger-12256] epoch=45, train_loss=0.04969128593802452\n[Logger-12256] epoch=50, train_loss=0.046591561287641525\n[Logger-12256] epoch=55, train_loss=0.04399402067065239\n[Logger-12256] epoch=60, train_loss=0.041921649128198624\n[Logger-12256] epoch=65, train_loss=0.04126708582043648\n[Logger-12256] epoch=70, train_loss=0.039966654032468796\n[Logger-12256] epoch=75, train_loss=0.039501842111349106\n[Logger-12256] epoch=80, train_loss=0.03884167969226837\n[Logger-12256] epoch=85, train_loss=0.03823719546198845\n[Logger-12256] epoch=90, train_loss=0.03753641992807388\n[Logger-12256] epoch=95, train_loss=0.03694074600934982\n[Logger-12256] epoch=0, train_loss=0.37040483951568604\n[Logger-12256] epoch=5, train_loss=0.26020899415016174\n[Logger-12256] epoch=10, train_loss=0.1694745570421219\n[Logger-12256] epoch=15, train_loss=0.10398110747337341\n[Logger-12256] epoch=20, train_loss=0.06349431723356247\n[Logger-12256] epoch=25, train_loss=0.0516061969101429\n\n\n41\ncoverage: 0.898\n\n\n[Logger-12256] epoch=30, train_loss=0.05339396744966507\n[Logger-12256] epoch=35, train_loss=0.05237196758389473\n[Logger-12256] epoch=40, train_loss=0.04901358485221863\n[Logger-12256] epoch=45, train_loss=0.04626164585351944\n[Logger-12256] epoch=50, train_loss=0.044901419430971146\n[Logger-12256] epoch=55, train_loss=0.04276397079229355\n[Logger-12256] epoch=60, train_loss=0.04161407798528671\n[Logger-12256] epoch=65, train_loss=0.041031189262866974\n[Logger-12256] epoch=70, train_loss=0.04045227915048599\n[Logger-12256] epoch=75, train_loss=0.03984275832772255\n[Logger-12256] epoch=80, train_loss=0.03934391960501671\n[Logger-12256] epoch=85, train_loss=0.038912151008844376\n[Logger-12256] epoch=90, train_loss=0.03838249295949936\n[Logger-12256] epoch=95, train_loss=0.0378950871527195\n[Logger-12256] epoch=0, train_loss=0.4048160910606384\n[Logger-12256] epoch=5, train_loss=0.29117995500564575\n[Logger-12256] epoch=10, train_loss=0.1980391889810562\n[Logger-12256] epoch=15, train_loss=0.12497714906930923\n[Logger-12256] epoch=20, train_loss=0.07225248962640762\n[Logger-12256] epoch=25, train_loss=0.054250139743089676\n[Logger-12256] epoch=30, train_loss=0.057036515325307846\n\n\n42\ncoverage: 0.896\n\n\n[Logger-12256] epoch=35, train_loss=0.057348862290382385\n[Logger-12256] epoch=40, train_loss=0.055115003138780594\n[Logger-12256] epoch=45, train_loss=0.051469799131155014\n[Logger-12256] epoch=50, train_loss=0.04838915169239044\n[Logger-12256] epoch=55, train_loss=0.0444226935505867\n[Logger-12256] epoch=60, train_loss=0.04142390564084053\n[Logger-12256] epoch=65, train_loss=0.04098057001829147\n[Logger-12256] epoch=70, train_loss=0.04024846851825714\n[Logger-12256] epoch=75, train_loss=0.03954317048192024\n[Logger-12256] epoch=80, train_loss=0.03896180912852287\n[Logger-12256] epoch=85, train_loss=0.03827822953462601\n[Logger-12256] epoch=90, train_loss=0.03771333768963814\n[Logger-12256] epoch=95, train_loss=0.03707174211740494\n[Logger-12256] epoch=0, train_loss=0.39976173639297485\n[Logger-12256] epoch=5, train_loss=0.2743483781814575\n[Logger-12256] epoch=10, train_loss=0.17375758290290833\n[Logger-12256] epoch=15, train_loss=0.09932761639356613\n[Logger-12256] epoch=20, train_loss=0.05753388628363609\n[Logger-12256] epoch=25, train_loss=0.04992127791047096\n\n\n43\ncoverage: 0.914\n\n\n[Logger-12256] epoch=30, train_loss=0.051717035472393036\n[Logger-12256] epoch=35, train_loss=0.050375331193208694\n[Logger-12256] epoch=40, train_loss=0.0494130440056324\n[Logger-12256] epoch=45, train_loss=0.04773441329598427\n[Logger-12256] epoch=50, train_loss=0.04509720206260681\n[Logger-12256] epoch=55, train_loss=0.043421871960163116\n[Logger-12256] epoch=60, train_loss=0.04113190621137619\n[Logger-12256] epoch=65, train_loss=0.04103397950530052\n[Logger-12256] epoch=70, train_loss=0.039799414575099945\n[Logger-12256] epoch=75, train_loss=0.03940236195921898\n[Logger-12256] epoch=80, train_loss=0.038623712956905365\n[Logger-12256] epoch=85, train_loss=0.038000740110874176\n[Logger-12256] epoch=90, train_loss=0.03740451857447624\n[Logger-12256] epoch=95, train_loss=0.036762699484825134\n[Logger-12256] epoch=0, train_loss=0.4739547371864319\n[Logger-12256] epoch=5, train_loss=0.3372320234775543\n[Logger-12256] epoch=10, train_loss=0.23119792342185974\n[Logger-12256] epoch=15, train_loss=0.15141591429710388\n[Logger-12256] epoch=20, train_loss=0.09394339472055435\n\n\n44\ncoverage: 0.93\n\n\n[Logger-12256] epoch=25, train_loss=0.06386154890060425\n[Logger-12256] epoch=30, train_loss=0.053745005279779434\n[Logger-12256] epoch=35, train_loss=0.05351453647017479\n[Logger-12256] epoch=40, train_loss=0.05265374109148979\n[Logger-12256] epoch=45, train_loss=0.050316233187913895\n[Logger-12256] epoch=50, train_loss=0.047953344881534576\n[Logger-12256] epoch=55, train_loss=0.045648492872714996\n[Logger-12256] epoch=60, train_loss=0.04350399598479271\n[Logger-12256] epoch=65, train_loss=0.04321334883570671\n[Logger-12256] epoch=70, train_loss=0.04214460402727127\n[Logger-12256] epoch=75, train_loss=0.04110570251941681\n[Logger-12256] epoch=80, train_loss=0.04077135771512985\n[Logger-12256] epoch=85, train_loss=0.04007309302687645\n[Logger-12256] epoch=90, train_loss=0.03953394293785095\n[Logger-12256] epoch=95, train_loss=0.03895082324743271\n[Logger-12256] epoch=0, train_loss=0.45509830117225647\n[Logger-12256] epoch=5, train_loss=0.3418348729610443\n[Logger-12256] epoch=10, train_loss=0.24682572484016418\n[Logger-12256] epoch=15, train_loss=0.16669093072414398\n[Logger-12256] epoch=20, train_loss=0.10106020420789719\n[Logger-12256] epoch=25, train_loss=0.05566415563225746\n[Logger-12256] epoch=30, train_loss=0.045862458646297455\n\n\n45\ncoverage: 0.906\n\n\n[Logger-12256] epoch=35, train_loss=0.048326823860406876\n[Logger-12256] epoch=40, train_loss=0.04884148761630058\n[Logger-12256] epoch=45, train_loss=0.04842355474829674\n[Logger-12256] epoch=50, train_loss=0.046549297869205475\n[Logger-12256] epoch=55, train_loss=0.04401872307062149\n[Logger-12256] epoch=60, train_loss=0.04107113927602768\n[Logger-12256] epoch=65, train_loss=0.0397535003721714\n[Logger-12256] epoch=70, train_loss=0.03938847407698631\n[Logger-12256] epoch=75, train_loss=0.03851288929581642\n[Logger-12256] epoch=80, train_loss=0.03775225207209587\n[Logger-12256] epoch=85, train_loss=0.037398964166641235\n[Logger-12256] epoch=90, train_loss=0.03680257499217987\n[Logger-12256] epoch=95, train_loss=0.03613078221678734\n[Logger-12256] epoch=0, train_loss=0.3780863881111145\n[Logger-12256] epoch=5, train_loss=0.2562567889690399\n[Logger-12256] epoch=10, train_loss=0.15879841148853302\n[Logger-12256] epoch=15, train_loss=0.08860857039690018\n[Logger-12256] epoch=20, train_loss=0.05350823327898979\n[Logger-12256] epoch=25, train_loss=0.05494755133986473\n\n\n46\ncoverage: 0.884\n\n\n[Logger-12256] epoch=30, train_loss=0.05595312640070915\n[Logger-12256] epoch=35, train_loss=0.05368053540587425\n[Logger-12256] epoch=40, train_loss=0.05004141479730606\n[Logger-12256] epoch=45, train_loss=0.04564782977104187\n[Logger-12256] epoch=50, train_loss=0.042966146022081375\n[Logger-12256] epoch=55, train_loss=0.04192483052611351\n[Logger-12256] epoch=60, train_loss=0.04049890488386154\n[Logger-12256] epoch=65, train_loss=0.03999772667884827\n[Logger-12256] epoch=70, train_loss=0.03927617147564888\n[Logger-12256] epoch=75, train_loss=0.03877684846520424\n[Logger-12256] epoch=80, train_loss=0.03801988437771797\n[Logger-12256] epoch=85, train_loss=0.03742591291666031\n[Logger-12256] epoch=90, train_loss=0.03682733327150345\n[Logger-12256] epoch=95, train_loss=0.036184992641210556\n[Logger-12256] epoch=0, train_loss=0.4569464325904846\n[Logger-12256] epoch=5, train_loss=0.3326725959777832\n[Logger-12256] epoch=10, train_loss=0.23731981217861176\n[Logger-12256] epoch=15, train_loss=0.16684158146381378\n[Logger-12256] epoch=20, train_loss=0.1090986430644989\n\n\n47\ncoverage: 0.914\n\n\n[Logger-12256] epoch=25, train_loss=0.06921326369047165\n[Logger-12256] epoch=30, train_loss=0.05262860655784607\n[Logger-12256] epoch=35, train_loss=0.046400584280490875\n[Logger-12256] epoch=40, train_loss=0.047370441257953644\n[Logger-12256] epoch=45, train_loss=0.046862512826919556\n[Logger-12256] epoch=50, train_loss=0.04535696655511856\n[Logger-12256] epoch=55, train_loss=0.04371820390224457\n[Logger-12256] epoch=60, train_loss=0.04151376336812973\n[Logger-12256] epoch=65, train_loss=0.04014231637120247\n[Logger-12256] epoch=70, train_loss=0.03947426751255989\n[Logger-12256] epoch=75, train_loss=0.03866168111562729\n[Logger-12256] epoch=80, train_loss=0.037928689271211624\n[Logger-12256] epoch=85, train_loss=0.03731110319495201\n[Logger-12256] epoch=90, train_loss=0.036703936755657196\n[Logger-12256] epoch=95, train_loss=0.03600554168224335\n[Logger-12256] epoch=0, train_loss=0.47324737906455994\n[Logger-12256] epoch=5, train_loss=0.35594847798347473\n[Logger-12256] epoch=10, train_loss=0.2653234302997589\n[Logger-12256] epoch=15, train_loss=0.1922398805618286\n[Logger-12256] epoch=20, train_loss=0.13270197808742523\n[Logger-12256] epoch=25, train_loss=0.08867158740758896\n\n\n48\ncoverage: 0.862\n\n\n[Logger-12256] epoch=30, train_loss=0.05894573777914047\n[Logger-12256] epoch=35, train_loss=0.045396801084280014\n[Logger-12256] epoch=40, train_loss=0.0462888740003109\n[Logger-12256] epoch=45, train_loss=0.04654134809970856\n[Logger-12256] epoch=50, train_loss=0.04650465026497841\n[Logger-12256] epoch=55, train_loss=0.045657169073820114\n[Logger-12256] epoch=60, train_loss=0.04416607692837715\n[Logger-12256] epoch=65, train_loss=0.04258368909358978\n[Logger-12256] epoch=70, train_loss=0.04132474586367607\n[Logger-12256] epoch=75, train_loss=0.039890557527542114\n[Logger-12256] epoch=80, train_loss=0.03966968134045601\n[Logger-12256] epoch=85, train_loss=0.039003677666187286\n[Logger-12256] epoch=90, train_loss=0.03831152245402336\n[Logger-12256] epoch=95, train_loss=0.03785813972353935\n[Logger-12256] epoch=0, train_loss=0.42925843596458435\n[Logger-12256] epoch=5, train_loss=0.2936103045940399\n[Logger-12256] epoch=10, train_loss=0.18646381795406342\n[Logger-12256] epoch=15, train_loss=0.10773541033267975\n[Logger-12256] epoch=20, train_loss=0.05924220383167267\n\n\n49\ncoverage: 0.922\n\n\n[Logger-12256] epoch=25, train_loss=0.04956156015396118\n[Logger-12256] epoch=30, train_loss=0.05219339206814766\n[Logger-12256] epoch=35, train_loss=0.05139360949397087\n[Logger-12256] epoch=40, train_loss=0.049777690321207047\n[Logger-12256] epoch=45, train_loss=0.0475371815264225\n[Logger-12256] epoch=50, train_loss=0.04430624470114708\n[Logger-12256] epoch=55, train_loss=0.041423190385103226\n[Logger-12256] epoch=60, train_loss=0.04047514870762825\n[Logger-12256] epoch=65, train_loss=0.03991711512207985\n[Logger-12256] epoch=70, train_loss=0.039283424615859985\n[Logger-12256] epoch=75, train_loss=0.038560349494218826\n[Logger-12256] epoch=80, train_loss=0.037986524403095245\n[Logger-12256] epoch=85, train_loss=0.03750632703304291\n[Logger-12256] epoch=90, train_loss=0.03691389784216881\n[Logger-12256] epoch=95, train_loss=0.03640518710017204\n[Logger-12256] epoch=0, train_loss=0.45604366064071655\n[Logger-12256] epoch=5, train_loss=0.30904287099838257\n[Logger-12256] epoch=10, train_loss=0.20330040156841278\n[Logger-12256] epoch=15, train_loss=0.12531904876232147\n[Logger-12256] epoch=20, train_loss=0.08465149253606796\n[Logger-12256] epoch=25, train_loss=0.06416034698486328\n\n\n50\ncoverage: 0.912\n\n\n[Logger-12256] epoch=30, train_loss=0.05496210232377052\n[Logger-12256] epoch=35, train_loss=0.054349567741155624\n[Logger-12256] epoch=40, train_loss=0.05079517140984535\n[Logger-12256] epoch=45, train_loss=0.04823749139904976\n[Logger-12256] epoch=50, train_loss=0.04408450797200203\n[Logger-12256] epoch=55, train_loss=0.04259149730205536\n[Logger-12256] epoch=60, train_loss=0.041128747165203094\n[Logger-12256] epoch=65, train_loss=0.0401734784245491\n[Logger-12256] epoch=70, train_loss=0.03974151238799095\n[Logger-12256] epoch=75, train_loss=0.03932473435997963\n[Logger-12256] epoch=80, train_loss=0.03859071806073189\n[Logger-12256] epoch=85, train_loss=0.03813370317220688\n[Logger-12256] epoch=90, train_loss=0.037659335881471634\n[Logger-12256] epoch=95, train_loss=0.03715527430176735\n[Logger-12256] epoch=0, train_loss=0.41848573088645935\n[Logger-12256] epoch=5, train_loss=0.31470707058906555\n[Logger-12256] epoch=10, train_loss=0.2313094288110733\n[Logger-12256] epoch=15, train_loss=0.15872983634471893\n[Logger-12256] epoch=20, train_loss=0.09628787636756897\n\n\n51\ncoverage: 0.932\n\n\n[Logger-12256] epoch=25, train_loss=0.055473823100328445\n[Logger-12256] epoch=30, train_loss=0.04867621138691902\n[Logger-12256] epoch=35, train_loss=0.0517195425927639\n[Logger-12256] epoch=40, train_loss=0.05186101049184799\n[Logger-12256] epoch=45, train_loss=0.05009057745337486\n[Logger-12256] epoch=50, train_loss=0.04717395827174187\n[Logger-12256] epoch=55, train_loss=0.04399849846959114\n[Logger-12256] epoch=60, train_loss=0.041906703263521194\n[Logger-12256] epoch=65, train_loss=0.03979015722870827\n[Logger-12256] epoch=70, train_loss=0.0390663705766201\n[Logger-12256] epoch=75, train_loss=0.03762292116880417\n[Logger-12256] epoch=80, train_loss=0.036998506635427475\n[Logger-12256] epoch=85, train_loss=0.036081429570913315\n[Logger-12256] epoch=90, train_loss=0.03514929488301277\n[Logger-12256] epoch=95, train_loss=0.0342455692589283\n[Logger-12256] epoch=0, train_loss=0.4041976034641266\n[Logger-12256] epoch=5, train_loss=0.2910078763961792\n[Logger-12256] epoch=10, train_loss=0.19898651540279388\n[Logger-12256] epoch=15, train_loss=0.1263432651758194\n[Logger-12256] epoch=20, train_loss=0.07719030231237411\n[Logger-12256] epoch=25, train_loss=0.05570593476295471\n[Logger-12256] epoch=30, train_loss=0.053848277777433395\n\n\n52\ncoverage: 0.9\n\n\n[Logger-12256] epoch=35, train_loss=0.054213862866163254\n[Logger-12256] epoch=40, train_loss=0.05159416422247887\n[Logger-12256] epoch=45, train_loss=0.04738185927271843\n[Logger-12256] epoch=50, train_loss=0.04421602934598923\n[Logger-12256] epoch=55, train_loss=0.042752549052238464\n[Logger-12256] epoch=60, train_loss=0.04198131337761879\n[Logger-12256] epoch=65, train_loss=0.04071774333715439\n[Logger-12256] epoch=70, train_loss=0.040309615433216095\n[Logger-12256] epoch=75, train_loss=0.03956396505236626\n[Logger-12256] epoch=80, train_loss=0.039054255932569504\n[Logger-12256] epoch=85, train_loss=0.03839137777686119\n[Logger-12256] epoch=90, train_loss=0.037755727767944336\n[Logger-12256] epoch=95, train_loss=0.03706911578774452\n[Logger-12256] epoch=0, train_loss=0.4780927896499634\n[Logger-12256] epoch=5, train_loss=0.3373009264469147\n[Logger-12256] epoch=10, train_loss=0.2300768345594406\n[Logger-12256] epoch=15, train_loss=0.14689533412456512\n[Logger-12256] epoch=20, train_loss=0.08348353952169418\n[Logger-12256] epoch=25, train_loss=0.05230928957462311\n\n\n53\ncoverage: 0.848\n\n\n[Logger-12256] epoch=30, train_loss=0.052181728184223175\n[Logger-12256] epoch=35, train_loss=0.05345429480075836\n[Logger-12256] epoch=40, train_loss=0.05192122608423233\n[Logger-12256] epoch=45, train_loss=0.050086088478565216\n[Logger-12256] epoch=50, train_loss=0.04734475910663605\n[Logger-12256] epoch=55, train_loss=0.04352559149265289\n[Logger-12256] epoch=60, train_loss=0.04171879589557648\n[Logger-12256] epoch=65, train_loss=0.04158252850174904\n[Logger-12256] epoch=70, train_loss=0.0402282290160656\n[Logger-12256] epoch=75, train_loss=0.039908021688461304\n[Logger-12256] epoch=80, train_loss=0.03931669145822525\n[Logger-12256] epoch=85, train_loss=0.03902581334114075\n[Logger-12256] epoch=90, train_loss=0.0385185107588768\n[Logger-12256] epoch=95, train_loss=0.038113072514534\n[Logger-12256] epoch=0, train_loss=0.4406116306781769\n[Logger-12256] epoch=5, train_loss=0.3059099614620209\n[Logger-12256] epoch=10, train_loss=0.20205160975456238\n[Logger-12256] epoch=15, train_loss=0.12621894478797913\n[Logger-12256] epoch=20, train_loss=0.07191958278417587\n\n\n54\ncoverage: 0.912\n\n\n[Logger-12256] epoch=25, train_loss=0.05415906757116318\n[Logger-12256] epoch=30, train_loss=0.05794927105307579\n[Logger-12256] epoch=35, train_loss=0.05747371166944504\n[Logger-12256] epoch=40, train_loss=0.05421057343482971\n[Logger-12256] epoch=45, train_loss=0.049289271235466\n[Logger-12256] epoch=50, train_loss=0.04531161114573479\n[Logger-12256] epoch=55, train_loss=0.042745012789964676\n[Logger-12256] epoch=60, train_loss=0.042566072195768356\n[Logger-12256] epoch=65, train_loss=0.040863968431949615\n[Logger-12256] epoch=70, train_loss=0.04041582718491554\n[Logger-12256] epoch=75, train_loss=0.039566148072481155\n[Logger-12256] epoch=80, train_loss=0.03887057676911354\n[Logger-12256] epoch=85, train_loss=0.038100115954875946\n[Logger-12256] epoch=90, train_loss=0.03738904744386673\n[Logger-12256] epoch=95, train_loss=0.03674818575382233\n[Logger-12256] epoch=0, train_loss=0.4553988575935364\n[Logger-12256] epoch=5, train_loss=0.3307177424430847\n[Logger-12256] epoch=10, train_loss=0.2282867431640625\n[Logger-12256] epoch=15, train_loss=0.14579778909683228\n[Logger-12256] epoch=20, train_loss=0.08628115803003311\n[Logger-12256] epoch=25, train_loss=0.060542453080415726\n\n\n55\ncoverage: 0.85\n\n\n[Logger-12256] epoch=30, train_loss=0.05806289240717888\n[Logger-12256] epoch=35, train_loss=0.0589451789855957\n[Logger-12256] epoch=40, train_loss=0.05652983859181404\n[Logger-12256] epoch=45, train_loss=0.0520511269569397\n[Logger-12256] epoch=50, train_loss=0.04637758806347847\n[Logger-12256] epoch=55, train_loss=0.043024200946092606\n[Logger-12256] epoch=60, train_loss=0.041910648345947266\n[Logger-12256] epoch=65, train_loss=0.04058706760406494\n[Logger-12256] epoch=70, train_loss=0.04019445925951004\n[Logger-12256] epoch=75, train_loss=0.03921826183795929\n[Logger-12256] epoch=80, train_loss=0.03864550217986107\n[Logger-12256] epoch=85, train_loss=0.038076482713222504\n[Logger-12256] epoch=90, train_loss=0.037515558302402496\n[Logger-12256] epoch=95, train_loss=0.037022124975919724\n[Logger-12256] epoch=0, train_loss=0.5669509172439575\n[Logger-12256] epoch=5, train_loss=0.42107322812080383\n[Logger-12256] epoch=10, train_loss=0.3084779977798462\n[Logger-12256] epoch=15, train_loss=0.21643048524856567\n[Logger-12256] epoch=20, train_loss=0.14094790816307068\n[Logger-12256] epoch=25, train_loss=0.08353117108345032\n\n\n56\ncoverage: 0.928\n\n\n[Logger-12256] epoch=30, train_loss=0.049722373485565186\n[Logger-12256] epoch=35, train_loss=0.044895946979522705\n[Logger-12256] epoch=40, train_loss=0.04685523733496666\n[Logger-12256] epoch=45, train_loss=0.04623204842209816\n[Logger-12256] epoch=50, train_loss=0.04600438103079796\n[Logger-12256] epoch=55, train_loss=0.044703975319862366\n[Logger-12256] epoch=60, train_loss=0.04272610694169998\n[Logger-12256] epoch=65, train_loss=0.04091019555926323\n[Logger-12256] epoch=70, train_loss=0.0397188700735569\n[Logger-12256] epoch=75, train_loss=0.03833559527993202\n[Logger-12256] epoch=80, train_loss=0.03827640041708946\n[Logger-12256] epoch=85, train_loss=0.03742297366261482\n[Logger-12256] epoch=90, train_loss=0.037238627672195435\n[Logger-12256] epoch=95, train_loss=0.036748528480529785\n[Logger-12256] epoch=0, train_loss=0.4522625207901001\n[Logger-12256] epoch=5, train_loss=0.3141877055168152\n[Logger-12256] epoch=10, train_loss=0.20244906842708588\n[Logger-12256] epoch=15, train_loss=0.12205637246370316\n[Logger-12256] epoch=20, train_loss=0.06834523379802704\n[Logger-12256] epoch=25, train_loss=0.04748177155852318\n\n\n57\ncoverage: 0.924\n\n\n[Logger-12256] epoch=30, train_loss=0.05110292136669159\n[Logger-12256] epoch=35, train_loss=0.05123455449938774\n[Logger-12256] epoch=40, train_loss=0.05036608502268791\n[Logger-12256] epoch=45, train_loss=0.04837866500020027\n[Logger-12256] epoch=50, train_loss=0.0460146963596344\n[Logger-12256] epoch=55, train_loss=0.04398508369922638\n[Logger-12256] epoch=60, train_loss=0.041642025113105774\n[Logger-12256] epoch=65, train_loss=0.04017260670661926\n[Logger-12256] epoch=70, train_loss=0.03957965224981308\n[Logger-12256] epoch=75, train_loss=0.0389305055141449\n[Logger-12256] epoch=80, train_loss=0.03835339471697807\n[Logger-12256] epoch=85, train_loss=0.03784539923071861\n[Logger-12256] epoch=90, train_loss=0.037183549255132675\n[Logger-12256] epoch=95, train_loss=0.03662896901369095\n[Logger-12256] epoch=0, train_loss=0.4714129865169525\n[Logger-12256] epoch=5, train_loss=0.32645735144615173\n[Logger-12256] epoch=10, train_loss=0.21493381261825562\n[Logger-12256] epoch=15, train_loss=0.13578976690769196\n[Logger-12256] epoch=20, train_loss=0.08005194365978241\n\n\n58\ncoverage: 0.882\n\n\n[Logger-12256] epoch=25, train_loss=0.05121032893657684\n[Logger-12256] epoch=30, train_loss=0.04876008257269859\n[Logger-12256] epoch=35, train_loss=0.049991872161626816\n[Logger-12256] epoch=40, train_loss=0.04869556799530983\n[Logger-12256] epoch=45, train_loss=0.04698171094059944\n[Logger-12256] epoch=50, train_loss=0.04560042917728424\n[Logger-12256] epoch=55, train_loss=0.043590836226940155\n[Logger-12256] epoch=60, train_loss=0.04099992290139198\n[Logger-12256] epoch=65, train_loss=0.04026159271597862\n[Logger-12256] epoch=70, train_loss=0.03966887295246124\n[Logger-12256] epoch=75, train_loss=0.03892053663730621\n[Logger-12256] epoch=80, train_loss=0.0380614697933197\n[Logger-12256] epoch=85, train_loss=0.03748435154557228\n[Logger-12256] epoch=90, train_loss=0.036877404898405075\n[Logger-12256] epoch=95, train_loss=0.036277253180742264\n[Logger-12256] epoch=0, train_loss=0.43034717440605164\n[Logger-12256] epoch=5, train_loss=0.2887076735496521\n[Logger-12256] epoch=10, train_loss=0.17814210057258606\n[Logger-12256] epoch=15, train_loss=0.09819066524505615\n[Logger-12256] epoch=20, train_loss=0.05754230171442032\n[Logger-12256] epoch=25, train_loss=0.06078162044286728\n\n\n59\ncoverage: 0.908\n\n\n[Logger-12256] epoch=30, train_loss=0.06291025876998901\n[Logger-12256] epoch=35, train_loss=0.061570558696985245\n[Logger-12256] epoch=40, train_loss=0.05805523321032524\n[Logger-12256] epoch=45, train_loss=0.0531989149749279\n[Logger-12256] epoch=50, train_loss=0.04785018786787987\n[Logger-12256] epoch=55, train_loss=0.04466155543923378\n[Logger-12256] epoch=60, train_loss=0.04211016371846199\n[Logger-12256] epoch=65, train_loss=0.041191086173057556\n[Logger-12256] epoch=70, train_loss=0.04002339020371437\n[Logger-12256] epoch=75, train_loss=0.03960855305194855\n[Logger-12256] epoch=80, train_loss=0.03863728418946266\n[Logger-12256] epoch=85, train_loss=0.038093846291303635\n[Logger-12256] epoch=90, train_loss=0.03729349002242088\n[Logger-12256] epoch=95, train_loss=0.03655397146940231\n[Logger-12256] epoch=0, train_loss=0.4539698362350464\n[Logger-12256] epoch=5, train_loss=0.33127671480178833\n[Logger-12256] epoch=10, train_loss=0.23097781836986542\n[Logger-12256] epoch=15, train_loss=0.1502017080783844\n[Logger-12256] epoch=20, train_loss=0.08945663273334503\n[Logger-12256] epoch=25, train_loss=0.05495309829711914\n\n\n60\ncoverage: 0.888\n\n\n[Logger-12256] epoch=30, train_loss=0.05316190421581268\n[Logger-12256] epoch=35, train_loss=0.054659273475408554\n[Logger-12256] epoch=40, train_loss=0.05292635038495064\n[Logger-12256] epoch=45, train_loss=0.04926569014787674\n[Logger-12256] epoch=50, train_loss=0.04556247591972351\n[Logger-12256] epoch=55, train_loss=0.04374850541353226\n[Logger-12256] epoch=60, train_loss=0.04149607568979263\n[Logger-12256] epoch=65, train_loss=0.04081717133522034\n[Logger-12256] epoch=70, train_loss=0.04000309854745865\n[Logger-12256] epoch=75, train_loss=0.03955182805657387\n[Logger-12256] epoch=80, train_loss=0.0389912985265255\n[Logger-12256] epoch=85, train_loss=0.03851873055100441\n[Logger-12256] epoch=90, train_loss=0.03797878324985504\n[Logger-12256] epoch=95, train_loss=0.03752501681447029\n[Logger-12256] epoch=0, train_loss=0.45166271924972534\n[Logger-12256] epoch=5, train_loss=0.3192254900932312\n[Logger-12256] epoch=10, train_loss=0.21319574117660522\n[Logger-12256] epoch=15, train_loss=0.131784588098526\n[Logger-12256] epoch=20, train_loss=0.07579702883958817\n[Logger-12256] epoch=25, train_loss=0.0492449551820755\n\n\n61\ncoverage: 0.904\n\n\n[Logger-12256] epoch=30, train_loss=0.05179061368107796\n[Logger-12256] epoch=35, train_loss=0.05232644081115723\n[Logger-12256] epoch=40, train_loss=0.050410620868206024\n[Logger-12256] epoch=45, train_loss=0.04772830009460449\n[Logger-12256] epoch=50, train_loss=0.04491646587848663\n[Logger-12256] epoch=55, train_loss=0.04323703050613403\n[Logger-12256] epoch=60, train_loss=0.04164775460958481\n[Logger-12256] epoch=65, train_loss=0.04073787108063698\n[Logger-12256] epoch=70, train_loss=0.04018843173980713\n[Logger-12256] epoch=75, train_loss=0.03955524414777756\n[Logger-12256] epoch=80, train_loss=0.03930255398154259\n[Logger-12256] epoch=85, train_loss=0.03877367451786995\n[Logger-12256] epoch=90, train_loss=0.038337592035532\n[Logger-12256] epoch=95, train_loss=0.03793715685606003\n[Logger-12256] epoch=0, train_loss=0.5273000001907349\n[Logger-12256] epoch=5, train_loss=0.39057502150535583\n[Logger-12256] epoch=10, train_loss=0.2809622287750244\n[Logger-12256] epoch=15, train_loss=0.19398172199726105\n[Logger-12256] epoch=20, train_loss=0.12752237915992737\n[Logger-12256] epoch=25, train_loss=0.08111792057752609\n\n\n62\ncoverage: 0.894\n\n\n[Logger-12256] epoch=30, train_loss=0.054786816239356995\n[Logger-12256] epoch=35, train_loss=0.0501183420419693\n[Logger-12256] epoch=40, train_loss=0.051597271114587784\n[Logger-12256] epoch=45, train_loss=0.05042370408773422\n[Logger-12256] epoch=50, train_loss=0.04751487821340561\n[Logger-12256] epoch=55, train_loss=0.04454086348414421\n[Logger-12256] epoch=60, train_loss=0.043139200657606125\n[Logger-12256] epoch=65, train_loss=0.04113869369029999\n[Logger-12256] epoch=70, train_loss=0.040583204478025436\n[Logger-12256] epoch=75, train_loss=0.03999179229140282\n[Logger-12256] epoch=80, train_loss=0.0393647700548172\n[Logger-12256] epoch=85, train_loss=0.03884764015674591\n[Logger-12256] epoch=90, train_loss=0.038335759192705154\n[Logger-12256] epoch=95, train_loss=0.037726473063230515\n[Logger-12256] epoch=0, train_loss=0.46016913652420044\n[Logger-12256] epoch=5, train_loss=0.34821516275405884\n[Logger-12256] epoch=10, train_loss=0.2564598619937897\n[Logger-12256] epoch=15, train_loss=0.18088752031326294\n[Logger-12256] epoch=20, train_loss=0.1178639605641365\n[Logger-12256] epoch=25, train_loss=0.07025665789842606\n\n\n63\ncoverage: 0.916\n\n\n[Logger-12256] epoch=30, train_loss=0.04676814377307892\n[Logger-12256] epoch=35, train_loss=0.049811411648988724\n[Logger-12256] epoch=40, train_loss=0.05071314051747322\n[Logger-12256] epoch=45, train_loss=0.04988338425755501\n[Logger-12256] epoch=50, train_loss=0.04784884303808212\n[Logger-12256] epoch=55, train_loss=0.04566046968102455\n[Logger-12256] epoch=60, train_loss=0.043956562876701355\n[Logger-12256] epoch=65, train_loss=0.042088113725185394\n[Logger-12256] epoch=70, train_loss=0.04083394259214401\n[Logger-12256] epoch=75, train_loss=0.039478152990341187\n[Logger-12256] epoch=80, train_loss=0.03934800252318382\n[Logger-12256] epoch=85, train_loss=0.03854168578982353\n[Logger-12256] epoch=90, train_loss=0.03821012005209923\n[Logger-12256] epoch=95, train_loss=0.03748501092195511\n[Logger-12256] epoch=0, train_loss=0.4679936170578003\n[Logger-12256] epoch=5, train_loss=0.35074254870414734\n[Logger-12256] epoch=10, train_loss=0.253955215215683\n[Logger-12256] epoch=15, train_loss=0.17074738442897797\n[Logger-12256] epoch=20, train_loss=0.10448598116636276\n[Logger-12256] epoch=25, train_loss=0.06110338494181633\n\n\n64\ncoverage: 0.902\n\n\n[Logger-12256] epoch=30, train_loss=0.04817502945661545\n[Logger-12256] epoch=35, train_loss=0.049902793020009995\n[Logger-12256] epoch=40, train_loss=0.04939026013016701\n[Logger-12256] epoch=45, train_loss=0.0482097789645195\n[Logger-12256] epoch=50, train_loss=0.046041179448366165\n[Logger-12256] epoch=55, train_loss=0.04361913353204727\n[Logger-12256] epoch=60, train_loss=0.04140838608145714\n[Logger-12256] epoch=65, train_loss=0.03974141925573349\n[Logger-12256] epoch=70, train_loss=0.03892107680439949\n[Logger-12256] epoch=75, train_loss=0.037872571498155594\n[Logger-12256] epoch=80, train_loss=0.03709135577082634\n[Logger-12256] epoch=85, train_loss=0.036472827196121216\n[Logger-12256] epoch=90, train_loss=0.03591481223702431\n[Logger-12256] epoch=95, train_loss=0.03522336483001709\n[Logger-12256] epoch=0, train_loss=0.4040033221244812\n[Logger-12256] epoch=5, train_loss=0.28711578249931335\n[Logger-12256] epoch=10, train_loss=0.1905003935098648\n[Logger-12256] epoch=15, train_loss=0.11349590867757797\n[Logger-12256] epoch=20, train_loss=0.06239345669746399\n[Logger-12256] epoch=25, train_loss=0.04532986506819725\n[Logger-12256] epoch=30, train_loss=0.04870981723070145\n\n\n65\ncoverage: 0.888\n\n\n[Logger-12256] epoch=35, train_loss=0.04868951812386513\n[Logger-12256] epoch=40, train_loss=0.04733540862798691\n[Logger-12256] epoch=45, train_loss=0.04668303206562996\n[Logger-12256] epoch=50, train_loss=0.044966671615839005\n[Logger-12256] epoch=55, train_loss=0.04259350895881653\n[Logger-12256] epoch=60, train_loss=0.04033255949616432\n[Logger-12256] epoch=65, train_loss=0.03947730362415314\n[Logger-12256] epoch=70, train_loss=0.03920094668865204\n[Logger-12256] epoch=75, train_loss=0.038227517157793045\n[Logger-12256] epoch=80, train_loss=0.03782175853848457\n[Logger-12256] epoch=85, train_loss=0.037281449884176254\n[Logger-12256] epoch=90, train_loss=0.03662650287151337\n[Logger-12256] epoch=95, train_loss=0.0360928438603878\n[Logger-12256] epoch=0, train_loss=0.40507540106773376\n[Logger-12256] epoch=5, train_loss=0.2763429582118988\n[Logger-12256] epoch=10, train_loss=0.17223474383354187\n[Logger-12256] epoch=15, train_loss=0.09907236695289612\n[Logger-12256] epoch=20, train_loss=0.05745888128876686\n\n\n66\ncoverage: 0.884\n\n\n[Logger-12256] epoch=25, train_loss=0.054924126714468\n[Logger-12256] epoch=30, train_loss=0.05699238181114197\n[Logger-12256] epoch=35, train_loss=0.055608510971069336\n[Logger-12256] epoch=40, train_loss=0.05243612080812454\n[Logger-12256] epoch=45, train_loss=0.049473896622657776\n[Logger-12256] epoch=50, train_loss=0.04582400619983673\n[Logger-12256] epoch=55, train_loss=0.043978869915008545\n[Logger-12256] epoch=60, train_loss=0.04255473241209984\n[Logger-12256] epoch=65, train_loss=0.04187075421214104\n[Logger-12256] epoch=70, train_loss=0.041087858378887177\n[Logger-12256] epoch=75, train_loss=0.04041830077767372\n[Logger-12256] epoch=80, train_loss=0.03976926952600479\n[Logger-12256] epoch=85, train_loss=0.03917154297232628\n[Logger-12256] epoch=90, train_loss=0.03870171681046486\n[Logger-12256] epoch=95, train_loss=0.038134194910526276\n[Logger-12256] epoch=0, train_loss=0.48989999294281006\n[Logger-12256] epoch=5, train_loss=0.3302239179611206\n[Logger-12256] epoch=10, train_loss=0.21031056344509125\n[Logger-12256] epoch=15, train_loss=0.11678964644670486\n[Logger-12256] epoch=20, train_loss=0.05971243232488632\n[Logger-12256] epoch=25, train_loss=0.04767441749572754\n[Logger-12256] epoch=30, train_loss=0.05063582956790924\n\n\n67\ncoverage: 0.912\n\n\n[Logger-12256] epoch=35, train_loss=0.05004963278770447\n[Logger-12256] epoch=40, train_loss=0.047652993351221085\n[Logger-12256] epoch=45, train_loss=0.045253098011016846\n[Logger-12256] epoch=50, train_loss=0.042759474366903305\n[Logger-12256] epoch=55, train_loss=0.04013648256659508\n[Logger-12256] epoch=60, train_loss=0.038413990288972855\n[Logger-12256] epoch=65, train_loss=0.0379473902285099\n[Logger-12256] epoch=70, train_loss=0.0373036228120327\n[Logger-12256] epoch=75, train_loss=0.03684817999601364\n[Logger-12256] epoch=80, train_loss=0.03628595173358917\n[Logger-12256] epoch=85, train_loss=0.035732705146074295\n[Logger-12256] epoch=90, train_loss=0.03525908663868904\n[Logger-12256] epoch=95, train_loss=0.03480631485581398\n[Logger-12256] epoch=0, train_loss=0.46636727452278137\n[Logger-12256] epoch=5, train_loss=0.3421221673488617\n[Logger-12256] epoch=10, train_loss=0.2480873316526413\n[Logger-12256] epoch=15, train_loss=0.1725776195526123\n[Logger-12256] epoch=20, train_loss=0.11726293712854385\n[Logger-12256] epoch=25, train_loss=0.07897071540355682\n\n\n68\ncoverage: 0.914\n\n\n[Logger-12256] epoch=30, train_loss=0.058984555304050446\n[Logger-12256] epoch=35, train_loss=0.04901833459734917\n[Logger-12256] epoch=40, train_loss=0.04824116453528404\n[Logger-12256] epoch=45, train_loss=0.04862188920378685\n[Logger-12256] epoch=50, train_loss=0.04709567502140999\n[Logger-12256] epoch=55, train_loss=0.045087069272994995\n[Logger-12256] epoch=60, train_loss=0.043523695319890976\n[Logger-12256] epoch=65, train_loss=0.042278073728084564\n[Logger-12256] epoch=70, train_loss=0.040891826152801514\n[Logger-12256] epoch=75, train_loss=0.04032192379236221\n[Logger-12256] epoch=80, train_loss=0.04006839171051979\n[Logger-12256] epoch=85, train_loss=0.03937910497188568\n[Logger-12256] epoch=90, train_loss=0.03885374218225479\n[Logger-12256] epoch=95, train_loss=0.03838672488927841\n[Logger-12256] epoch=0, train_loss=0.4545557498931885\n[Logger-12256] epoch=5, train_loss=0.3244512677192688\n[Logger-12256] epoch=10, train_loss=0.22017472982406616\n[Logger-12256] epoch=15, train_loss=0.13788333535194397\n[Logger-12256] epoch=20, train_loss=0.07678873836994171\n[Logger-12256] epoch=25, train_loss=0.049733422696590424\n\n\n69\ncoverage: 0.95\n\n\n[Logger-12256] epoch=30, train_loss=0.052999887615442276\n[Logger-12256] epoch=35, train_loss=0.05324399843811989\n[Logger-12256] epoch=40, train_loss=0.05114932358264923\n[Logger-12256] epoch=45, train_loss=0.04899736866354942\n[Logger-12256] epoch=50, train_loss=0.04620213434100151\n[Logger-12256] epoch=55, train_loss=0.043510206043720245\n[Logger-12256] epoch=60, train_loss=0.042258646339178085\n[Logger-12256] epoch=65, train_loss=0.04110327363014221\n[Logger-12256] epoch=70, train_loss=0.0400569923222065\n[Logger-12256] epoch=75, train_loss=0.039746277034282684\n[Logger-12256] epoch=80, train_loss=0.039026349782943726\n[Logger-12256] epoch=85, train_loss=0.03829747065901756\n[Logger-12256] epoch=90, train_loss=0.03769884631037712\n[Logger-12256] epoch=95, train_loss=0.03706301003694534\n[Logger-12256] epoch=0, train_loss=0.432918906211853\n[Logger-12256] epoch=5, train_loss=0.3313738703727722\n[Logger-12256] epoch=10, train_loss=0.24646180868148804\n[Logger-12256] epoch=15, train_loss=0.1747845560312271\n[Logger-12256] epoch=20, train_loss=0.11415834724903107\n[Logger-12256] epoch=25, train_loss=0.07082774490118027\n\n\n70\ncoverage: 0.908\n\n\n[Logger-12256] epoch=30, train_loss=0.05210838094353676\n[Logger-12256] epoch=35, train_loss=0.04948260635137558\n[Logger-12256] epoch=40, train_loss=0.050590891391038895\n[Logger-12256] epoch=45, train_loss=0.049893178045749664\n[Logger-12256] epoch=50, train_loss=0.047982923686504364\n[Logger-12256] epoch=55, train_loss=0.04586848244071007\n[Logger-12256] epoch=60, train_loss=0.04410981014370918\n[Logger-12256] epoch=65, train_loss=0.042974840849637985\n[Logger-12256] epoch=70, train_loss=0.04107571765780449\n[Logger-12256] epoch=75, train_loss=0.04037602245807648\n[Logger-12256] epoch=80, train_loss=0.039683494716882706\n[Logger-12256] epoch=85, train_loss=0.03870673477649689\n[Logger-12256] epoch=90, train_loss=0.03804740682244301\n[Logger-12256] epoch=95, train_loss=0.03735144063830376\n[Logger-12256] epoch=0, train_loss=0.47483888268470764\n[Logger-12256] epoch=5, train_loss=0.3383603096008301\n[Logger-12256] epoch=10, train_loss=0.23210014402866364\n[Logger-12256] epoch=15, train_loss=0.15447364747524261\n[Logger-12256] epoch=20, train_loss=0.09584209322929382\n[Logger-12256] epoch=25, train_loss=0.059661123901605606\n\n\n71\ncoverage: 0.902\n\n\n[Logger-12256] epoch=30, train_loss=0.05122574791312218\n[Logger-12256] epoch=35, train_loss=0.05270704999566078\n[Logger-12256] epoch=40, train_loss=0.051767099648714066\n[Logger-12256] epoch=45, train_loss=0.04948493465781212\n[Logger-12256] epoch=50, train_loss=0.04673464596271515\n[Logger-12256] epoch=55, train_loss=0.04491215571761131\n[Logger-12256] epoch=60, train_loss=0.04298194497823715\n[Logger-12256] epoch=65, train_loss=0.04109019786119461\n[Logger-12256] epoch=70, train_loss=0.040904320776462555\n[Logger-12256] epoch=75, train_loss=0.04017564654350281\n[Logger-12256] epoch=80, train_loss=0.0394529290497303\n[Logger-12256] epoch=85, train_loss=0.03898133710026741\n[Logger-12256] epoch=90, train_loss=0.03844006359577179\n[Logger-12256] epoch=95, train_loss=0.03786695376038551\n[Logger-12256] epoch=0, train_loss=0.39077186584472656\n[Logger-12256] epoch=5, train_loss=0.26523557305336\n[Logger-12256] epoch=10, train_loss=0.17068049311637878\n[Logger-12256] epoch=15, train_loss=0.10128408670425415\n[Logger-12256] epoch=20, train_loss=0.0638912171125412\n[Logger-12256] epoch=25, train_loss=0.05699820816516876\n\n\n72\ncoverage: 0.89\n\n\n[Logger-12256] epoch=30, train_loss=0.05944722890853882\n[Logger-12256] epoch=35, train_loss=0.05819253996014595\n[Logger-12256] epoch=40, train_loss=0.0546671599149704\n[Logger-12256] epoch=45, train_loss=0.04984722286462784\n[Logger-12256] epoch=50, train_loss=0.04545397311449051\n[Logger-12256] epoch=55, train_loss=0.04394673928618431\n[Logger-12256] epoch=60, train_loss=0.04321001097559929\n[Logger-12256] epoch=65, train_loss=0.04162846878170967\n[Logger-12256] epoch=70, train_loss=0.041450873017311096\n[Logger-12256] epoch=75, train_loss=0.04064004123210907\n[Logger-12256] epoch=80, train_loss=0.04020766541361809\n[Logger-12256] epoch=85, train_loss=0.03965570405125618\n[Logger-12256] epoch=90, train_loss=0.03919848054647446\n[Logger-12256] epoch=95, train_loss=0.03871885687112808\n[Logger-12256] epoch=0, train_loss=0.43192923069000244\n[Logger-12256] epoch=5, train_loss=0.3244688808917999\n[Logger-12256] epoch=10, train_loss=0.23487672209739685\n[Logger-12256] epoch=15, train_loss=0.15961956977844238\n[Logger-12256] epoch=20, train_loss=0.10148587822914124\n[Logger-12256] epoch=25, train_loss=0.06732112169265747\n[Logger-12256] epoch=30, train_loss=0.054564423859119415\n\n\n73\ncoverage: 0.896\n\n\n[Logger-12256] epoch=35, train_loss=0.05518701300024986\n[Logger-12256] epoch=40, train_loss=0.05443594977259636\n[Logger-12256] epoch=45, train_loss=0.051431044936180115\n[Logger-12256] epoch=50, train_loss=0.04744976386427879\n[Logger-12256] epoch=55, train_loss=0.04363347217440605\n[Logger-12256] epoch=60, train_loss=0.04275597631931305\n[Logger-12256] epoch=65, train_loss=0.041866105049848557\n[Logger-12256] epoch=70, train_loss=0.04076186567544937\n[Logger-12256] epoch=75, train_loss=0.040114786475896835\n[Logger-12256] epoch=80, train_loss=0.03960658609867096\n[Logger-12256] epoch=85, train_loss=0.03904305025935173\n[Logger-12256] epoch=90, train_loss=0.03839147463440895\n[Logger-12256] epoch=95, train_loss=0.03781704977154732\n[Logger-12256] epoch=0, train_loss=0.43813520669937134\n[Logger-12256] epoch=5, train_loss=0.3401870131492615\n[Logger-12256] epoch=10, train_loss=0.25918298959732056\n[Logger-12256] epoch=15, train_loss=0.19502660632133484\n[Logger-12256] epoch=20, train_loss=0.14342734217643738\n[Logger-12256] epoch=25, train_loss=0.10686413943767548\n\n\n74\ncoverage: 0.922\n\n\n[Logger-12256] epoch=30, train_loss=0.08242470026016235\n[Logger-12256] epoch=35, train_loss=0.06370352953672409\n[Logger-12256] epoch=40, train_loss=0.053508806973695755\n[Logger-12256] epoch=45, train_loss=0.052333954721689224\n[Logger-12256] epoch=50, train_loss=0.051121216267347336\n[Logger-12256] epoch=55, train_loss=0.047752633690834045\n[Logger-12256] epoch=60, train_loss=0.04615544527769089\n[Logger-12256] epoch=65, train_loss=0.044408366084098816\n[Logger-12256] epoch=70, train_loss=0.04332657903432846\n[Logger-12256] epoch=75, train_loss=0.04203924909234047\n[Logger-12256] epoch=80, train_loss=0.041492849588394165\n[Logger-12256] epoch=85, train_loss=0.04063905030488968\n[Logger-12256] epoch=90, train_loss=0.03988700732588768\n[Logger-12256] epoch=95, train_loss=0.039200395345687866\n[Logger-12256] epoch=0, train_loss=0.47192439436912537\n[Logger-12256] epoch=5, train_loss=0.3346640169620514\n[Logger-12256] epoch=10, train_loss=0.22814717888832092\n[Logger-12256] epoch=15, train_loss=0.14345072209835052\n[Logger-12256] epoch=20, train_loss=0.0809493213891983\n[Logger-12256] epoch=25, train_loss=0.05737925320863724\n\n\n75\ncoverage: 0.922\n\n\n[Logger-12256] epoch=30, train_loss=0.056717563420534134\n[Logger-12256] epoch=35, train_loss=0.057984430342912674\n[Logger-12256] epoch=40, train_loss=0.056258514523506165\n[Logger-12256] epoch=45, train_loss=0.05271882191300392\n[Logger-12256] epoch=50, train_loss=0.04817778989672661\n[Logger-12256] epoch=55, train_loss=0.0436236597597599\n[Logger-12256] epoch=60, train_loss=0.04259047284722328\n[Logger-12256] epoch=65, train_loss=0.04205694794654846\n[Logger-12256] epoch=70, train_loss=0.040483444929122925\n[Logger-12256] epoch=75, train_loss=0.04033186659216881\n[Logger-12256] epoch=80, train_loss=0.03964971750974655\n[Logger-12256] epoch=85, train_loss=0.039284367114305496\n[Logger-12256] epoch=90, train_loss=0.03868710249662399\n[Logger-12256] epoch=95, train_loss=0.03817523270845413\n[Logger-12256] epoch=0, train_loss=0.4958306550979614\n[Logger-12256] epoch=5, train_loss=0.3663189113140106\n[Logger-12256] epoch=10, train_loss=0.2650652527809143\n[Logger-12256] epoch=15, train_loss=0.1892320215702057\n[Logger-12256] epoch=20, train_loss=0.12995295226573944\n\n\n76\ncoverage: 0.894\n\n\n[Logger-12256] epoch=25, train_loss=0.0833011046051979\n[Logger-12256] epoch=30, train_loss=0.057967767119407654\n[Logger-12256] epoch=35, train_loss=0.05194849520921707\n[Logger-12256] epoch=40, train_loss=0.05327211692929268\n[Logger-12256] epoch=45, train_loss=0.05174368992447853\n[Logger-12256] epoch=50, train_loss=0.048478104174137115\n[Logger-12256] epoch=55, train_loss=0.046683572232723236\n[Logger-12256] epoch=60, train_loss=0.04455140605568886\n[Logger-12256] epoch=65, train_loss=0.042606253176927567\n[Logger-12256] epoch=70, train_loss=0.041242122650146484\n[Logger-12256] epoch=75, train_loss=0.04069279506802559\n[Logger-12256] epoch=80, train_loss=0.040165580809116364\n[Logger-12256] epoch=85, train_loss=0.039682477712631226\n[Logger-12256] epoch=90, train_loss=0.039265137165784836\n[Logger-12256] epoch=95, train_loss=0.038674455136060715\n[Logger-12256] epoch=0, train_loss=0.5101366639137268\n[Logger-12256] epoch=5, train_loss=0.3860487937927246\n[Logger-12256] epoch=10, train_loss=0.2846323251724243\n[Logger-12256] epoch=15, train_loss=0.2018875777721405\n[Logger-12256] epoch=20, train_loss=0.13124343752861023\n[Logger-12256] epoch=25, train_loss=0.07868170738220215\n[Logger-12256] epoch=30, train_loss=0.048598237335681915\n\n\n77\ncoverage: 0.868\n\n\n[Logger-12256] epoch=35, train_loss=0.04707905650138855\n[Logger-12256] epoch=40, train_loss=0.048500750213861465\n[Logger-12256] epoch=45, train_loss=0.04764953628182411\n[Logger-12256] epoch=50, train_loss=0.04631504416465759\n[Logger-12256] epoch=55, train_loss=0.0448300875723362\n[Logger-12256] epoch=60, train_loss=0.042987603694200516\n[Logger-12256] epoch=65, train_loss=0.0409197099506855\n[Logger-12256] epoch=70, train_loss=0.03978867456316948\n[Logger-12256] epoch=75, train_loss=0.038996364921331406\n[Logger-12256] epoch=80, train_loss=0.0384197011590004\n[Logger-12256] epoch=85, train_loss=0.03764347359538078\n[Logger-12256] epoch=90, train_loss=0.03725099563598633\n[Logger-12256] epoch=95, train_loss=0.03671513497829437\n[Logger-12256] epoch=0, train_loss=0.4386158585548401\n[Logger-12256] epoch=5, train_loss=0.31128132343292236\n[Logger-12256] epoch=10, train_loss=0.21036297082901\n[Logger-12256] epoch=15, train_loss=0.12809468805789948\n[Logger-12256] epoch=20, train_loss=0.0677591860294342\n[Logger-12256] epoch=25, train_loss=0.04740384966135025\n[Logger-12256] epoch=30, train_loss=0.05145895481109619\n\n\n78\ncoverage: 0.882\n\n\n[Logger-12256] epoch=35, train_loss=0.05241304263472557\n[Logger-12256] epoch=40, train_loss=0.050778187811374664\n[Logger-12256] epoch=45, train_loss=0.0490032322704792\n[Logger-12256] epoch=50, train_loss=0.04667239636182785\n[Logger-12256] epoch=55, train_loss=0.043632566928863525\n[Logger-12256] epoch=60, train_loss=0.04212184250354767\n[Logger-12256] epoch=65, train_loss=0.040810007601976395\n[Logger-12256] epoch=70, train_loss=0.03979836404323578\n[Logger-12256] epoch=75, train_loss=0.03962365910410881\n[Logger-12256] epoch=80, train_loss=0.03870094567537308\n[Logger-12256] epoch=85, train_loss=0.03810255602002144\n[Logger-12256] epoch=90, train_loss=0.03745928779244423\n[Logger-12256] epoch=95, train_loss=0.03682948648929596\n[Logger-12256] epoch=0, train_loss=0.3583585321903229\n[Logger-12256] epoch=5, train_loss=0.24898909032344818\n[Logger-12256] epoch=10, train_loss=0.17024169862270355\n[Logger-12256] epoch=15, train_loss=0.11628270149230957\n\n\n79\ncoverage: 0.892\n\n\n[Logger-12256] epoch=20, train_loss=0.08671855181455612\n[Logger-12256] epoch=25, train_loss=0.06629197299480438\n[Logger-12256] epoch=30, train_loss=0.05545875057578087\n[Logger-12256] epoch=35, train_loss=0.05389079079031944\n[Logger-12256] epoch=40, train_loss=0.0510878711938858\n[Logger-12256] epoch=45, train_loss=0.04979218542575836\n[Logger-12256] epoch=50, train_loss=0.04783345386385918\n[Logger-12256] epoch=55, train_loss=0.045363474637269974\n[Logger-12256] epoch=60, train_loss=0.04321211576461792\n[Logger-12256] epoch=65, train_loss=0.042995620518922806\n[Logger-12256] epoch=70, train_loss=0.04233114793896675\n[Logger-12256] epoch=75, train_loss=0.04129459708929062\n[Logger-12256] epoch=80, train_loss=0.04075302556157112\n[Logger-12256] epoch=85, train_loss=0.040233708918094635\n[Logger-12256] epoch=90, train_loss=0.03955822065472603\n[Logger-12256] epoch=95, train_loss=0.03901008889079094\n[Logger-12256] epoch=0, train_loss=0.4558483362197876\n[Logger-12256] epoch=5, train_loss=0.30925777554512024\n[Logger-12256] epoch=10, train_loss=0.19793616235256195\n[Logger-12256] epoch=15, train_loss=0.12421135604381561\n[Logger-12256] epoch=20, train_loss=0.08317477256059647\n[Logger-12256] epoch=25, train_loss=0.06385724991559982\n\n\n80\ncoverage: 0.932\n\n\n[Logger-12256] epoch=30, train_loss=0.05155795440077782\n[Logger-12256] epoch=35, train_loss=0.051622454077005386\n[Logger-12256] epoch=40, train_loss=0.05025624483823776\n[Logger-12256] epoch=45, train_loss=0.04810892790555954\n[Logger-12256] epoch=50, train_loss=0.045008912682533264\n[Logger-12256] epoch=55, train_loss=0.04368041455745697\n[Logger-12256] epoch=60, train_loss=0.041271090507507324\n[Logger-12256] epoch=65, train_loss=0.04007873684167862\n[Logger-12256] epoch=70, train_loss=0.03953494504094124\n[Logger-12256] epoch=75, train_loss=0.03855656832456589\n[Logger-12256] epoch=80, train_loss=0.03805512562394142\n[Logger-12256] epoch=85, train_loss=0.037398237735033035\n[Logger-12256] epoch=90, train_loss=0.03671305254101753\n[Logger-12256] epoch=95, train_loss=0.03611401468515396\n[Logger-12256] epoch=0, train_loss=0.3604421615600586\n[Logger-12256] epoch=5, train_loss=0.23799100518226624\n[Logger-12256] epoch=10, train_loss=0.14452825486660004\n[Logger-12256] epoch=15, train_loss=0.07521547377109528\n[Logger-12256] epoch=20, train_loss=0.043970171362161636\n[Logger-12256] epoch=25, train_loss=0.04756571352481842\n[Logger-12256] epoch=30, train_loss=0.048327744007110596\n\n\n81\ncoverage: 0.878\n\n\n[Logger-12256] epoch=35, train_loss=0.04854816943407059\n[Logger-12256] epoch=40, train_loss=0.04688210040330887\n[Logger-12256] epoch=45, train_loss=0.04412849619984627\n[Logger-12256] epoch=50, train_loss=0.04086591675877571\n[Logger-12256] epoch=55, train_loss=0.038761384785175323\n[Logger-12256] epoch=60, train_loss=0.03799450770020485\n[Logger-12256] epoch=65, train_loss=0.03720693662762642\n[Logger-12256] epoch=70, train_loss=0.03598594665527344\n[Logger-12256] epoch=75, train_loss=0.035380247980356216\n[Logger-12256] epoch=80, train_loss=0.03454388305544853\n[Logger-12256] epoch=85, train_loss=0.03363216668367386\n[Logger-12256] epoch=90, train_loss=0.032669685781002045\n[Logger-12256] epoch=95, train_loss=0.03178353235125542\n[Logger-12256] epoch=0, train_loss=0.4038960039615631\n[Logger-12256] epoch=5, train_loss=0.25989004969596863\n[Logger-12256] epoch=10, train_loss=0.14922647178173065\n[Logger-12256] epoch=15, train_loss=0.07894448190927505\n[Logger-12256] epoch=20, train_loss=0.056262679398059845\n\n\n82\ncoverage: 0.902\n\n\n[Logger-12256] epoch=25, train_loss=0.05494467541575432\n[Logger-12256] epoch=30, train_loss=0.05578625574707985\n[Logger-12256] epoch=35, train_loss=0.05356166511774063\n[Logger-12256] epoch=40, train_loss=0.050126537680625916\n[Logger-12256] epoch=45, train_loss=0.047527194023132324\n[Logger-12256] epoch=50, train_loss=0.04602600261569023\n[Logger-12256] epoch=55, train_loss=0.04327058419585228\n[Logger-12256] epoch=60, train_loss=0.041160229593515396\n[Logger-12256] epoch=65, train_loss=0.04104096069931984\n[Logger-12256] epoch=70, train_loss=0.0401528999209404\n[Logger-12256] epoch=75, train_loss=0.039611004292964935\n[Logger-12256] epoch=80, train_loss=0.03889288380742073\n[Logger-12256] epoch=85, train_loss=0.03834952786564827\n[Logger-12256] epoch=90, train_loss=0.0377657413482666\n[Logger-12256] epoch=95, train_loss=0.03715720400214195\n[Logger-12256] epoch=0, train_loss=0.46185627579689026\n[Logger-12256] epoch=5, train_loss=0.32318708300590515\n[Logger-12256] epoch=10, train_loss=0.2182362675666809\n[Logger-12256] epoch=15, train_loss=0.13752982020378113\n[Logger-12256] epoch=20, train_loss=0.08366948366165161\n[Logger-12256] epoch=25, train_loss=0.06122400984168053\n\n\n83\ncoverage: 0.932\n\n\n[Logger-12256] epoch=30, train_loss=0.050249967724084854\n[Logger-12256] epoch=35, train_loss=0.049334362149238586\n[Logger-12256] epoch=40, train_loss=0.04838329553604126\n[Logger-12256] epoch=45, train_loss=0.046967219561338425\n[Logger-12256] epoch=50, train_loss=0.04510613530874252\n[Logger-12256] epoch=55, train_loss=0.04327000677585602\n[Logger-12256] epoch=60, train_loss=0.04095805063843727\n[Logger-12256] epoch=65, train_loss=0.04060135781764984\n[Logger-12256] epoch=70, train_loss=0.0400862917304039\n[Logger-12256] epoch=75, train_loss=0.039248209446668625\n[Logger-12256] epoch=80, train_loss=0.03873636946082115\n[Logger-12256] epoch=85, train_loss=0.03815658763051033\n[Logger-12256] epoch=90, train_loss=0.03757801279425621\n[Logger-12256] epoch=95, train_loss=0.03699544072151184\n[Logger-12256] epoch=0, train_loss=0.5348079800605774\n[Logger-12256] epoch=5, train_loss=0.35711807012557983\n[Logger-12256] epoch=10, train_loss=0.2213040441274643\n[Logger-12256] epoch=15, train_loss=0.12465465813875198\n[Logger-12256] epoch=20, train_loss=0.07285384833812714\n[Logger-12256] epoch=25, train_loss=0.049096256494522095\n\n\n84\ncoverage: 0.906\n\n\n[Logger-12256] epoch=30, train_loss=0.04986691474914551\n[Logger-12256] epoch=35, train_loss=0.050603900104761124\n[Logger-12256] epoch=40, train_loss=0.0494166724383831\n[Logger-12256] epoch=45, train_loss=0.04669780284166336\n[Logger-12256] epoch=50, train_loss=0.044967424124479294\n[Logger-12256] epoch=55, train_loss=0.04387083649635315\n[Logger-12256] epoch=60, train_loss=0.04219641909003258\n[Logger-12256] epoch=65, train_loss=0.04036775231361389\n[Logger-12256] epoch=70, train_loss=0.03862060606479645\n[Logger-12256] epoch=75, train_loss=0.03843851014971733\n[Logger-12256] epoch=80, train_loss=0.03794747218489647\n[Logger-12256] epoch=85, train_loss=0.03741202875971794\n[Logger-12256] epoch=90, train_loss=0.0370420403778553\n[Logger-12256] epoch=95, train_loss=0.03655039519071579\n[Logger-12256] epoch=0, train_loss=0.4440978169441223\n[Logger-12256] epoch=5, train_loss=0.31485116481781006\n[Logger-12256] epoch=10, train_loss=0.21596118807792664\n[Logger-12256] epoch=15, train_loss=0.1357245296239853\n[Logger-12256] epoch=20, train_loss=0.07755589485168457\n[Logger-12256] epoch=25, train_loss=0.049520544707775116\n\n\n85\ncoverage: 0.894\n\n\n[Logger-12256] epoch=30, train_loss=0.049354370683431625\n[Logger-12256] epoch=35, train_loss=0.05020863935351372\n[Logger-12256] epoch=40, train_loss=0.04846091568470001\n[Logger-12256] epoch=45, train_loss=0.04745243117213249\n[Logger-12256] epoch=50, train_loss=0.046075597405433655\n[Logger-12256] epoch=55, train_loss=0.04345422983169556\n[Logger-12256] epoch=60, train_loss=0.04083242639899254\n[Logger-12256] epoch=65, train_loss=0.040189873427152634\n[Logger-12256] epoch=70, train_loss=0.03935958072543144\n[Logger-12256] epoch=75, train_loss=0.03840519115328789\n[Logger-12256] epoch=80, train_loss=0.03795062378048897\n[Logger-12256] epoch=85, train_loss=0.03728495165705681\n[Logger-12256] epoch=90, train_loss=0.03665927052497864\n[Logger-12256] epoch=95, train_loss=0.0360102541744709\n[Logger-12256] epoch=0, train_loss=0.49305257201194763\n[Logger-12256] epoch=5, train_loss=0.3451977074146271\n[Logger-12256] epoch=10, train_loss=0.22782737016677856\n[Logger-12256] epoch=15, train_loss=0.13609391450881958\n[Logger-12256] epoch=20, train_loss=0.07786756008863449\n[Logger-12256] epoch=25, train_loss=0.05967727303504944\n[Logger-12256] epoch=30, train_loss=0.06002466380596161\n\n\n86\ncoverage: 0.858\n\n\n[Logger-12256] epoch=35, train_loss=0.06047961860895157\n[Logger-12256] epoch=40, train_loss=0.05797173082828522\n[Logger-12256] epoch=45, train_loss=0.05363679677248001\n[Logger-12256] epoch=50, train_loss=0.04849296435713768\n[Logger-12256] epoch=55, train_loss=0.044956810772418976\n[Logger-12256] epoch=60, train_loss=0.04356136545538902\n[Logger-12256] epoch=65, train_loss=0.04263198375701904\n[Logger-12256] epoch=70, train_loss=0.041359879076480865\n[Logger-12256] epoch=75, train_loss=0.041105762124061584\n[Logger-12256] epoch=80, train_loss=0.040449827909469604\n[Logger-12256] epoch=85, train_loss=0.03975779190659523\n[Logger-12256] epoch=90, train_loss=0.03930855169892311\n[Logger-12256] epoch=95, train_loss=0.03887568786740303\n[Logger-12256] epoch=0, train_loss=0.5003805756568909\n[Logger-12256] epoch=5, train_loss=0.38020533323287964\n[Logger-12256] epoch=10, train_loss=0.28373757004737854\n[Logger-12256] epoch=15, train_loss=0.20464304089546204\n[Logger-12256] epoch=20, train_loss=0.1388877034187317\n[Logger-12256] epoch=25, train_loss=0.09220419824123383\n\n\n87\ncoverage: 0.874\n\n\n[Logger-12256] epoch=30, train_loss=0.07088837772607803\n[Logger-12256] epoch=35, train_loss=0.060699161142110825\n[Logger-12256] epoch=40, train_loss=0.05796036496758461\n[Logger-12256] epoch=45, train_loss=0.057121556252241135\n[Logger-12256] epoch=50, train_loss=0.05392540991306305\n[Logger-12256] epoch=55, train_loss=0.050472382456064224\n[Logger-12256] epoch=60, train_loss=0.047190286219120026\n[Logger-12256] epoch=65, train_loss=0.044337209314107895\n[Logger-12256] epoch=70, train_loss=0.04341769963502884\n[Logger-12256] epoch=75, train_loss=0.042030639946460724\n[Logger-12256] epoch=80, train_loss=0.04081467166543007\n[Logger-12256] epoch=85, train_loss=0.040558505803346634\n[Logger-12256] epoch=90, train_loss=0.03997844457626343\n[Logger-12256] epoch=95, train_loss=0.03938929736614227\n[Logger-12256] epoch=0, train_loss=0.5525731444358826\n[Logger-12256] epoch=5, train_loss=0.41635817289352417\n[Logger-12256] epoch=10, train_loss=0.2995949983596802\n[Logger-12256] epoch=15, train_loss=0.20505207777023315\n[Logger-12256] epoch=20, train_loss=0.12853214144706726\n[Logger-12256] epoch=25, train_loss=0.07754533737897873\n[Logger-12256] epoch=30, train_loss=0.05619581788778305\n\n\n88\ncoverage: 0.924\n\n\n[Logger-12256] epoch=35, train_loss=0.05190717428922653\n[Logger-12256] epoch=40, train_loss=0.05366792157292366\n[Logger-12256] epoch=45, train_loss=0.05312570556998253\n[Logger-12256] epoch=50, train_loss=0.051008425652980804\n[Logger-12256] epoch=55, train_loss=0.048130206763744354\n[Logger-12256] epoch=60, train_loss=0.04491173103451729\n[Logger-12256] epoch=65, train_loss=0.04326288774609566\n[Logger-12256] epoch=70, train_loss=0.04143543541431427\n[Logger-12256] epoch=75, train_loss=0.04126135632395744\n[Logger-12256] epoch=80, train_loss=0.04023896902799606\n[Logger-12256] epoch=85, train_loss=0.039843056350946426\n[Logger-12256] epoch=90, train_loss=0.039288345724344254\n[Logger-12256] epoch=95, train_loss=0.03878124803304672\n[Logger-12256] epoch=0, train_loss=0.42841267585754395\n[Logger-12256] epoch=5, train_loss=0.32751697301864624\n[Logger-12256] epoch=10, train_loss=0.23626498878002167\n[Logger-12256] epoch=15, train_loss=0.16138125956058502\n[Logger-12256] epoch=20, train_loss=0.10249900072813034\n[Logger-12256] epoch=25, train_loss=0.06043250486254692\n[Logger-12256] epoch=30, train_loss=0.04961178079247475\n\n\n89\ncoverage: 0.902\n\n\n[Logger-12256] epoch=35, train_loss=0.05203256383538246\n[Logger-12256] epoch=40, train_loss=0.05149594321846962\n[Logger-12256] epoch=45, train_loss=0.04879148676991463\n[Logger-12256] epoch=50, train_loss=0.045812394469976425\n[Logger-12256] epoch=55, train_loss=0.04388270899653435\n[Logger-12256] epoch=60, train_loss=0.041643962264060974\n[Logger-12256] epoch=65, train_loss=0.04022470861673355\n[Logger-12256] epoch=70, train_loss=0.039103128015995026\n[Logger-12256] epoch=75, train_loss=0.03868521749973297\n[Logger-12256] epoch=80, train_loss=0.037992388010025024\n[Logger-12256] epoch=85, train_loss=0.03747883066534996\n[Logger-12256] epoch=90, train_loss=0.036883674561977386\n[Logger-12256] epoch=95, train_loss=0.036296602338552475\n[Logger-12256] epoch=0, train_loss=0.555645227432251\n[Logger-12256] epoch=5, train_loss=0.40982627868652344\n[Logger-12256] epoch=10, train_loss=0.29218927025794983\n[Logger-12256] epoch=15, train_loss=0.19791458547115326\n[Logger-12256] epoch=20, train_loss=0.12372827529907227\n[Logger-12256] epoch=25, train_loss=0.07096312195062637\n\n\n90\ncoverage: 0.88\n\n\n[Logger-12256] epoch=30, train_loss=0.05301058292388916\n[Logger-12256] epoch=35, train_loss=0.054695919156074524\n[Logger-12256] epoch=40, train_loss=0.05484715476632118\n[Logger-12256] epoch=45, train_loss=0.052153490483760834\n[Logger-12256] epoch=50, train_loss=0.048034049570560455\n[Logger-12256] epoch=55, train_loss=0.04380375146865845\n[Logger-12256] epoch=60, train_loss=0.04209907725453377\n[Logger-12256] epoch=65, train_loss=0.04109681770205498\n[Logger-12256] epoch=70, train_loss=0.04019208624958992\n[Logger-12256] epoch=75, train_loss=0.039511095732450485\n[Logger-12256] epoch=80, train_loss=0.03915656358003616\n[Logger-12256] epoch=85, train_loss=0.03892030566930771\n[Logger-12256] epoch=90, train_loss=0.038505204021930695\n[Logger-12256] epoch=95, train_loss=0.038184091448783875\n[Logger-12256] epoch=0, train_loss=0.4261487126350403\n[Logger-12256] epoch=5, train_loss=0.29783278703689575\n[Logger-12256] epoch=10, train_loss=0.19857460260391235\n[Logger-12256] epoch=15, train_loss=0.1262204945087433\n[Logger-12256] epoch=20, train_loss=0.07449554651975632\n[Logger-12256] epoch=25, train_loss=0.0519225038588047\n\n\n91\ncoverage: 0.876\n\n\n[Logger-12256] epoch=30, train_loss=0.05445556342601776\n[Logger-12256] epoch=35, train_loss=0.054835688322782516\n[Logger-12256] epoch=40, train_loss=0.05277067795395851\n[Logger-12256] epoch=45, train_loss=0.04931781068444252\n[Logger-12256] epoch=50, train_loss=0.04713878780603409\n[Logger-12256] epoch=55, train_loss=0.045144934207201004\n[Logger-12256] epoch=60, train_loss=0.043577685952186584\n[Logger-12256] epoch=65, train_loss=0.041235294193029404\n[Logger-12256] epoch=70, train_loss=0.04147231578826904\n[Logger-12256] epoch=75, train_loss=0.04041288048028946\n[Logger-12256] epoch=80, train_loss=0.03998004272580147\n[Logger-12256] epoch=85, train_loss=0.03934504836797714\n[Logger-12256] epoch=90, train_loss=0.03876421973109245\n[Logger-12256] epoch=95, train_loss=0.0382695309817791\n[Logger-12256] epoch=0, train_loss=0.4035535454750061\n[Logger-12256] epoch=5, train_loss=0.27495014667510986\n[Logger-12256] epoch=10, train_loss=0.17238940298557281\n[Logger-12256] epoch=15, train_loss=0.09689836949110031\n[Logger-12256] epoch=20, train_loss=0.06108700856566429\n[Logger-12256] epoch=25, train_loss=0.04989857226610184\n\n\n92\ncoverage: 0.888\n\n\n[Logger-12256] epoch=30, train_loss=0.05035955086350441\n[Logger-12256] epoch=35, train_loss=0.049364663660526276\n[Logger-12256] epoch=40, train_loss=0.047510478645563126\n[Logger-12256] epoch=45, train_loss=0.04535440355539322\n[Logger-12256] epoch=50, train_loss=0.04322020336985588\n[Logger-12256] epoch=55, train_loss=0.04125402122735977\n[Logger-12256] epoch=60, train_loss=0.0393996424973011\n[Logger-12256] epoch=65, train_loss=0.039100099354982376\n[Logger-12256] epoch=70, train_loss=0.038562074303627014\n[Logger-12256] epoch=75, train_loss=0.037909120321273804\n[Logger-12256] epoch=80, train_loss=0.03747133910655975\n[Logger-12256] epoch=85, train_loss=0.0369177870452404\n[Logger-12256] epoch=90, train_loss=0.03645189851522446\n[Logger-12256] epoch=95, train_loss=0.03597957268357277\n[Logger-12256] epoch=0, train_loss=0.42886441946029663\n[Logger-12256] epoch=5, train_loss=0.29447823762893677\n[Logger-12256] epoch=10, train_loss=0.17930039763450623\n[Logger-12256] epoch=15, train_loss=0.09769755601882935\n[Logger-12256] epoch=20, train_loss=0.05457908660173416\n[Logger-12256] epoch=25, train_loss=0.05518260598182678\n\n\n93\ncoverage: 0.936\n\n\n[Logger-12256] epoch=30, train_loss=0.057018522173166275\n[Logger-12256] epoch=35, train_loss=0.05563294515013695\n[Logger-12256] epoch=40, train_loss=0.05275869369506836\n[Logger-12256] epoch=45, train_loss=0.04940052330493927\n[Logger-12256] epoch=50, train_loss=0.045346032828092575\n[Logger-12256] epoch=55, train_loss=0.042308446019887924\n[Logger-12256] epoch=60, train_loss=0.039981674402952194\n[Logger-12256] epoch=65, train_loss=0.03915724158287048\n[Logger-12256] epoch=70, train_loss=0.0386255718767643\n[Logger-12256] epoch=75, train_loss=0.03827963024377823\n[Logger-12256] epoch=80, train_loss=0.03745907172560692\n[Logger-12256] epoch=85, train_loss=0.037010494619607925\n[Logger-12256] epoch=90, train_loss=0.036559924483299255\n[Logger-12256] epoch=95, train_loss=0.036084145307540894\n[Logger-12256] epoch=0, train_loss=0.4885547459125519\n[Logger-12256] epoch=5, train_loss=0.36985543370246887\n[Logger-12256] epoch=10, train_loss=0.27791592478752136\n[Logger-12256] epoch=15, train_loss=0.2026977390050888\n[Logger-12256] epoch=20, train_loss=0.141810804605484\n[Logger-12256] epoch=25, train_loss=0.09133556485176086\n\n\n94\ncoverage: 0.85\n\n\n[Logger-12256] epoch=30, train_loss=0.056991178542375565\n[Logger-12256] epoch=35, train_loss=0.05028737336397171\n[Logger-12256] epoch=40, train_loss=0.05228940770030022\n[Logger-12256] epoch=45, train_loss=0.051363252103328705\n[Logger-12256] epoch=50, train_loss=0.0489630363881588\n[Logger-12256] epoch=55, train_loss=0.04645301029086113\n[Logger-12256] epoch=60, train_loss=0.04477689415216446\n[Logger-12256] epoch=65, train_loss=0.042681366205215454\n[Logger-12256] epoch=70, train_loss=0.04092171788215637\n[Logger-12256] epoch=75, train_loss=0.040426190942525864\n[Logger-12256] epoch=80, train_loss=0.03968409076333046\n[Logger-12256] epoch=85, train_loss=0.03911863639950752\n[Logger-12256] epoch=90, train_loss=0.03846622630953789\n[Logger-12256] epoch=95, train_loss=0.03805838152766228\n[Logger-12256] epoch=0, train_loss=0.43243885040283203\n[Logger-12256] epoch=5, train_loss=0.32018595933914185\n[Logger-12256] epoch=10, train_loss=0.2312690019607544\n[Logger-12256] epoch=15, train_loss=0.16045351326465607\n[Logger-12256] epoch=20, train_loss=0.10467290133237839\n[Logger-12256] epoch=25, train_loss=0.06303632259368896\n\n\n95\ncoverage: 0.888\n\n\n[Logger-12256] epoch=30, train_loss=0.04840483143925667\n[Logger-12256] epoch=35, train_loss=0.05107712373137474\n[Logger-12256] epoch=40, train_loss=0.05060253292322159\n[Logger-12256] epoch=45, train_loss=0.04977763444185257\n[Logger-12256] epoch=50, train_loss=0.048491206020116806\n[Logger-12256] epoch=55, train_loss=0.04600107669830322\n[Logger-12256] epoch=60, train_loss=0.04331737756729126\n[Logger-12256] epoch=65, train_loss=0.04228217527270317\n[Logger-12256] epoch=70, train_loss=0.04165012389421463\n[Logger-12256] epoch=75, train_loss=0.04071315377950668\n[Logger-12256] epoch=80, train_loss=0.039957981556653976\n[Logger-12256] epoch=85, train_loss=0.039331335574388504\n[Logger-12256] epoch=90, train_loss=0.03857287019491196\n[Logger-12256] epoch=95, train_loss=0.037819452583789825\n[Logger-12256] epoch=0, train_loss=0.39127421379089355\n[Logger-12256] epoch=5, train_loss=0.2584647238254547\n[Logger-12256] epoch=10, train_loss=0.15555208921432495\n[Logger-12256] epoch=15, train_loss=0.08348039537668228\n[Logger-12256] epoch=20, train_loss=0.051174815744161606\n\n\n96\ncoverage: 0.924\n\n\n[Logger-12256] epoch=25, train_loss=0.052002258598804474\n[Logger-12256] epoch=30, train_loss=0.05256889387965202\n[Logger-12256] epoch=35, train_loss=0.05001956596970558\n[Logger-12256] epoch=40, train_loss=0.04604793339967728\n[Logger-12256] epoch=45, train_loss=0.04320717602968216\n[Logger-12256] epoch=50, train_loss=0.039849914610385895\n[Logger-12256] epoch=55, train_loss=0.03928997367620468\n[Logger-12256] epoch=60, train_loss=0.037735480815172195\n[Logger-12256] epoch=65, train_loss=0.037370745092630386\n[Logger-12256] epoch=70, train_loss=0.036487579345703125\n[Logger-12256] epoch=75, train_loss=0.036093175411224365\n[Logger-12256] epoch=80, train_loss=0.03540559485554695\n[Logger-12256] epoch=85, train_loss=0.03478879854083061\n[Logger-12256] epoch=90, train_loss=0.034188851714134216\n[Logger-12256] epoch=95, train_loss=0.03357511758804321\n[Logger-12256] epoch=0, train_loss=0.4909975826740265\n[Logger-12256] epoch=5, train_loss=0.33390700817108154\n[Logger-12256] epoch=10, train_loss=0.2143675982952118\n[Logger-12256] epoch=15, train_loss=0.12312082201242447\n[Logger-12256] epoch=20, train_loss=0.06498870253562927\n\n\n97\ncoverage: 0.874\n\n\n[Logger-12256] epoch=25, train_loss=0.047924380749464035\n[Logger-12256] epoch=30, train_loss=0.05043193697929382\n[Logger-12256] epoch=35, train_loss=0.050855603069067\n[Logger-12256] epoch=40, train_loss=0.04995203763246536\n[Logger-12256] epoch=45, train_loss=0.04915793612599373\n[Logger-12256] epoch=50, train_loss=0.047668490558862686\n[Logger-12256] epoch=55, train_loss=0.04544883221387863\n[Logger-12256] epoch=60, train_loss=0.04309288412332535\n[Logger-12256] epoch=65, train_loss=0.04177439957857132\n[Logger-12256] epoch=70, train_loss=0.04042035713791847\n[Logger-12256] epoch=75, train_loss=0.040255106985569\n[Logger-12256] epoch=80, train_loss=0.03949299827218056\n[Logger-12256] epoch=85, train_loss=0.03908047825098038\n[Logger-12256] epoch=90, train_loss=0.03847428038716316\n[Logger-12256] epoch=95, train_loss=0.038009632378816605\n[Logger-12256] epoch=0, train_loss=0.42069151997566223\n[Logger-12256] epoch=5, train_loss=0.27062344551086426\n[Logger-12256] epoch=10, train_loss=0.15688297152519226\n[Logger-12256] epoch=15, train_loss=0.0848878026008606\n[Logger-12256] epoch=20, train_loss=0.058533985167741776\n[Logger-12256] epoch=25, train_loss=0.05682986602187157\n\n\n98\ncoverage: 0.894\n\n\n[Logger-12256] epoch=30, train_loss=0.057804133743047714\n[Logger-12256] epoch=35, train_loss=0.055468302220106125\n[Logger-12256] epoch=40, train_loss=0.05215047672390938\n[Logger-12256] epoch=45, train_loss=0.0483589842915535\n[Logger-12256] epoch=50, train_loss=0.04319218546152115\n[Logger-12256] epoch=55, train_loss=0.04351486265659332\n[Logger-12256] epoch=60, train_loss=0.04188632220029831\n[Logger-12256] epoch=65, train_loss=0.04146003723144531\n[Logger-12256] epoch=70, train_loss=0.04057975858449936\n[Logger-12256] epoch=75, train_loss=0.03986416012048721\n[Logger-12256] epoch=80, train_loss=0.03902144730091095\n[Logger-12256] epoch=85, train_loss=0.0383807010948658\n[Logger-12256] epoch=90, train_loss=0.037747468799352646\n[Logger-12256] epoch=95, train_loss=0.03708880394697189\n\n\n99\ncoverage: 0.918\n</code></pre> <pre><code>The Kernel crashed while executing code in the current cell or a previous cell.\n\n\nPlease review the code in the cell(s) to identify a possible cause of the failure.\n\n\nClick &lt;a href='https://aka.ms/vscodeJupyterKernelCrash'&gt;here&lt;/a&gt; for more info.\n\n\nView Jupyter &lt;a href='command:jupyter.viewOutput'&gt;log&lt;/a&gt; for further details.\n</code></pre>"},{"location":"examples/examples_sin_1d/","title":"Getting Started:","text":"<p>This is an example script to demonstrate the capabilities of UQregressors with some examples that you can copy and paste to start generating results in a matter of minutes. If you are considering whether to use this package and do not need a detailed implementation and explanation yet, please check the \"Is UQregressors right for you?\" examples page. </p> <p>There are five main capabilities of UQregessors: </p> <ol> <li>Dataset Loading and Validation </li> <li>Regressors of various types created with Uncertainty Quantification (UQ) capability</li> <li>Hyperparameter Tuning using bayesian optimization (wrapper around Optuna)</li> <li>Metrics for evaluating goodness of fit and quality of uncertainty intervals</li> <li>Visualization of metrics, goodness of fit, and quality of uncertainty intervals</li> </ol> <p>This script demonstrates basic usage of each of these five features by creating a data set from realizations of a 1-dimensional sine wave generator with some small added noise, the magnitude of which varies with the input coordinate. </p>"},{"location":"examples/examples_sin_1d/#dataset-generation-validation","title":"Dataset Generation / Validation","text":"<p>As an example for this notebook, we will draw samples from a sine function with small added noise that scales with x.  </p> <p>A dataset is considered to be a sequence of input values (<code>x</code>) of shape (n_samples, n_features), and a one dimensional target (<code>y</code>), which is contained in a 2D array of shape (n_samples, 1). </p> <p>We introduce the methods <code>clean_dataset</code> and <code>validate_dataset</code> to deal with missing values and to verify that the inputs and targets are shaped correctly and have the same number of samples. <code>validate_dataset</code> should be called each time a new dataset is loaded. If <code>validate_dataset</code> fails, we can call <code>clean_dataset</code> before in order to coerce <code>x</code> and <code>y</code> into the right form. Additionally, we generate a test set of data samples to evaluate on. </p> <pre><code>import numpy as np\nimport torch \nfrom uqregressors.utils.data_loader import clean_dataset, validate_dataset\nimport matplotlib.pyplot as plt\nimport seaborn as sns # For visualization\nplt.rcParams['font.size'] = 20\n\n# Set Random Seed for Reproducibility\nseed = 42 \nnp.random.seed(seed)\ntorch.manual_seed(seed)\nrng = np.random.RandomState(seed)\n\n# Define a data generator function to generate targets from features\ndef true_function(x, beta=0.1):\n    noise = beta * x * np.random.standard_normal((len(x), 1))\n    return np.sin(2 * np.pi * x) + noise\n\nn_test = 250 \nn_train = 150\n\nX_test = np.linspace(0, 1, n_test).reshape(-1, 1)\ny_test = true_function(X_test)\ny_noiseless = true_function(X_test, beta=0)\n\nX_train = np.sort(rng.rand(n_train, 1))\ny_train = true_function(X_train).ravel() \n\n# clean_dataset drops missing or NaN values and reshapes X and y to 2D np arrays\nX_train, y_train = clean_dataset(X_train, y_train)\n\n# Confirm the shapes of X and y, and that there are no missing or NaN values\nvalidate_dataset(X_train, y_train, name=\"Synthetic Sine\")\n</code></pre> <pre><code>Summary for: Synthetic Sine dataset\n===================================\nNumber of samples: 150\nNumber of features: 1\nOutput shape: (150, 1)\nDataset validation passed.\n</code></pre> <p>We also define a plotting function that can be used to visualize regressor results: </p> <pre><code>sns.set(style=\"whitegrid\", font_scale=1.5)\ncolors = sns.color_palette(\"deep\")\n# Seaborn colors\ncolor_true = colors[3]    # blue\ncolor_train = colors[1]   # orange\ncolor_test = colors[2]    # green\ncolor_mean = colors[0]    # red\ncolor_interval = colors[0]  # purple or teal depending on palette\n\nplt.figure(figsize=(10, 6))\nplt.plot(X_test, y_noiseless, color=color_true, linestyle='--', linewidth=2, label=\"True Function\")\nplt.scatter(X_train, y_train, color=color_train, alpha=0.9, s=30, label=\"Training Data\")\nplt.scatter(X_test, y_test, color=color_test, alpha=0.9, s=15, label=\"Testing Data\")\nplt.legend()\nplt.show()\n\ndef plot_uncertainty_results(mean, lower, upper, model_name): \n    plt.figure(figsize=(10, 6))\n\n    # Plot true function\n    plt.plot(X_test, y_noiseless, color=color_true, linestyle='--', linewidth=2, label=\"True Function\")\n\n    # Training and testing data\n    plt.scatter(X_train, y_train, color=color_train, alpha=0.9, s=30, label=\"Training Data\")\n    plt.scatter(X_test, y_test, color=color_test, alpha=0.9, s=15, label=\"Testing Data\")\n\n    # Predicted mean and uncertainty\n    plt.plot(X_test, mean, color=color_mean, linewidth=2, label=\"Predicted Mean\")\n    plt.fill_between(X_test.ravel(), lower, upper, color=color_interval, alpha=0.4, label=\"Uncertainty Interval\")\n\n    # Plot settings\n    plt.title(f\"{model_name} Uncertainty Test\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.legend(frameon=False)\n    plt.tight_layout()\n    plt.show()\n</code></pre> <p></p>"},{"location":"examples/examples_sin_1d/#regressors","title":"Regressors","text":"<p>Regressors are models which predict <code>y</code> from <code>x</code>. Regressors follow the scikit-learn API, where they are first initialized with all relevant settings, then optimized to fit the training data with the <code>fit(X, y)</code> function. New predictions are made with the <code>predict(X)</code> method, which will return the Tuple <code>(mean, lower, upper)</code>, where each of these elements is a one dimensional array containing the mean prediction, the predicted lower bound, and the predicted upper bound. Confidence is controlled with the <code>alpha</code> parameter, where the confidence level is 1 - <code>alpha</code>. For example, to construct 95% confidence intervals, set <code>alpha=0.05</code>. </p> <p>Each regressor also has a <code>save</code> and <code>load</code> method, which stores the regressor parameters, along with any metrics, training, and testing data to disk. These functions are explored in detail in other example files. Each type of regressor currently implemented is fit to the sine function above, and visualized. </p> <p>These examples solely describe the implementation and some key parameters of the regressor types. A detailed description of each regressor type is available in the Regressor Details section of the documentation. </p>"},{"location":"examples/examples_sin_1d/#mc-dropout","title":"MC Dropout","text":"<pre><code>from uqregressors.bayesian.dropout import MCDropoutRegressor\nfrom uqregressors.utils.logging import set_logging_config\n\nset_logging_config(print=False) # Disable logging for all future regressors for cleanliness\n\ndropout = MCDropoutRegressor(\n    hidden_sizes=[100, 100],\n    dropout=0.1, # Dropout probability before each layer\n    alpha=0.1,  # 90% confidence\n    tau=1e6, # Aleatoric Uncertainty; should be tuned to provide accurate intervals\n    n_samples=100, # Number of forward passes during predictions\n    scale_data=True, # Internally standardizes the data before training and prediction\n    epochs=1000,\n    learning_rate=1e-3,\n    device=\"cpu\",  # use \"cuda\" if GPU available\n    use_wandb=False # Weights and biases logging as an experimental feature\n)\n\n# sklearn fit and predict API\ndropout.fit(X_train, y_train)\ndropout_sol = dropout.predict(X_test) # dropout_sol = (mean_prediction, lower_bound, upper_bound)\n\nplot_uncertainty_results(*dropout_sol, \"MC Dropout Regressor\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#deep-ensemble","title":"Deep Ensemble","text":"<pre><code>from uqregressors.bayesian.deep_ens import DeepEnsembleRegressor\n\ndeep_ens = DeepEnsembleRegressor(\n    n_estimators=5, # Number of estimators to use within the ensemble\n    hidden_sizes=[100, 100],\n    alpha=0.1,\n    scale_data=True,\n    epochs=1000,\n    learning_rate=1e-3,\n    device=\"cpu\", \n    n_jobs=1, # Experimental: Number of parallel jobs using joblib\n    use_wandb=False)\n\ndeep_ens.fit(X_train, y_train)\ndeep_ens_sol = deep_ens.predict(X_test)\n\nplot_uncertainty_results(*deep_ens_sol, \"Deep Ensemble Regressor\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#standard-gaussian-process-regression-gpr","title":"Standard Gaussian Process Regression (GPR)","text":"<pre><code>from uqregressors.bayesian.gaussian_process import GPRegressor\nfrom sklearn.gaussian_process.kernels import RBF, WhiteKernel\n\ngp_kwargs = {\"normalize_y\": True}\ngpr = GPRegressor(\n    kernel= RBF(length_scale=0.2, length_scale_bounds=(0.05, 1)) + WhiteKernel(noise_level=1), # Kernel function supported by sklearn\n    alpha=0.1, \n    gp_kwargs=gp_kwargs # keyword arguments to sklearn's GPRegressor\n    )\n\ngpr.fit(X_train, y_train)\ngp_sol = gpr.predict(X_test)\nplot_uncertainty_results(*gp_sol, \"Gaussian Process Regressor\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#bbmm-gaussian-process","title":"BBMM Gaussian Process","text":"<pre><code>from uqregressors.bayesian.bbmm_gp import BBMM_GP\nimport gpytorch\n\nbbmm_gp = BBMM_GP(kernel=gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel()), # gpytorch kernel\n                           likelihood=gpytorch.likelihoods.GaussianLikelihood(), # gpytorch likelihood\n                           alpha = 0.1,\n                           epochs=1000,\n                           learning_rate=1e-1,\n                           device=\"cpu\",\n                           use_wandb=False)\n\nbbmm_gp.fit(X_train, y_train)\nbbmm_gp_sol = bbmm_gp.predict(X_test)\nplot_uncertainty_results(*bbmm_gp_sol, \"BBMM Gaussian Process Regressor\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#split-conformal-quantile-regression","title":"Split Conformal Quantile Regression","text":"<pre><code>from uqregressors.conformal.cqr import ConformalQuantileRegressor \n\ncqr = ConformalQuantileRegressor(hidden_sizes = [100, 100], \n                                 cal_size=0.2, # Proportion of training data to use for conformal calibration\n                                 alpha=0.1, \n                                 tau_lo=None, # Lower quantile the underlying regressor is trained for; can be tuned\n                                 tau_hi=None, # Upper quantile the underlying regressor is trained for; can be tuned \n                                 dropout=None, # Dropout probability in the underlying neural network (only during training)\n                                 epochs=2500, \n                                 learning_rate=1e-3, \n                                 device=\"cpu\", \n                                 use_wandb=False \n                                 )\n\ncqr.fit(X_train, y_train)\ncqr_sol = cqr.predict(X_test)\n\nplot_uncertainty_results(*cqr_sol, \"Split Conformal Quantile Regression\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#k-fold-conformal-quantile-regression","title":"K-fold Conformal Quantile Regression","text":"<pre><code>from uqregressors.conformal.k_fold_cqr import KFoldCQR\n\nk_fold_cqr = KFoldCQR(\n    n_estimators=5, # Number of models in the ensemble\n    hidden_sizes=[100, 100],\n    alpha=0.1, \n    tau_lo=None, # Lower quantile the underlying regressor is trained for; can be tuned\n    tau_hi=None, # Upper quantile the underlying regressor is trained for; can be tuned \n    dropout=None,\n    epochs=2500,\n    learning_rate=1e-3,\n    device=\"cpu\",\n    n_jobs=1, # Experimental: number of parallel processes using joblib\n    use_wandb=False)\n\nk_fold_cqr.fit(X_train, y_train)\nk_fold_cqr_sol = k_fold_cqr.predict(X_test)\n\nplot_uncertainty_results(*k_fold_cqr_sol, \"K-Fold Conformal Quantile Regression\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#normalized-conformalens","title":"Normalized ConformalEns","text":"<pre><code>from uqregressors.conformal.conformal_ens import ConformalEnsRegressor\n\nconformal_ens = ConformalEnsRegressor(\n    n_estimators=5, \n    hidden_sizes=[100, 100],\n    alpha=0.1,\n    cal_size=0.2,\n    epochs=1000,\n    gamma=0, # Normalization constant added for stability; can be tuned\n    dropout=None,\n    learning_rate=1e-3,\n    device=\"cpu\",\n    n_jobs=1,\n    use_wandb=False)\n\nconformal_ens.fit(X_train, y_train)\nconformal_ens_sol = conformal_ens.predict(X_test)\n\nplot_uncertainty_results(*conformal_ens_sol, \"Normalized Conformal Ensemble\")\n</code></pre>"},{"location":"examples/examples_sin_1d/#metrics","title":"Metrics","text":"<p>Several metrics can be calculated from the predicted and true values on the test set. If just computing metrics for one model, see the previous example script for usage of the method: <code>compute_all_metrics</code>. Otherwise, to graphically compare the metrics of several models, use the method <code>plot_metrics_comparisons</code>. For details on each of the metrics, see the metrics example. </p> <pre><code>from uqregressors.utils.file_manager import FileManager\nfrom uqregressors.plotting.plotting import plot_metrics_comparisons\nfrom pathlib import Path\n\nsns.set(style=\"whitegrid\", font_scale=1)\n\nsol_dict = {\"MC Dropout\": dropout_sol, \n            \"Deep Ensemble Regressor\": deep_ens_sol, \n            \"Standard GP\": gp_sol, \n            \"BBMM GP\": bbmm_gp_sol, \n            \"Split CQR\": cqr_sol, \n            \"K-fold-CQR\": k_fold_cqr_sol, \n            \"Normalized Conformal Ens\": conformal_ens_sol\n            }\n\nplot_metrics_comparisons(sol_dict, \n                         y_test, \n                         alpha=0.1, \n                         show=True, \n                         save_dir=Path.home()/\".uqregressors\"/\"metrics_curve_tests\", \n                         log_metrics=[], # Which metrics to display on a log scale\n                         filename=\"metrics_test.png\")\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\rmse_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\rmse_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\coverage_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\coverage_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\average_interval_width_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\average_interval_width_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\interval_score_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\interval_score_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\nll_gaussian_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\nll_gaussian_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\error_width_corr_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\error_width_corr_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_under_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\RMSCD_under_metrics_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\lowest_group_coverage_metrics_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\metrics_curve_tests\\plots\\lowest_group_coverage_metrics_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/metrics_curve_tests')\n</code></pre>"},{"location":"examples/examples_sin_1d/#visualization","title":"Visualization","text":""},{"location":"examples/examples_sin_1d/#calibration-curves","title":"Calibration Curves","text":"<p>Generates a calibration curve for the model. This sweeps the predictions through a range of confidence levels and evaluates how close the coverage given by the predicted intervals is to the desired confidence level. The two methods useful here are <code>generate_cal_curve</code>, which outputs the data needed for plotting the calibration curve, and <code>plot_cal_curve</code>, which plots the calibration curve. </p> <pre><code>from uqregressors.plotting.plotting import generate_cal_curve, plot_cal_curve\nfrom pathlib import Path\n\n\"\"\"\nGenerate data with generate_cal_curve. If true, refit will re-train the \nmodel for each confidence level (only necessary for quantile regressors)\n\nReturns desired coverage, empirical coverage, and average interval width.\n\"\"\"\n\ndes_cov, emp_cov, avg_width = generate_cal_curve(dropout, X_test, y_test, \n                                                 refit=False, X_train=X_train, \n                                                 y_train=y_train)\n\n\nplot_cal_curve(des_cov, \n               emp_cov, \n               show=True, \n               save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n               filename=\"dropout_test.png\", \n               title=\"Calibration Curve: Dropout\")\n</code></pre> <pre><code>Model and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp_zma25_5\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpkntl7um3\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp8w3s_uc5\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpjix8a44u\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpad5mm5tn\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpr79nkajn\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpxwxo8guq\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp0y07es5r\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmp0u9sh1vx\\models\\MCDropoutRegressor_20250703_150234\nModel and additional artifacts saved to: C:\\Users\\arsha\\AppData\\Local\\Temp\\tmpl6g1sfq1\\models\\MCDropoutRegressor_20250703_150234\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\nSaved calibration curve to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests/plots/dropout_test.png')\n</code></pre>"},{"location":"examples/examples_sin_1d/#predicted-vs-true-values","title":"Predicted vs. True Values","text":"<p>The method <code>plot_pred_vs_true</code> plots the predicted values against the true values in the test set, with the option to include the predicted confidence intervals.</p> <pre><code>from uqregressors.plotting.plotting import plot_pred_vs_true\n\nplot_pred_vs_true(*dropout_sol, \n                  y_test, \n                  samples=100, # Number of points randomly subsampled for plotting\n                  include_confidence=True, # Whether to include the confidence interval\n                  alpha=0.1, \n                  title=\"Predicted vs Actual: Dropout\",  \n                  save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n                  filename=\"dropout_test.png\", \n                  show=True)\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\nSaved calibration curve to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests/plots/dropout_test.png')\n</code></pre>"},{"location":"examples/examples_sin_1d/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>A simple example of using the <code>tune_hyperparams</code> method is used to tune the lower and upper quantiles of a split conformal quantile regressor using Bayesian Optimization. More detail on how to use the trial objects to suggest parameters for Bayesian Optimization is given in the Optuna documentation. </p> <pre><code>from uqregressors.tuning.tuning import tune_hyperparams, interval_width\n\n# Use Optuna to suggest parameters for the upper and lower quantiles of CQR\nparam_space = {\n    \"tau_lo\": lambda trial: trial.suggest_float(\"tau_lo\", 0.01, 0.1), # Parameter bounds\n    \"tau_hi\": lambda trial: trial.suggest_float(\"tau_hi\", 0.9, 0.99),\n}\n\n# Run hyperparameter tuning study\nopt_cqr, opt_score, study = tune_hyperparams(\n                                            regressor=cqr,\n                                            param_space=param_space,\n                                            X=X_train,\n                                            y=y_train,\n                                            score_fn=interval_width, # Can use custom scoring functions\n                                            greater_is_better=False, # Minimize score function\n                                            n_trials=5,\n                                            n_splits=3, # cross validation used if n_splits &gt; 1\n                                            verbose=False,\n                                            )\nopt_cqr_sol = opt_cqr.predict(X_test)\n\n# Plot predictions from the tuned method\nplot_uncertainty_results(*opt_cqr_sol, \"Tuned Quantile Split Conformal Quantile Regression\")\n\n# Plot metrics comparisons between the tuned and untuned models\nhyperparam_comparison_dict = {\"CQR_untuned\": cqr_sol, \n                              \"CQR_tuned\": opt_cqr_sol}\n\nplot_metrics_comparisons(hyperparam_comparison_dict, y_test, alpha=0.1, show=True, \n                         save_dir=Path.home()/\".uqregressors\"/\"calibration_curve_tests\", \n                         filename=\"dropout_test.png\", log_metrics=[], \n                         excluded_metrics=[\"rmse\", \"interval_score\", \"nll_gaussian\", \n                                           \"RMSCD_under\", \"RMSCD\", \"lowest_group_coverage\", \n                                           \"error_width_corr\"])\n</code></pre> <pre><code>[I 2025-07-03 15:09:29,218] A new study created in memory with name: no-name-47ee4411-08dd-4d34-a2a5-392a43eee1a0\n[I 2025-07-03 15:10:23,735] Trial 0 finished with value: 0.21255050599575043 and parameters: {'tau_lo': 0.07702064489146865, 'tau_hi': 0.9532812424682986}. Best is trial 0 with value: 0.21255050599575043.\n[I 2025-07-03 15:11:18,085] Trial 1 finished with value: 0.17628514766693115 and parameters: {'tau_lo': 0.03544573536094887, 'tau_hi': 0.9619633081353718}. Best is trial 1 with value: 0.17628514766693115.\n[I 2025-07-03 15:12:16,919] Trial 2 finished with value: 0.2198190689086914 and parameters: {'tau_lo': 0.0671337467996006, 'tau_hi': 0.9106960103195387}. Best is trial 1 with value: 0.17628514766693115.\n[I 2025-07-03 15:13:15,684] Trial 3 finished with value: 0.18643641471862793 and parameters: {'tau_lo': 0.07271364248499564, 'tau_hi': 0.984949554237519}. Best is trial 1 with value: 0.17628514766693115.\n[I 2025-07-03 15:14:15,869] Trial 4 finished with value: 0.23922699689865112 and parameters: {'tau_lo': 0.09848472741635608, 'tau_hi': 0.9645779319660116}. Best is trial 1 with value: 0.17628514766693115.\n</code></pre> <p></p> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\coverage_dropout_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\coverage_dropout_test.png\n</code></pre> <p></p> <pre><code>Plot saved to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\average_interval_width_dropout_test.png\nSaved model comparison to C:\\Users\\arsha\\.uqregressors\\calibration_curve_tests\\plots\\average_interval_width_dropout_test.png\n\n\n\n\n\nWindowsPath('C:/Users/arsha/.uqregressors/calibration_curve_tests')\n</code></pre> <p>For this simple example, hyperparameter tuning of the quantiles has resulted in slightly smaller average interval width while maintaining coverage (note that the optimization was not run to convergence)</p>"}]}